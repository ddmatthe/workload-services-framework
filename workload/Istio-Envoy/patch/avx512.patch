diff -Naur boringssl/BUILD.generated.bzl boringssl-patched/BUILD.generated.bzl
--- boringssl/BUILD.generated.bzl	2022-07-20 05:35:27.000000000 +0800
+++ boringssl-patched/BUILD.generated.bzl	2023-08-07 15:03:54.000000000 +0800
@@ -650,6 +650,7 @@
     "linux-x86_64/crypto/cipher_extra/aes128gcmsiv-x86_64.S",
     "linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S",
     "linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S",
+    "linux-x86_64/crypto/fipsmodule/aes-gcm-avx512.S",
     "linux-x86_64/crypto/fipsmodule/aesni-x86_64.S",
     "linux-x86_64/crypto/fipsmodule/ghash-ssse3-x86_64.S",
     "linux-x86_64/crypto/fipsmodule/ghash-x86_64.S",
diff -Naur boringssl/CMakeLists.txt boringssl-patched/CMakeLists.txt
--- boringssl/CMakeLists.txt	2022-07-20 05:35:27.000000000 +0800
+++ boringssl-patched/CMakeLists.txt	2023-08-07 15:03:54.000000000 +0800
@@ -269,6 +269,7 @@
   linux-x86_64/crypto/chacha/chacha-x86_64.S
   linux-x86_64/crypto/cipher_extra/aes128gcmsiv-x86_64.S
   linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S
+  linux-x86_64/crypto/fipsmodule/aes-gcm-avx512.S
   linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S
   linux-x86_64/crypto/fipsmodule/aesni-x86_64.S
   linux-x86_64/crypto/fipsmodule/ghash-ssse3-x86_64.S
diff -Naur boringssl/linux-x86_64/crypto/fipsmodule/aes-gcm-avx512.S boringssl-patched/linux-x86_64/crypto/fipsmodule/aes-gcm-avx512.S
--- boringssl/linux-x86_64/crypto/fipsmodule/aes-gcm-avx512.S	1970-01-01 08:00:00.000000000 +0800
+++ boringssl-patched/linux-x86_64/crypto/fipsmodule/aes-gcm-avx512.S	2023-08-07 15:03:54.000000000 +0800
@@ -0,0 +1,136472 @@
+// This file is generated from a similarly-named Perl script in the BoringSSL
+// source tree. Do not edit by hand.
+
+#if defined(__has_feature)
+#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+#define OPENSSL_NO_ASM
+#endif
+#endif
+
+#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
+#if defined(BORINGSSL_PREFIX)
+#include <boringssl_prefix_symbols_asm.h>
+#endif
+.text	
+.globl	gcm_init_avx512
+.hidden gcm_init_avx512
+.hidden	gcm_init_avx512
+.type	gcm_init_avx512,@function
+.align	32
+gcm_init_avx512:
+.cfi_startproc	
+.byte	243,15,30,250
+	vmovdqu64	(%rsi),%xmm16
+	vpalignr	$8,%xmm16,%xmm16,%xmm16
+
+	vmovdqa64	%xmm16,%xmm2
+	vpsllq	$1,%xmm16,%xmm16
+	vpsrlq	$63,%xmm2,%xmm2
+	vmovdqa	%xmm2,%xmm1
+	vpslldq	$8,%xmm2,%xmm2
+	vpsrldq	$8,%xmm1,%xmm1
+	vporq	%xmm2,%xmm16,%xmm16
+
+	vpshufd	$36,%xmm1,%xmm2
+	vpcmpeqd	TWOONE(%rip),%xmm2,%xmm2
+	vpand	POLY(%rip),%xmm2,%xmm2
+	vpxorq	%xmm2,%xmm16,%xmm16
+
+	vmovdqu64	%xmm16,240(%rdi)
+	vshufi32x4	$0x00,%ymm16,%ymm16,%ymm4
+	vmovdqa	%ymm4,%ymm3
+
+	vpclmulqdq	$0x11,%ymm4,%ymm3,%ymm0
+	vpclmulqdq	$0x00,%ymm4,%ymm3,%ymm1
+	vpclmulqdq	$0x01,%ymm4,%ymm3,%ymm2
+	vpclmulqdq	$0x10,%ymm4,%ymm3,%ymm3
+	vpxorq	%ymm2,%ymm3,%ymm3
+
+	vpsrldq	$8,%ymm3,%ymm2
+	vpslldq	$8,%ymm3,%ymm3
+	vpxorq	%ymm2,%ymm0,%ymm0
+	vpxorq	%ymm1,%ymm3,%ymm3
+
+
+
+	vmovdqu64	POLY2(%rip),%ymm2
+
+	vpclmulqdq	$0x01,%ymm3,%ymm2,%ymm1
+	vpslldq	$8,%ymm1,%ymm1
+	vpxorq	%ymm1,%ymm3,%ymm3
+
+
+
+	vpclmulqdq	$0x00,%ymm3,%ymm2,%ymm1
+	vpsrldq	$4,%ymm1,%ymm1
+	vpclmulqdq	$0x10,%ymm3,%ymm2,%ymm3
+	vpslldq	$4,%ymm3,%ymm3
+
+	vpternlogq	$0x96,%ymm1,%ymm0,%ymm3
+
+	vmovdqu64	%xmm3,224(%rdi)
+	vinserti64x2	$1,%xmm16,%ymm3,%ymm4
+	vmovdqa64	%ymm4,%ymm5
+
+	vpclmulqdq	$0x11,%ymm3,%ymm4,%ymm0
+	vpclmulqdq	$0x00,%ymm3,%ymm4,%ymm1
+	vpclmulqdq	$0x01,%ymm3,%ymm4,%ymm2
+	vpclmulqdq	$0x10,%ymm3,%ymm4,%ymm4
+	vpxorq	%ymm2,%ymm4,%ymm4
+
+	vpsrldq	$8,%ymm4,%ymm2
+	vpslldq	$8,%ymm4,%ymm4
+	vpxorq	%ymm2,%ymm0,%ymm0
+	vpxorq	%ymm1,%ymm4,%ymm4
+
+
+
+	vmovdqu64	POLY2(%rip),%ymm2
+
+	vpclmulqdq	$0x01,%ymm4,%ymm2,%ymm1
+	vpslldq	$8,%ymm1,%ymm1
+	vpxorq	%ymm1,%ymm4,%ymm4
+
+
+
+	vpclmulqdq	$0x00,%ymm4,%ymm2,%ymm1
+	vpsrldq	$4,%ymm1,%ymm1
+	vpclmulqdq	$0x10,%ymm4,%ymm2,%ymm4
+	vpslldq	$4,%ymm4,%ymm4
+
+	vpternlogq	$0x96,%ymm1,%ymm0,%ymm4
+
+	vmovdqu64	%ymm4,192(%rdi)
+
+	vinserti64x4	$1,%ymm5,%zmm4,%zmm4
+
+
+	vshufi64x2	$0x00,%zmm4,%zmm4,%zmm3
+	vmovdqa64	%zmm4,%zmm5
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm0
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm1
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm2
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm2,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm2
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm2,%zmm0,%zmm0
+	vpxorq	%zmm1,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm2
+
+	vpclmulqdq	$0x01,%zmm4,%zmm2,%zmm1
+	vpslldq	$8,%zmm1,%zmm1
+	vpxorq	%zmm1,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm2,%zmm1
+	vpsrldq	$4,%zmm1,%zmm1
+	vpclmulqdq	$0x10,%zmm4,%zmm2,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm1,%zmm0,%zmm4
+
+	vmovdqu64	%zmm4,128(%rdi)
+	vshufi64x2	$0x00,%zmm4,%zmm4,%zmm3
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm0
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm1
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm2
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm2,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm2
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm2,%zmm0,%zmm0
+	vpxorq	%zmm1,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm2
+
+	vpclmulqdq	$0x01,%zmm5,%zmm2,%zmm1
+	vpslldq	$8,%zmm1,%zmm1
+	vpxorq	%zmm1,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm2,%zmm1
+	vpsrldq	$4,%zmm1,%zmm1
+	vpclmulqdq	$0x10,%zmm5,%zmm2,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm1,%zmm0,%zmm5
+
+	vmovdqu64	%zmm5,64(%rdi)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm0
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm1
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm2
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm2,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm2
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm2,%zmm0,%zmm0
+	vpxorq	%zmm1,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm2
+
+	vpclmulqdq	$0x01,%zmm4,%zmm2,%zmm1
+	vpslldq	$8,%zmm1,%zmm1
+	vpxorq	%zmm1,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm2,%zmm1
+	vpsrldq	$4,%zmm1,%zmm1
+	vpclmulqdq	$0x10,%zmm4,%zmm2,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm1,%zmm0,%zmm4
+
+	vmovdqu64	%zmm4,0(%rdi)
+	vzeroupper
+.Lexit_init:
+	.byte	0xf3,0xc3
+.cfi_endproc	
+.size	gcm_init_avx512, .-gcm_init_avx512
+.globl	gcm_gmult_avx512
+.hidden gcm_gmult_avx512
+.hidden	gcm_gmult_avx512
+.type	gcm_gmult_avx512,@function
+.align	32
+gcm_gmult_avx512:
+.cfi_startproc	
+.byte	243,15,30,250
+	vmovdqu64	(%rdi),%xmm1
+
+
+	vpshufb	SHUF_MASK(%rip),%xmm1,%xmm1
+	vmovdqu64	240(%rsi),%xmm2
+
+	vpclmulqdq	$0x11,%xmm2,%xmm1,%xmm3
+	vpclmulqdq	$0x00,%xmm2,%xmm1,%xmm4
+	vpclmulqdq	$0x01,%xmm2,%xmm1,%xmm5
+	vpclmulqdq	$0x10,%xmm2,%xmm1,%xmm1
+	vpxorq	%xmm5,%xmm1,%xmm1
+
+	vpsrldq	$8,%xmm1,%xmm5
+	vpslldq	$8,%xmm1,%xmm1
+	vpxorq	%xmm5,%xmm3,%xmm3
+	vpxorq	%xmm4,%xmm1,%xmm1
+
+
+
+	vmovdqu64	POLY2(%rip),%xmm5
+
+	vpclmulqdq	$0x01,%xmm1,%xmm5,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm1,%xmm1
+
+
+
+	vpclmulqdq	$0x00,%xmm1,%xmm5,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm5,%xmm1
+	vpslldq	$4,%xmm1,%xmm1
+
+	vpternlogq	$0x96,%xmm4,%xmm3,%xmm1
+
+	vpshufb	SHUF_MASK(%rip),%xmm1,%xmm1
+	vmovdqu64	%xmm1,(%rdi)
+	vzeroupper
+.Lexit_gmult:
+	.byte	0xf3,0xc3
+.cfi_endproc	
+.size	gcm_gmult_avx512, .-gcm_gmult_avx512
+.globl	gcm_ghash_avx512
+.hidden gcm_ghash_avx512
+.hidden	gcm_ghash_avx512
+.type	gcm_ghash_avx512,@function
+.align	32
+gcm_ghash_avx512:
+.cfi_startproc	
+.Lghash_seh_begin:
+.byte	243,15,30,250
+	pushq	%rbx
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%rbx,-16
+.Lghash_seh_push_rbx:
+	pushq	%rbp
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%rbp,-24
+.Lghash_seh_push_rbp:
+	pushq	%r12
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r12,-32
+.Lghash_seh_push_r12:
+	pushq	%r13
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r13,-40
+.Lghash_seh_push_r13:
+	pushq	%r14
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r14,-48
+.Lghash_seh_push_r14:
+	pushq	%r15
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r15,-56
+.Lghash_seh_push_r15:
+
+
+
+
+
+
+
+
+
+
+	leaq	0(%rsp),%rbp
+.cfi_def_cfa_register	%rbp
+.Lghash_seh_setfp:
+
+.Lghash_seh_prolog_end:
+	subq	$820,%rsp
+	andq	$(-64),%rsp
+	vmovdqu64	(%rdi),%xmm14
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+	movq	%rdx,%r10
+	movq	%rcx,%r11
+	orq	%r11,%r11
+	jz	.L_CALC_AAD_done_osegGstECuzccFb
+
+	xorq	%rbx,%rbx
+	vmovdqa64	SHUF_MASK(%rip),%zmm16
+
+.L_get_AAD_loop48x16_osegGstECuzccFb:
+	cmpq	$768,%r11
+	jl	.L_exit_AAD_loop48x16_osegGstECuzccFb
+	vmovdqu64	0(%r10),%zmm11
+	vmovdqu64	64(%r10),%zmm3
+	vmovdqu64	128(%r10),%zmm4
+	vmovdqu64	192(%r10),%zmm5
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	testq	%rbx,%rbx
+	jnz	.L_skip_hkeys_precomputation_ojArtkqkkCxvBmh
+
+	vmovdqu64	192(%rsi),%zmm1
+	vmovdqu64	%zmm1,704(%rsp)
+
+	vmovdqu64	128(%rsi),%zmm9
+	vmovdqu64	%zmm9,640(%rsp)
+
+
+	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9
+
+	vmovdqu64	64(%rsi),%zmm10
+	vmovdqu64	%zmm10,576(%rsp)
+
+	vmovdqu64	0(%rsi),%zmm12
+	vmovdqu64	%zmm12,512(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm10,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm17
+	vpslldq	$8,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
+	vpslldq	$4,%zmm10,%zmm10
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10
+
+	vmovdqu64	%zmm10,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm12,%zmm12
+
+	vpsrldq	$8,%zmm12,%zmm17
+	vpslldq	$8,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
+	vpslldq	$4,%zmm12,%zmm12
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12
+
+	vmovdqu64	%zmm12,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm10,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm17
+	vpslldq	$8,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
+	vpslldq	$4,%zmm10,%zmm10
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10
+
+	vmovdqu64	%zmm10,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm12,%zmm12
+
+	vpsrldq	$8,%zmm12,%zmm17
+	vpslldq	$8,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
+	vpslldq	$4,%zmm12,%zmm12
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12
+
+	vmovdqu64	%zmm12,256(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm10,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm17
+	vpslldq	$8,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
+	vpslldq	$4,%zmm10,%zmm10
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10
+
+	vmovdqu64	%zmm10,192(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm12,%zmm12
+
+	vpsrldq	$8,%zmm12,%zmm17
+	vpslldq	$8,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
+	vpslldq	$4,%zmm12,%zmm12
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12
+
+	vmovdqu64	%zmm12,128(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm10,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm17
+	vpslldq	$8,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
+	vpslldq	$4,%zmm10,%zmm10
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10
+
+	vmovdqu64	%zmm10,64(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm12,%zmm12
+
+	vpsrldq	$8,%zmm12,%zmm17
+	vpslldq	$8,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
+	vpslldq	$4,%zmm12,%zmm12
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12
+
+	vmovdqu64	%zmm12,0(%rsp)
+.L_skip_hkeys_precomputation_ojArtkqkkCxvBmh:
+	movq	$1,%rbx
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	0(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
+	vmovdqu64	64(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
+	vpxorq	%zmm17,%zmm10,%zmm7
+	vpxorq	%zmm13,%zmm1,%zmm6
+	vpxorq	%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	128(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
+	vmovdqu64	192(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18
+
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	256(%r10),%zmm11
+	vmovdqu64	320(%r10),%zmm3
+	vmovdqu64	384(%r10),%zmm4
+	vmovdqu64	448(%r10),%zmm5
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	vmovdqu64	256(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
+	vmovdqu64	320(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	384(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
+	vmovdqu64	448(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18
+
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	512(%r10),%zmm11
+	vmovdqu64	576(%r10),%zmm3
+	vmovdqu64	640(%r10),%zmm4
+	vmovdqu64	704(%r10),%zmm5
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	vmovdqu64	512(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
+	vmovdqu64	576(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	640(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
+	vmovdqu64	704(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18
+
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+
+	vpsrldq	$8,%zmm7,%zmm1
+	vpslldq	$8,%zmm7,%zmm9
+	vpxorq	%zmm1,%zmm6,%zmm6
+	vpxorq	%zmm9,%zmm8,%zmm8
+	vextracti64x4	$1,%zmm6,%ymm1
+	vpxorq	%ymm1,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm1
+	vpxorq	%xmm1,%xmm6,%xmm6
+	vextracti64x4	$1,%zmm8,%ymm9
+	vpxorq	%ymm9,%ymm8,%ymm8
+	vextracti32x4	$1,%ymm8,%xmm9
+	vpxorq	%xmm9,%xmm8,%xmm8
+	vmovdqa64	POLY2(%rip),%xmm10
+
+
+	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
+	vpslldq	$8,%xmm1,%xmm1
+	vpxorq	%xmm1,%xmm8,%xmm1
+
+
+	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
+	vpsrldq	$4,%xmm9,%xmm9
+	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm6,%xmm9,%xmm14
+
+	subq	$768,%r11
+	je	.L_CALC_AAD_done_osegGstECuzccFb
+
+	addq	$768,%r10
+	jmp	.L_get_AAD_loop48x16_osegGstECuzccFb
+
+.L_exit_AAD_loop48x16_osegGstECuzccFb:
+
+	cmpq	$512,%r11
+	jl	.L_less_than_32x16_osegGstECuzccFb
+
+	vmovdqu64	0(%r10),%zmm11
+	vmovdqu64	64(%r10),%zmm3
+	vmovdqu64	128(%r10),%zmm4
+	vmovdqu64	192(%r10),%zmm5
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	testq	%rbx,%rbx
+	jnz	.L_skip_hkeys_precomputation_dnAotGByelkekkD
+
+	vmovdqu64	192(%rsi),%zmm1
+	vmovdqu64	%zmm1,704(%rsp)
+
+	vmovdqu64	128(%rsi),%zmm9
+	vmovdqu64	%zmm9,640(%rsp)
+
+
+	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9
+
+	vmovdqu64	64(%rsi),%zmm10
+	vmovdqu64	%zmm10,576(%rsp)
+
+	vmovdqu64	0(%rsi),%zmm12
+	vmovdqu64	%zmm12,512(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm10,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm17
+	vpslldq	$8,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
+	vpslldq	$4,%zmm10,%zmm10
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10
+
+	vmovdqu64	%zmm10,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm12,%zmm12
+
+	vpsrldq	$8,%zmm12,%zmm17
+	vpslldq	$8,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
+	vpslldq	$4,%zmm12,%zmm12
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12
+
+	vmovdqu64	%zmm12,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm10,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm17
+	vpslldq	$8,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
+	vpslldq	$4,%zmm10,%zmm10
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10
+
+	vmovdqu64	%zmm10,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm12,%zmm12
+
+	vpsrldq	$8,%zmm12,%zmm17
+	vpslldq	$8,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
+	vpslldq	$4,%zmm12,%zmm12
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12
+
+	vmovdqu64	%zmm12,256(%rsp)
+.L_skip_hkeys_precomputation_dnAotGByelkekkD:
+	movq	$1,%rbx
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	256(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
+	vmovdqu64	320(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
+	vpxorq	%zmm17,%zmm10,%zmm7
+	vpxorq	%zmm13,%zmm1,%zmm6
+	vpxorq	%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	384(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
+	vmovdqu64	448(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18
+
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	256(%r10),%zmm11
+	vmovdqu64	320(%r10),%zmm3
+	vmovdqu64	384(%r10),%zmm4
+	vmovdqu64	448(%r10),%zmm5
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	vmovdqu64	512(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
+	vmovdqu64	576(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	640(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
+	vmovdqu64	704(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18
+
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+
+	vpsrldq	$8,%zmm7,%zmm1
+	vpslldq	$8,%zmm7,%zmm9
+	vpxorq	%zmm1,%zmm6,%zmm6
+	vpxorq	%zmm9,%zmm8,%zmm8
+	vextracti64x4	$1,%zmm6,%ymm1
+	vpxorq	%ymm1,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm1
+	vpxorq	%xmm1,%xmm6,%xmm6
+	vextracti64x4	$1,%zmm8,%ymm9
+	vpxorq	%ymm9,%ymm8,%ymm8
+	vextracti32x4	$1,%ymm8,%xmm9
+	vpxorq	%xmm9,%xmm8,%xmm8
+	vmovdqa64	POLY2(%rip),%xmm10
+
+
+	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
+	vpslldq	$8,%xmm1,%xmm1
+	vpxorq	%xmm1,%xmm8,%xmm1
+
+
+	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
+	vpsrldq	$4,%xmm9,%xmm9
+	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm6,%xmm9,%xmm14
+
+	subq	$512,%r11
+	je	.L_CALC_AAD_done_osegGstECuzccFb
+
+	addq	$512,%r10
+	jmp	.L_less_than_16x16_osegGstECuzccFb
+
+.L_less_than_32x16_osegGstECuzccFb:
+	cmpq	$256,%r11
+	jl	.L_less_than_16x16_osegGstECuzccFb
+
+	vmovdqu64	0(%r10),%zmm11
+	vmovdqu64	64(%r10),%zmm3
+	vmovdqu64	128(%r10),%zmm4
+	vmovdqu64	192(%r10),%zmm5
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	0(%rsi),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
+	vmovdqu64	64(%rsi),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
+	vpxorq	%zmm17,%zmm10,%zmm7
+	vpxorq	%zmm13,%zmm1,%zmm6
+	vpxorq	%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	128(%rsi),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
+	vmovdqu64	192(%rsi),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18
+
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+
+	vpsrldq	$8,%zmm7,%zmm1
+	vpslldq	$8,%zmm7,%zmm9
+	vpxorq	%zmm1,%zmm6,%zmm6
+	vpxorq	%zmm9,%zmm8,%zmm8
+	vextracti64x4	$1,%zmm6,%ymm1
+	vpxorq	%ymm1,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm1
+	vpxorq	%xmm1,%xmm6,%xmm6
+	vextracti64x4	$1,%zmm8,%ymm9
+	vpxorq	%ymm9,%ymm8,%ymm8
+	vextracti32x4	$1,%ymm8,%xmm9
+	vpxorq	%xmm9,%xmm8,%xmm8
+	vmovdqa64	POLY2(%rip),%xmm10
+
+
+	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
+	vpslldq	$8,%xmm1,%xmm1
+	vpxorq	%xmm1,%xmm8,%xmm1
+
+
+	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
+	vpsrldq	$4,%xmm9,%xmm9
+	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm6,%xmm9,%xmm14
+
+	subq	$256,%r11
+	je	.L_CALC_AAD_done_osegGstECuzccFb
+
+	addq	$256,%r10
+
+.L_less_than_16x16_osegGstECuzccFb:
+
+	leaq	byte64_len_to_mask_table(%rip),%r12
+	leaq	(%r12,%r11,8),%r12
+
+
+	addl	$15,%r11d
+	shrl	$4,%r11d
+	cmpl	$2,%r11d
+	jb	.L_AAD_blocks_1_osegGstECuzccFb
+	je	.L_AAD_blocks_2_osegGstECuzccFb
+	cmpl	$4,%r11d
+	jb	.L_AAD_blocks_3_osegGstECuzccFb
+	je	.L_AAD_blocks_4_osegGstECuzccFb
+	cmpl	$6,%r11d
+	jb	.L_AAD_blocks_5_osegGstECuzccFb
+	je	.L_AAD_blocks_6_osegGstECuzccFb
+	cmpl	$8,%r11d
+	jb	.L_AAD_blocks_7_osegGstECuzccFb
+	je	.L_AAD_blocks_8_osegGstECuzccFb
+	cmpl	$10,%r11d
+	jb	.L_AAD_blocks_9_osegGstECuzccFb
+	je	.L_AAD_blocks_10_osegGstECuzccFb
+	cmpl	$12,%r11d
+	jb	.L_AAD_blocks_11_osegGstECuzccFb
+	je	.L_AAD_blocks_12_osegGstECuzccFb
+	cmpl	$14,%r11d
+	jb	.L_AAD_blocks_13_osegGstECuzccFb
+	je	.L_AAD_blocks_14_osegGstECuzccFb
+	cmpl	$15,%r11d
+	je	.L_AAD_blocks_15_osegGstECuzccFb
+.L_AAD_blocks_16_osegGstECuzccFb:
+	subq	$1536,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%zmm4
+	vmovdqu8	192(%r10),%zmm5{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	0(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	64(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vmovdqu64	128(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm9,%zmm11,%zmm1
+	vpternlogq	$0x96,%zmm10,%zmm3,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm12,%zmm11,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm3,%zmm8
+	vmovdqu64	192(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm13
+	vpxorq	%zmm9,%zmm1,%zmm9
+	vpxorq	%zmm10,%zmm6,%zmm10
+	vpxorq	%zmm12,%zmm7,%zmm12
+	vpxorq	%zmm13,%zmm8,%zmm13
+
+	vpxorq	%zmm13,%zmm12,%zmm12
+	vpsrldq	$8,%zmm12,%zmm7
+	vpslldq	$8,%zmm12,%zmm8
+	vpxorq	%zmm7,%zmm9,%zmm1
+	vpxorq	%zmm8,%zmm10,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_15_osegGstECuzccFb:
+	subq	$1536,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%zmm4
+	vmovdqu8	192(%r10),%zmm5{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	16(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	80(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vmovdqu64	144(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
+	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
+	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
+	vmovdqu64	208(%rsi),%ymm15
+	vinserti64x2	$2,240(%rsi),%zmm15,%zmm15
+	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm8
+	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_14_osegGstECuzccFb:
+	subq	$1536,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%zmm4
+	vmovdqu8	192(%r10),%ymm5{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%ymm16,%ymm5,%ymm5
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	32(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	96(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vmovdqu64	160(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
+	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
+	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
+	vmovdqu64	224(%rsi),%ymm15
+	vpclmulqdq	$0x01,%ymm15,%ymm5,%ymm7
+	vpclmulqdq	$0x10,%ymm15,%ymm5,%ymm8
+	vpclmulqdq	$0x11,%ymm15,%ymm5,%ymm1
+	vpclmulqdq	$0x00,%ymm15,%ymm5,%ymm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_13_osegGstECuzccFb:
+	subq	$1536,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%zmm4
+	vmovdqu8	192(%r10),%xmm5{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%xmm16,%xmm5,%xmm5
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	48(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	112(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vmovdqu64	176(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
+	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
+	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
+	vmovdqu64	240(%rsi),%xmm15
+	vpclmulqdq	$0x01,%xmm15,%xmm5,%xmm7
+	vpclmulqdq	$0x10,%xmm15,%xmm5,%xmm8
+	vpclmulqdq	$0x11,%xmm15,%xmm5,%xmm1
+	vpclmulqdq	$0x00,%xmm15,%xmm5,%xmm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_12_osegGstECuzccFb:
+	subq	$1024,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%zmm4{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	64(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	128(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vmovdqu64	192(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
+	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
+	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
+
+	vpxorq	%zmm13,%zmm12,%zmm12
+	vpsrldq	$8,%zmm12,%zmm7
+	vpslldq	$8,%zmm12,%zmm8
+	vpxorq	%zmm7,%zmm9,%zmm1
+	vpxorq	%zmm8,%zmm10,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_11_osegGstECuzccFb:
+	subq	$1024,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%zmm4{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	80(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	144(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vpxorq	%zmm9,%zmm1,%zmm9
+	vpxorq	%zmm10,%zmm6,%zmm10
+	vpxorq	%zmm12,%zmm7,%zmm12
+	vpxorq	%zmm13,%zmm8,%zmm13
+	vmovdqu64	208(%rsi),%ymm15
+	vinserti64x2	$2,240(%rsi),%zmm15,%zmm15
+	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm8
+	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_10_osegGstECuzccFb:
+	subq	$1024,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%ymm4{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%ymm16,%ymm4,%ymm4
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	96(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	160(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vpxorq	%zmm9,%zmm1,%zmm9
+	vpxorq	%zmm10,%zmm6,%zmm10
+	vpxorq	%zmm12,%zmm7,%zmm12
+	vpxorq	%zmm13,%zmm8,%zmm13
+	vmovdqu64	224(%rsi),%ymm15
+	vpclmulqdq	$0x01,%ymm15,%ymm4,%ymm7
+	vpclmulqdq	$0x10,%ymm15,%ymm4,%ymm8
+	vpclmulqdq	$0x11,%ymm15,%ymm4,%ymm1
+	vpclmulqdq	$0x00,%ymm15,%ymm4,%ymm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_9_osegGstECuzccFb:
+	subq	$1024,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%xmm4{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%xmm16,%xmm4,%xmm4
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	112(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	176(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vpxorq	%zmm9,%zmm1,%zmm9
+	vpxorq	%zmm10,%zmm6,%zmm10
+	vpxorq	%zmm12,%zmm7,%zmm12
+	vpxorq	%zmm13,%zmm8,%zmm13
+	vmovdqu64	240(%rsi),%xmm15
+	vpclmulqdq	$0x01,%xmm15,%xmm4,%xmm7
+	vpclmulqdq	$0x10,%xmm15,%xmm4,%xmm8
+	vpclmulqdq	$0x11,%xmm15,%xmm4,%xmm1
+	vpclmulqdq	$0x00,%xmm15,%xmm4,%xmm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_8_osegGstECuzccFb:
+	subq	$512,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	128(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	192(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vpxorq	%zmm9,%zmm1,%zmm9
+	vpxorq	%zmm10,%zmm6,%zmm10
+	vpxorq	%zmm12,%zmm7,%zmm12
+	vpxorq	%zmm13,%zmm8,%zmm13
+
+	vpxorq	%zmm13,%zmm12,%zmm12
+	vpsrldq	$8,%zmm12,%zmm7
+	vpslldq	$8,%zmm12,%zmm8
+	vpxorq	%zmm7,%zmm9,%zmm1
+	vpxorq	%zmm8,%zmm10,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_7_osegGstECuzccFb:
+	subq	$512,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	144(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
+	vmovdqu64	208(%rsi),%ymm15
+	vinserti64x2	$2,240(%rsi),%zmm15,%zmm15
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm8
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_6_osegGstECuzccFb:
+	subq	$512,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%ymm3{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%ymm16,%ymm3,%ymm3
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	160(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
+	vmovdqu64	224(%rsi),%ymm15
+	vpclmulqdq	$0x01,%ymm15,%ymm3,%ymm7
+	vpclmulqdq	$0x10,%ymm15,%ymm3,%ymm8
+	vpclmulqdq	$0x11,%ymm15,%ymm3,%ymm1
+	vpclmulqdq	$0x00,%ymm15,%ymm3,%ymm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_5_osegGstECuzccFb:
+	subq	$512,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%xmm3{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%xmm16,%xmm3,%xmm3
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	176(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
+	vmovdqu64	240(%rsi),%xmm15
+	vpclmulqdq	$0x01,%xmm15,%xmm3,%xmm7
+	vpclmulqdq	$0x10,%xmm15,%xmm3,%xmm8
+	vpclmulqdq	$0x11,%xmm15,%xmm3,%xmm1
+	vpclmulqdq	$0x00,%xmm15,%xmm3,%xmm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_4_osegGstECuzccFb:
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	192(%rsi),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
+
+	vpxorq	%zmm13,%zmm12,%zmm12
+	vpsrldq	$8,%zmm12,%zmm7
+	vpslldq	$8,%zmm12,%zmm8
+	vpxorq	%zmm7,%zmm9,%zmm1
+	vpxorq	%zmm8,%zmm10,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_3_osegGstECuzccFb:
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	208(%rsi),%ymm15
+	vinserti64x2	$2,240(%rsi),%zmm15,%zmm15
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_2_osegGstECuzccFb:
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%ymm11{%k1}{z}
+	vpshufb	%ymm16,%ymm11,%ymm11
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	224(%rsi),%ymm15
+	vpclmulqdq	$0x01,%ymm15,%ymm11,%ymm7
+	vpclmulqdq	$0x10,%ymm15,%ymm11,%ymm8
+	vpclmulqdq	$0x11,%ymm15,%ymm11,%ymm1
+	vpclmulqdq	$0x00,%ymm15,%ymm11,%ymm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+	jmp	.L_CALC_AAD_done_osegGstECuzccFb
+.L_AAD_blocks_1_osegGstECuzccFb:
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%xmm11{%k1}{z}
+	vpshufb	%xmm16,%xmm11,%xmm11
+	vpxorq	%zmm14,%zmm11,%zmm11
+	vmovdqu64	240(%rsi),%xmm15
+	vpclmulqdq	$0x01,%xmm15,%xmm11,%xmm7
+	vpclmulqdq	$0x10,%xmm15,%xmm11,%xmm8
+	vpclmulqdq	$0x11,%xmm15,%xmm11,%xmm1
+	vpclmulqdq	$0x00,%xmm15,%xmm11,%xmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14
+
+.L_CALC_AAD_done_osegGstECuzccFb:
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+	vmovdqu64	%xmm14,(%rdi)
+	cmpq	$256,%rcx
+	jbe	.Lskip_hkeys_cleanup_ppavAtphgcbquGg
+	vpxor	%xmm0,%xmm0,%xmm0
+	vmovdqa64	%zmm0,0(%rsp)
+	vmovdqa64	%zmm0,64(%rsp)
+	vmovdqa64	%zmm0,128(%rsp)
+	vmovdqa64	%zmm0,192(%rsp)
+	vmovdqa64	%zmm0,256(%rsp)
+	vmovdqa64	%zmm0,320(%rsp)
+	vmovdqa64	%zmm0,384(%rsp)
+	vmovdqa64	%zmm0,448(%rsp)
+	vmovdqa64	%zmm0,512(%rsp)
+	vmovdqa64	%zmm0,576(%rsp)
+	vmovdqa64	%zmm0,640(%rsp)
+	vmovdqa64	%zmm0,704(%rsp)
+.Lskip_hkeys_cleanup_ppavAtphgcbquGg:
+	vzeroupper
+	leaq	(%rbp),%rsp
+.cfi_def_cfa_register	%rsp
+	popq	%r15
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r15
+	popq	%r14
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r14
+	popq	%r13
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r13
+	popq	%r12
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r12
+	popq	%rbp
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%rbp
+	popq	%rbx
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%rbx
+.Lexit_ghash:
+	.byte	0xf3,0xc3
+.Lghash_seh_end:
+.cfi_endproc	
+.size	gcm_ghash_avx512, .-gcm_ghash_avx512
+.globl	gcm_setiv_avx512
+.hidden gcm_setiv_avx512
+.hidden	gcm_setiv_avx512
+.type	gcm_setiv_avx512,@function
+.align	32
+gcm_setiv_avx512:
+.cfi_startproc	
+.Lsetiv_seh_begin:
+.byte	243,15,30,250
+	pushq	%rbx
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%rbx,-16
+.Lsetiv_seh_push_rbx:
+	pushq	%rbp
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%rbp,-24
+.Lsetiv_seh_push_rbp:
+	pushq	%r12
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r12,-32
+.Lsetiv_seh_push_r12:
+	pushq	%r13
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r13,-40
+.Lsetiv_seh_push_r13:
+	pushq	%r14
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r14,-48
+.Lsetiv_seh_push_r14:
+	pushq	%r15
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r15,-56
+.Lsetiv_seh_push_r15:
+
+
+
+
+
+
+
+
+
+
+	leaq	0(%rsp),%rbp
+.cfi_def_cfa_register	%rbp
+.Lsetiv_seh_setfp:
+
+.Lsetiv_seh_prolog_end:
+	subq	$820,%rsp
+	andq	$(-64),%rsp
+	cmpq	$12,%rcx
+	je	iv_len_12_init_IV
+	vpxor	%xmm2,%xmm2,%xmm2
+	leaq	96(%rsi),%r13
+	movq	%rdx,%r10
+	movq	%rcx,%r11
+	orq	%r11,%r11
+	jz	.L_CALC_AAD_done_vfqnefdewhiBulw
+
+	xorq	%rbx,%rbx
+	vmovdqa64	SHUF_MASK(%rip),%zmm16
+
+.L_get_AAD_loop48x16_vfqnefdewhiBulw:
+	cmpq	$768,%r11
+	jl	.L_exit_AAD_loop48x16_vfqnefdewhiBulw
+	vmovdqu64	0(%r10),%zmm11
+	vmovdqu64	64(%r10),%zmm3
+	vmovdqu64	128(%r10),%zmm4
+	vmovdqu64	192(%r10),%zmm5
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	testq	%rbx,%rbx
+	jnz	.L_skip_hkeys_precomputation_nplaEiglFgezBkG
+
+	vmovdqu64	192(%r13),%zmm1
+	vmovdqu64	%zmm1,704(%rsp)
+
+	vmovdqu64	128(%r13),%zmm9
+	vmovdqu64	%zmm9,640(%rsp)
+
+
+	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9
+
+	vmovdqu64	64(%r13),%zmm10
+	vmovdqu64	%zmm10,576(%rsp)
+
+	vmovdqu64	0(%r13),%zmm12
+	vmovdqu64	%zmm12,512(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm10,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm17
+	vpslldq	$8,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
+	vpslldq	$4,%zmm10,%zmm10
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10
+
+	vmovdqu64	%zmm10,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm12,%zmm12
+
+	vpsrldq	$8,%zmm12,%zmm17
+	vpslldq	$8,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
+	vpslldq	$4,%zmm12,%zmm12
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12
+
+	vmovdqu64	%zmm12,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm10,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm17
+	vpslldq	$8,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
+	vpslldq	$4,%zmm10,%zmm10
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10
+
+	vmovdqu64	%zmm10,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm12,%zmm12
+
+	vpsrldq	$8,%zmm12,%zmm17
+	vpslldq	$8,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
+	vpslldq	$4,%zmm12,%zmm12
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12
+
+	vmovdqu64	%zmm12,256(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm10,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm17
+	vpslldq	$8,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
+	vpslldq	$4,%zmm10,%zmm10
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10
+
+	vmovdqu64	%zmm10,192(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm12,%zmm12
+
+	vpsrldq	$8,%zmm12,%zmm17
+	vpslldq	$8,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
+	vpslldq	$4,%zmm12,%zmm12
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12
+
+	vmovdqu64	%zmm12,128(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm10,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm17
+	vpslldq	$8,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
+	vpslldq	$4,%zmm10,%zmm10
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10
+
+	vmovdqu64	%zmm10,64(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm12,%zmm12
+
+	vpsrldq	$8,%zmm12,%zmm17
+	vpslldq	$8,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
+	vpslldq	$4,%zmm12,%zmm12
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12
+
+	vmovdqu64	%zmm12,0(%rsp)
+.L_skip_hkeys_precomputation_nplaEiglFgezBkG:
+	movq	$1,%rbx
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	0(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
+	vmovdqu64	64(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
+	vpxorq	%zmm17,%zmm10,%zmm7
+	vpxorq	%zmm13,%zmm1,%zmm6
+	vpxorq	%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	128(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
+	vmovdqu64	192(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18
+
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	256(%r10),%zmm11
+	vmovdqu64	320(%r10),%zmm3
+	vmovdqu64	384(%r10),%zmm4
+	vmovdqu64	448(%r10),%zmm5
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	vmovdqu64	256(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
+	vmovdqu64	320(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	384(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
+	vmovdqu64	448(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18
+
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	512(%r10),%zmm11
+	vmovdqu64	576(%r10),%zmm3
+	vmovdqu64	640(%r10),%zmm4
+	vmovdqu64	704(%r10),%zmm5
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	vmovdqu64	512(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
+	vmovdqu64	576(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	640(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
+	vmovdqu64	704(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18
+
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+
+	vpsrldq	$8,%zmm7,%zmm1
+	vpslldq	$8,%zmm7,%zmm9
+	vpxorq	%zmm1,%zmm6,%zmm6
+	vpxorq	%zmm9,%zmm8,%zmm8
+	vextracti64x4	$1,%zmm6,%ymm1
+	vpxorq	%ymm1,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm1
+	vpxorq	%xmm1,%xmm6,%xmm6
+	vextracti64x4	$1,%zmm8,%ymm9
+	vpxorq	%ymm9,%ymm8,%ymm8
+	vextracti32x4	$1,%ymm8,%xmm9
+	vpxorq	%xmm9,%xmm8,%xmm8
+	vmovdqa64	POLY2(%rip),%xmm10
+
+
+	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
+	vpslldq	$8,%xmm1,%xmm1
+	vpxorq	%xmm1,%xmm8,%xmm1
+
+
+	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
+	vpsrldq	$4,%xmm9,%xmm9
+	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm6,%xmm9,%xmm2
+
+	subq	$768,%r11
+	je	.L_CALC_AAD_done_vfqnefdewhiBulw
+
+	addq	$768,%r10
+	jmp	.L_get_AAD_loop48x16_vfqnefdewhiBulw
+
+.L_exit_AAD_loop48x16_vfqnefdewhiBulw:
+
+	cmpq	$512,%r11
+	jl	.L_less_than_32x16_vfqnefdewhiBulw
+
+	vmovdqu64	0(%r10),%zmm11
+	vmovdqu64	64(%r10),%zmm3
+	vmovdqu64	128(%r10),%zmm4
+	vmovdqu64	192(%r10),%zmm5
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	testq	%rbx,%rbx
+	jnz	.L_skip_hkeys_precomputation_ihegqDFdlqiolzx
+
+	vmovdqu64	192(%r13),%zmm1
+	vmovdqu64	%zmm1,704(%rsp)
+
+	vmovdqu64	128(%r13),%zmm9
+	vmovdqu64	%zmm9,640(%rsp)
+
+
+	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9
+
+	vmovdqu64	64(%r13),%zmm10
+	vmovdqu64	%zmm10,576(%rsp)
+
+	vmovdqu64	0(%r13),%zmm12
+	vmovdqu64	%zmm12,512(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm10,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm17
+	vpslldq	$8,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
+	vpslldq	$4,%zmm10,%zmm10
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10
+
+	vmovdqu64	%zmm10,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm12,%zmm12
+
+	vpsrldq	$8,%zmm12,%zmm17
+	vpslldq	$8,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
+	vpslldq	$4,%zmm12,%zmm12
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12
+
+	vmovdqu64	%zmm12,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm10,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm17
+	vpslldq	$8,%zmm10,%zmm10
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm10,%zmm10
+
+
+
+	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
+	vpslldq	$4,%zmm10,%zmm10
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10
+
+	vmovdqu64	%zmm10,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
+	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
+	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
+	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm12,%zmm12
+
+	vpsrldq	$8,%zmm12,%zmm17
+	vpslldq	$8,%zmm12,%zmm12
+	vpxorq	%zmm17,%zmm13,%zmm13
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm17
+
+	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
+	vpslldq	$8,%zmm15,%zmm15
+	vpxorq	%zmm15,%zmm12,%zmm12
+
+
+
+	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
+	vpsrldq	$4,%zmm15,%zmm15
+	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
+	vpslldq	$4,%zmm12,%zmm12
+
+	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12
+
+	vmovdqu64	%zmm12,256(%rsp)
+.L_skip_hkeys_precomputation_ihegqDFdlqiolzx:
+	movq	$1,%rbx
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	256(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
+	vmovdqu64	320(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
+	vpxorq	%zmm17,%zmm10,%zmm7
+	vpxorq	%zmm13,%zmm1,%zmm6
+	vpxorq	%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	384(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
+	vmovdqu64	448(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18
+
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	256(%r10),%zmm11
+	vmovdqu64	320(%r10),%zmm3
+	vmovdqu64	384(%r10),%zmm4
+	vmovdqu64	448(%r10),%zmm5
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	vmovdqu64	512(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
+	vmovdqu64	576(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	640(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
+	vmovdqu64	704(%rsp),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18
+
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+
+	vpsrldq	$8,%zmm7,%zmm1
+	vpslldq	$8,%zmm7,%zmm9
+	vpxorq	%zmm1,%zmm6,%zmm6
+	vpxorq	%zmm9,%zmm8,%zmm8
+	vextracti64x4	$1,%zmm6,%ymm1
+	vpxorq	%ymm1,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm1
+	vpxorq	%xmm1,%xmm6,%xmm6
+	vextracti64x4	$1,%zmm8,%ymm9
+	vpxorq	%ymm9,%ymm8,%ymm8
+	vextracti32x4	$1,%ymm8,%xmm9
+	vpxorq	%xmm9,%xmm8,%xmm8
+	vmovdqa64	POLY2(%rip),%xmm10
+
+
+	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
+	vpslldq	$8,%xmm1,%xmm1
+	vpxorq	%xmm1,%xmm8,%xmm1
+
+
+	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
+	vpsrldq	$4,%xmm9,%xmm9
+	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm6,%xmm9,%xmm2
+
+	subq	$512,%r11
+	je	.L_CALC_AAD_done_vfqnefdewhiBulw
+
+	addq	$512,%r10
+	jmp	.L_less_than_16x16_vfqnefdewhiBulw
+
+.L_less_than_32x16_vfqnefdewhiBulw:
+	cmpq	$256,%r11
+	jl	.L_less_than_16x16_vfqnefdewhiBulw
+
+	vmovdqu64	0(%r10),%zmm11
+	vmovdqu64	64(%r10),%zmm3
+	vmovdqu64	128(%r10),%zmm4
+	vmovdqu64	192(%r10),%zmm5
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	0(%r13),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
+	vmovdqu64	64(%r13),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
+	vpxorq	%zmm17,%zmm10,%zmm7
+	vpxorq	%zmm13,%zmm1,%zmm6
+	vpxorq	%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+	vmovdqu64	128(%r13),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
+	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
+	vmovdqu64	192(%r13),%zmm19
+	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
+	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
+	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
+	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18
+
+	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
+	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
+	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
+
+	vpsrldq	$8,%zmm7,%zmm1
+	vpslldq	$8,%zmm7,%zmm9
+	vpxorq	%zmm1,%zmm6,%zmm6
+	vpxorq	%zmm9,%zmm8,%zmm8
+	vextracti64x4	$1,%zmm6,%ymm1
+	vpxorq	%ymm1,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm1
+	vpxorq	%xmm1,%xmm6,%xmm6
+	vextracti64x4	$1,%zmm8,%ymm9
+	vpxorq	%ymm9,%ymm8,%ymm8
+	vextracti32x4	$1,%ymm8,%xmm9
+	vpxorq	%xmm9,%xmm8,%xmm8
+	vmovdqa64	POLY2(%rip),%xmm10
+
+
+	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
+	vpslldq	$8,%xmm1,%xmm1
+	vpxorq	%xmm1,%xmm8,%xmm1
+
+
+	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
+	vpsrldq	$4,%xmm9,%xmm9
+	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm6,%xmm9,%xmm2
+
+	subq	$256,%r11
+	je	.L_CALC_AAD_done_vfqnefdewhiBulw
+
+	addq	$256,%r10
+
+.L_less_than_16x16_vfqnefdewhiBulw:
+
+	leaq	byte64_len_to_mask_table(%rip),%r12
+	leaq	(%r12,%r11,8),%r12
+
+
+	addl	$15,%r11d
+	shrl	$4,%r11d
+	cmpl	$2,%r11d
+	jb	.L_AAD_blocks_1_vfqnefdewhiBulw
+	je	.L_AAD_blocks_2_vfqnefdewhiBulw
+	cmpl	$4,%r11d
+	jb	.L_AAD_blocks_3_vfqnefdewhiBulw
+	je	.L_AAD_blocks_4_vfqnefdewhiBulw
+	cmpl	$6,%r11d
+	jb	.L_AAD_blocks_5_vfqnefdewhiBulw
+	je	.L_AAD_blocks_6_vfqnefdewhiBulw
+	cmpl	$8,%r11d
+	jb	.L_AAD_blocks_7_vfqnefdewhiBulw
+	je	.L_AAD_blocks_8_vfqnefdewhiBulw
+	cmpl	$10,%r11d
+	jb	.L_AAD_blocks_9_vfqnefdewhiBulw
+	je	.L_AAD_blocks_10_vfqnefdewhiBulw
+	cmpl	$12,%r11d
+	jb	.L_AAD_blocks_11_vfqnefdewhiBulw
+	je	.L_AAD_blocks_12_vfqnefdewhiBulw
+	cmpl	$14,%r11d
+	jb	.L_AAD_blocks_13_vfqnefdewhiBulw
+	je	.L_AAD_blocks_14_vfqnefdewhiBulw
+	cmpl	$15,%r11d
+	je	.L_AAD_blocks_15_vfqnefdewhiBulw
+.L_AAD_blocks_16_vfqnefdewhiBulw:
+	subq	$1536,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%zmm4
+	vmovdqu8	192(%r10),%zmm5{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	0(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	64(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vmovdqu64	128(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm9,%zmm11,%zmm1
+	vpternlogq	$0x96,%zmm10,%zmm3,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm12,%zmm11,%zmm7
+	vpternlogq	$0x96,%zmm13,%zmm3,%zmm8
+	vmovdqu64	192(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm13
+	vpxorq	%zmm9,%zmm1,%zmm9
+	vpxorq	%zmm10,%zmm6,%zmm10
+	vpxorq	%zmm12,%zmm7,%zmm12
+	vpxorq	%zmm13,%zmm8,%zmm13
+
+	vpxorq	%zmm13,%zmm12,%zmm12
+	vpsrldq	$8,%zmm12,%zmm7
+	vpslldq	$8,%zmm12,%zmm8
+	vpxorq	%zmm7,%zmm9,%zmm1
+	vpxorq	%zmm8,%zmm10,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_15_vfqnefdewhiBulw:
+	subq	$1536,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%zmm4
+	vmovdqu8	192(%r10),%zmm5{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%zmm16,%zmm5,%zmm5
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	16(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	80(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vmovdqu64	144(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
+	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
+	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
+	vmovdqu64	208(%r13),%ymm15
+	vinserti64x2	$2,240(%r13),%zmm15,%zmm15
+	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm8
+	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_14_vfqnefdewhiBulw:
+	subq	$1536,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%zmm4
+	vmovdqu8	192(%r10),%ymm5{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%ymm16,%ymm5,%ymm5
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	32(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	96(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vmovdqu64	160(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
+	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
+	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
+	vmovdqu64	224(%r13),%ymm15
+	vpclmulqdq	$0x01,%ymm15,%ymm5,%ymm7
+	vpclmulqdq	$0x10,%ymm15,%ymm5,%ymm8
+	vpclmulqdq	$0x11,%ymm15,%ymm5,%ymm1
+	vpclmulqdq	$0x00,%ymm15,%ymm5,%ymm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_13_vfqnefdewhiBulw:
+	subq	$1536,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%zmm4
+	vmovdqu8	192(%r10),%xmm5{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpshufb	%xmm16,%xmm5,%xmm5
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	48(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	112(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vmovdqu64	176(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
+	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
+	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
+	vmovdqu64	240(%r13),%xmm15
+	vpclmulqdq	$0x01,%xmm15,%xmm5,%xmm7
+	vpclmulqdq	$0x10,%xmm15,%xmm5,%xmm8
+	vpclmulqdq	$0x11,%xmm15,%xmm5,%xmm1
+	vpclmulqdq	$0x00,%xmm15,%xmm5,%xmm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_12_vfqnefdewhiBulw:
+	subq	$1024,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%zmm4{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	64(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	128(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vmovdqu64	192(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
+	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
+	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
+	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
+	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
+
+	vpxorq	%zmm13,%zmm12,%zmm12
+	vpsrldq	$8,%zmm12,%zmm7
+	vpslldq	$8,%zmm12,%zmm8
+	vpxorq	%zmm7,%zmm9,%zmm1
+	vpxorq	%zmm8,%zmm10,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_11_vfqnefdewhiBulw:
+	subq	$1024,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%zmm4{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%zmm16,%zmm4,%zmm4
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	80(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	144(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vpxorq	%zmm9,%zmm1,%zmm9
+	vpxorq	%zmm10,%zmm6,%zmm10
+	vpxorq	%zmm12,%zmm7,%zmm12
+	vpxorq	%zmm13,%zmm8,%zmm13
+	vmovdqu64	208(%r13),%ymm15
+	vinserti64x2	$2,240(%r13),%zmm15,%zmm15
+	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm8
+	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_10_vfqnefdewhiBulw:
+	subq	$1024,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%ymm4{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%ymm16,%ymm4,%ymm4
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	96(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	160(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vpxorq	%zmm9,%zmm1,%zmm9
+	vpxorq	%zmm10,%zmm6,%zmm10
+	vpxorq	%zmm12,%zmm7,%zmm12
+	vpxorq	%zmm13,%zmm8,%zmm13
+	vmovdqu64	224(%r13),%ymm15
+	vpclmulqdq	$0x01,%ymm15,%ymm4,%ymm7
+	vpclmulqdq	$0x10,%ymm15,%ymm4,%ymm8
+	vpclmulqdq	$0x11,%ymm15,%ymm4,%ymm1
+	vpclmulqdq	$0x00,%ymm15,%ymm4,%ymm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_9_vfqnefdewhiBulw:
+	subq	$1024,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3
+	vmovdqu8	128(%r10),%xmm4{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpshufb	%xmm16,%xmm4,%xmm4
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	112(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	176(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vpxorq	%zmm9,%zmm1,%zmm9
+	vpxorq	%zmm10,%zmm6,%zmm10
+	vpxorq	%zmm12,%zmm7,%zmm12
+	vpxorq	%zmm13,%zmm8,%zmm13
+	vmovdqu64	240(%r13),%xmm15
+	vpclmulqdq	$0x01,%xmm15,%xmm4,%xmm7
+	vpclmulqdq	$0x10,%xmm15,%xmm4,%xmm8
+	vpclmulqdq	$0x11,%xmm15,%xmm4,%xmm1
+	vpclmulqdq	$0x00,%xmm15,%xmm4,%xmm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_8_vfqnefdewhiBulw:
+	subq	$512,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	128(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vmovdqu64	192(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
+	vpxorq	%zmm9,%zmm1,%zmm9
+	vpxorq	%zmm10,%zmm6,%zmm10
+	vpxorq	%zmm12,%zmm7,%zmm12
+	vpxorq	%zmm13,%zmm8,%zmm13
+
+	vpxorq	%zmm13,%zmm12,%zmm12
+	vpsrldq	$8,%zmm12,%zmm7
+	vpslldq	$8,%zmm12,%zmm8
+	vpxorq	%zmm7,%zmm9,%zmm1
+	vpxorq	%zmm8,%zmm10,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_7_vfqnefdewhiBulw:
+	subq	$512,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%zmm3{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%zmm16,%zmm3,%zmm3
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	144(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
+	vmovdqu64	208(%r13),%ymm15
+	vinserti64x2	$2,240(%r13),%zmm15,%zmm15
+	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm8
+	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_6_vfqnefdewhiBulw:
+	subq	$512,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%ymm3{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%ymm16,%ymm3,%ymm3
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	160(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
+	vmovdqu64	224(%r13),%ymm15
+	vpclmulqdq	$0x01,%ymm15,%ymm3,%ymm7
+	vpclmulqdq	$0x10,%ymm15,%ymm3,%ymm8
+	vpclmulqdq	$0x11,%ymm15,%ymm3,%ymm1
+	vpclmulqdq	$0x00,%ymm15,%ymm3,%ymm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_5_vfqnefdewhiBulw:
+	subq	$512,%r12
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11
+	vmovdqu8	64(%r10),%xmm3{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpshufb	%xmm16,%xmm3,%xmm3
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	176(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
+	vmovdqu64	240(%r13),%xmm15
+	vpclmulqdq	$0x01,%xmm15,%xmm3,%xmm7
+	vpclmulqdq	$0x10,%xmm15,%xmm3,%xmm8
+	vpclmulqdq	$0x11,%xmm15,%xmm3,%xmm1
+	vpclmulqdq	$0x00,%xmm15,%xmm3,%xmm6
+
+	vpxorq	%zmm12,%zmm7,%zmm7
+	vpxorq	%zmm13,%zmm8,%zmm8
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm6,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_4_vfqnefdewhiBulw:
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	192(%r13),%zmm15
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
+
+	vpxorq	%zmm13,%zmm12,%zmm12
+	vpsrldq	$8,%zmm12,%zmm7
+	vpslldq	$8,%zmm12,%zmm8
+	vpxorq	%zmm7,%zmm9,%zmm1
+	vpxorq	%zmm8,%zmm10,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_3_vfqnefdewhiBulw:
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%zmm11{%k1}{z}
+	vpshufb	%zmm16,%zmm11,%zmm11
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	208(%r13),%ymm15
+	vinserti64x2	$2,240(%r13),%zmm15,%zmm15
+	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
+	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
+	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
+	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_2_vfqnefdewhiBulw:
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%ymm11{%k1}{z}
+	vpshufb	%ymm16,%ymm11,%ymm11
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	224(%r13),%ymm15
+	vpclmulqdq	$0x01,%ymm15,%ymm11,%ymm7
+	vpclmulqdq	$0x10,%ymm15,%ymm11,%ymm8
+	vpclmulqdq	$0x11,%ymm15,%ymm11,%ymm1
+	vpclmulqdq	$0x00,%ymm15,%ymm11,%ymm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+	jmp	.L_CALC_AAD_done_vfqnefdewhiBulw
+.L_AAD_blocks_1_vfqnefdewhiBulw:
+	kmovq	(%r12),%k1
+	vmovdqu8	0(%r10),%xmm11{%k1}{z}
+	vpshufb	%xmm16,%xmm11,%xmm11
+	vpxorq	%zmm2,%zmm11,%zmm11
+	vmovdqu64	240(%r13),%xmm15
+	vpclmulqdq	$0x01,%xmm15,%xmm11,%xmm7
+	vpclmulqdq	$0x10,%xmm15,%xmm11,%xmm8
+	vpclmulqdq	$0x11,%xmm15,%xmm11,%xmm1
+	vpclmulqdq	$0x00,%xmm15,%xmm11,%xmm6
+
+	vpxorq	%zmm8,%zmm7,%zmm7
+	vpsrldq	$8,%zmm7,%zmm12
+	vpslldq	$8,%zmm7,%zmm13
+	vpxorq	%zmm12,%zmm1,%zmm1
+	vpxorq	%zmm13,%zmm6,%zmm6
+	vextracti64x4	$1,%zmm1,%ymm12
+	vpxorq	%ymm12,%ymm1,%ymm1
+	vextracti32x4	$1,%ymm1,%xmm12
+	vpxorq	%xmm12,%xmm1,%xmm1
+	vextracti64x4	$1,%zmm6,%ymm13
+	vpxorq	%ymm13,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm13
+	vpxorq	%xmm13,%xmm6,%xmm6
+	vmovdqa64	POLY2(%rip),%xmm15
+
+
+	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
+	vpslldq	$8,%xmm7,%xmm7
+	vpxorq	%xmm7,%xmm6,%xmm7
+
+
+	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
+	vpsrldq	$4,%xmm8,%xmm8
+	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2
+
+.L_CALC_AAD_done_vfqnefdewhiBulw:
+	movq	%rcx,%r10
+	shlq	$3,%r10
+	vmovq	%r10,%xmm3
+
+
+	vpxorq	%xmm2,%xmm3,%xmm2
+
+	vmovdqu64	240(%r13),%xmm1
+
+	vpclmulqdq	$0x11,%xmm1,%xmm2,%xmm11
+	vpclmulqdq	$0x00,%xmm1,%xmm2,%xmm3
+	vpclmulqdq	$0x01,%xmm1,%xmm2,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm2,%xmm2
+	vpxorq	%xmm4,%xmm2,%xmm2
+
+	vpsrldq	$8,%xmm2,%xmm4
+	vpslldq	$8,%xmm2,%xmm2
+	vpxorq	%xmm4,%xmm11,%xmm11
+	vpxorq	%xmm3,%xmm2,%xmm2
+
+
+
+	vmovdqu64	POLY2(%rip),%xmm4
+
+	vpclmulqdq	$0x01,%xmm2,%xmm4,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm2,%xmm2
+
+
+
+	vpclmulqdq	$0x00,%xmm2,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm2,%xmm4,%xmm2
+	vpslldq	$4,%xmm2,%xmm2
+
+	vpternlogq	$0x96,%xmm3,%xmm11,%xmm2
+
+	vpshufb	SHUF_MASK(%rip),%xmm2,%xmm2
+	jmp	skip_iv_len_12_init_IV
+iv_len_12_init_IV:
+
+	vmovdqu8	ONEf(%rip),%xmm2
+	movq	%rdx,%r11
+	movl	$0x0000000000000fff,%r10d
+	kmovq	%r10,%k1
+	vmovdqu8	(%r11),%xmm2{%k1}
+skip_iv_len_12_init_IV:
+	vmovdqu	%xmm2,%xmm1
+
+
+	movl	240(%rdi),%r10d
+	cmpl	$9,%r10d
+	je	.Laes_128_nxtlaqgmFhijgbu
+	cmpl	$11,%r10d
+	je	.Laes_192_nxtlaqgmFhijgbu
+	cmpl	$13,%r10d
+	je	.Laes_256_nxtlaqgmFhijgbu
+	jmp	.Lexit_aes_nxtlaqgmFhijgbu
+.align	32
+.Laes_128_nxtlaqgmFhijgbu:
+	vpxorq	0(%rdi),%xmm1,%xmm1
+
+	vaesenc	16(%rdi),%xmm1,%xmm1
+
+	vaesenc	32(%rdi),%xmm1,%xmm1
+
+	vaesenc	48(%rdi),%xmm1,%xmm1
+
+	vaesenc	64(%rdi),%xmm1,%xmm1
+
+	vaesenc	80(%rdi),%xmm1,%xmm1
+
+	vaesenc	96(%rdi),%xmm1,%xmm1
+
+	vaesenc	112(%rdi),%xmm1,%xmm1
+
+	vaesenc	128(%rdi),%xmm1,%xmm1
+
+	vaesenc	144(%rdi),%xmm1,%xmm1
+
+	vaesenclast	160(%rdi),%xmm1,%xmm1
+	jmp	.Lexit_aes_nxtlaqgmFhijgbu
+.align	32
+.Laes_192_nxtlaqgmFhijgbu:
+	vpxorq	0(%rdi),%xmm1,%xmm1
+
+	vaesenc	16(%rdi),%xmm1,%xmm1
+
+	vaesenc	32(%rdi),%xmm1,%xmm1
+
+	vaesenc	48(%rdi),%xmm1,%xmm1
+
+	vaesenc	64(%rdi),%xmm1,%xmm1
+
+	vaesenc	80(%rdi),%xmm1,%xmm1
+
+	vaesenc	96(%rdi),%xmm1,%xmm1
+
+	vaesenc	112(%rdi),%xmm1,%xmm1
+
+	vaesenc	128(%rdi),%xmm1,%xmm1
+
+	vaesenc	144(%rdi),%xmm1,%xmm1
+
+	vaesenc	160(%rdi),%xmm1,%xmm1
+
+	vaesenc	176(%rdi),%xmm1,%xmm1
+
+	vaesenclast	192(%rdi),%xmm1,%xmm1
+	jmp	.Lexit_aes_nxtlaqgmFhijgbu
+.align	32
+.Laes_256_nxtlaqgmFhijgbu:
+	vpxorq	0(%rdi),%xmm1,%xmm1
+
+	vaesenc	16(%rdi),%xmm1,%xmm1
+
+	vaesenc	32(%rdi),%xmm1,%xmm1
+
+	vaesenc	48(%rdi),%xmm1,%xmm1
+
+	vaesenc	64(%rdi),%xmm1,%xmm1
+
+	vaesenc	80(%rdi),%xmm1,%xmm1
+
+	vaesenc	96(%rdi),%xmm1,%xmm1
+
+	vaesenc	112(%rdi),%xmm1,%xmm1
+
+	vaesenc	128(%rdi),%xmm1,%xmm1
+
+	vaesenc	144(%rdi),%xmm1,%xmm1
+
+	vaesenc	160(%rdi),%xmm1,%xmm1
+
+	vaesenc	176(%rdi),%xmm1,%xmm1
+
+	vaesenc	192(%rdi),%xmm1,%xmm1
+
+	vaesenc	208(%rdi),%xmm1,%xmm1
+
+	vaesenclast	224(%rdi),%xmm1,%xmm1
+	jmp	.Lexit_aes_nxtlaqgmFhijgbu
+.Lexit_aes_nxtlaqgmFhijgbu:
+
+	vmovdqu	%xmm1,32(%rsi)
+
+
+	vpshufb	SHUF_MASK(%rip),%xmm2,%xmm2
+	vmovdqu	%xmm2,0(%rsi)
+.Lexit_setiv:
+	cmpq	$256,%rcx
+	jbe	.Lskip_hkeys_cleanup_cFvEjphiypBBbdd
+	vpxor	%xmm0,%xmm0,%xmm0
+	vmovdqa64	%zmm0,0(%rsp)
+	vmovdqa64	%zmm0,64(%rsp)
+	vmovdqa64	%zmm0,128(%rsp)
+	vmovdqa64	%zmm0,192(%rsp)
+	vmovdqa64	%zmm0,256(%rsp)
+	vmovdqa64	%zmm0,320(%rsp)
+	vmovdqa64	%zmm0,384(%rsp)
+	vmovdqa64	%zmm0,448(%rsp)
+	vmovdqa64	%zmm0,512(%rsp)
+	vmovdqa64	%zmm0,576(%rsp)
+	vmovdqa64	%zmm0,640(%rsp)
+	vmovdqa64	%zmm0,704(%rsp)
+.Lskip_hkeys_cleanup_cFvEjphiypBBbdd:
+	vzeroupper
+	leaq	(%rbp),%rsp
+.cfi_def_cfa_register	%rsp
+	popq	%r15
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r15
+	popq	%r14
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r14
+	popq	%r13
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r13
+	popq	%r12
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r12
+	popq	%rbp
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%rbp
+	popq	%rbx
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%rbx
+	.byte	0xf3,0xc3
+.Lsetiv_seh_end:
+.cfi_endproc	
+.size	gcm_setiv_avx512, .-gcm_setiv_avx512
+.globl	aes_gcm_encrypt_avx512
+.hidden aes_gcm_encrypt_avx512
+.hidden	aes_gcm_encrypt_avx512
+.type	aes_gcm_encrypt_avx512,@function
+.align	32
+aes_gcm_encrypt_avx512:
+.cfi_startproc	
+.Lencrypt_seh_begin:
+#ifdef BORINGSSL_DISPATCH_TEST
+
+	movb	$1,BORINGSSL_function_hit+6(%rip)
+#endif
+.byte	243,15,30,250
+	pushq	%rbx
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%rbx,-16
+.Lencrypt_seh_push_rbx:
+	pushq	%rbp
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%rbp,-24
+.Lencrypt_seh_push_rbp:
+	pushq	%r12
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r12,-32
+.Lencrypt_seh_push_r12:
+	pushq	%r13
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r13,-40
+.Lencrypt_seh_push_r13:
+	pushq	%r14
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r14,-48
+.Lencrypt_seh_push_r14:
+	pushq	%r15
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r15,-56
+.Lencrypt_seh_push_r15:
+
+
+
+
+
+
+
+
+
+
+	leaq	0(%rsp),%rbp
+.cfi_def_cfa_register	%rbp
+.Lencrypt_seh_setfp:
+
+.Lencrypt_seh_prolog_end:
+	subq	$1588,%rsp
+	andq	$(-64),%rsp
+
+
+	movl	240(%rdi),%eax
+	cmpl	$9,%eax
+	je	.Laes_gcm_encrypt_128_avx512
+	cmpl	$11,%eax
+	je	.Laes_gcm_encrypt_192_avx512
+	cmpl	$13,%eax
+	je	.Laes_gcm_encrypt_256_avx512
+	xorl	%eax,%eax
+	jmp	.Lexit_gcm_encrypt
+.align	32
+.Laes_gcm_encrypt_128_avx512:
+	orq	%r8,%r8
+	je	.L_enc_dec_abort_hsbsCDpixjzhgcA
+	xorq	%r14,%r14
+	vmovdqu64	64(%rsi),%xmm14
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+
+	movl	(%rdx),%eax
+	orq	%rax,%rax
+	je	.L_partial_block_done_thyBfwiuCtyonss
+	movl	$16,%r10d
+	leaq	byte_len_to_mask_table(%rip),%r12
+	cmpq	%r10,%r8
+	cmovcq	%r8,%r10
+	kmovw	(%r12,%r10,2),%k1
+	vmovdqu8	(%rcx),%xmm0{%k1}{z}
+
+	vmovdqu64	16(%rsi),%xmm3
+
+	leaq	96(%rsi),%r10
+	vmovdqu64	240(%r10),%xmm4
+
+
+
+	leaq	SHIFT_MASK(%rip),%r12
+	addq	%rax,%r12
+	vmovdqu64	(%r12),%xmm5
+	vpshufb	%xmm5,%xmm3,%xmm3
+	vpxorq	%xmm0,%xmm3,%xmm3
+
+
+	leaq	(%r8,%rax,1),%r13
+	subq	$16,%r13
+	jge	.L_no_extra_mask_thyBfwiuCtyonss
+	subq	%r13,%r12
+.L_no_extra_mask_thyBfwiuCtyonss:
+
+
+
+	vmovdqu64	16(%r12),%xmm0
+	vpand	%xmm0,%xmm3,%xmm3
+	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
+	vpshufb	%xmm5,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm14,%xmm14
+	cmpq	$0,%r13
+	jl	.L_partial_incomplete_thyBfwiuCtyonss
+
+	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
+	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
+	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
+	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
+	vpxorq	%xmm11,%xmm14,%xmm14
+
+	vpsrldq	$8,%xmm14,%xmm11
+	vpslldq	$8,%xmm14,%xmm14
+	vpxorq	%xmm11,%xmm7,%xmm7
+	vpxorq	%xmm10,%xmm14,%xmm14
+
+
+
+	vmovdqu64	POLY2(%rip),%xmm11
+
+	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
+	vpslldq	$8,%xmm10,%xmm10
+	vpxorq	%xmm10,%xmm14,%xmm14
+
+
+
+	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
+	vpsrldq	$4,%xmm10,%xmm10
+	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+
+	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14
+
+	movl	$0,(%rdx)
+
+	movq	%rax,%r12
+	movq	$16,%rax
+	subq	%r12,%rax
+	jmp	.L_enc_dec_done_thyBfwiuCtyonss
+
+.L_partial_incomplete_thyBfwiuCtyonss:
+	addl	%r8d,(%rdx)
+	movq	%r8,%rax
+
+.L_enc_dec_done_thyBfwiuCtyonss:
+
+
+	leaq	byte_len_to_mask_table(%rip),%r12
+	kmovw	(%r12,%rax,2),%k1
+
+	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
+	vpshufb	%xmm5,%xmm3,%xmm3
+	movq	%r9,%r12
+	vmovdqu8	%xmm3,(%r12){%k1}
+.L_partial_block_done_thyBfwiuCtyonss:
+	vmovdqu64	0(%rsi),%xmm2
+	subq	%rax,%r8
+	je	.L_enc_dec_done_hsbsCDpixjzhgcA
+	cmpq	$256,%r8
+	jbe	.L_message_below_equal_16_blocks_hsbsCDpixjzhgcA
+
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
+	vmovdqa64	ddq_addbe_1234(%rip),%zmm28
+
+
+
+
+
+
+	vmovd	%xmm2,%r15d
+	andl	$255,%r15d
+
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpshufb	%zmm29,%zmm2,%zmm2
+
+
+
+	cmpb	$240,%r15b
+	jae	.L_next_16_overflow_rAvwBFyjevnwsxA
+	vpaddd	%zmm28,%zmm2,%zmm7
+	vpaddd	%zmm27,%zmm7,%zmm10
+	vpaddd	%zmm27,%zmm10,%zmm11
+	vpaddd	%zmm27,%zmm11,%zmm12
+	jmp	.L_next_16_ok_rAvwBFyjevnwsxA
+.L_next_16_overflow_rAvwBFyjevnwsxA:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm12
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
+	vpaddd	%zmm12,%zmm7,%zmm10
+	vpaddd	%zmm12,%zmm10,%zmm11
+	vpaddd	%zmm12,%zmm11,%zmm12
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+.L_next_16_ok_rAvwBFyjevnwsxA:
+	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
+	addb	$16,%r15b
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm0
+	vmovdqu8	64(%rcx,%rax,1),%zmm3
+	vmovdqu8	128(%rcx,%rax,1),%zmm4
+	vmovdqu8	192(%rcx,%rax,1),%zmm5
+
+
+	vbroadcastf64x2	0(%rdi),%zmm6
+	vpxorq	%zmm6,%zmm7,%zmm7
+	vpxorq	%zmm6,%zmm10,%zmm10
+	vpxorq	%zmm6,%zmm11,%zmm11
+	vpxorq	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	16(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	32(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	48(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	64(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	80(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	96(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	112(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	128(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	144(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	160(%rdi),%zmm6
+	vaesenclast	%zmm6,%zmm7,%zmm7
+	vaesenclast	%zmm6,%zmm10,%zmm10
+	vaesenclast	%zmm6,%zmm11,%zmm11
+	vaesenclast	%zmm6,%zmm12,%zmm12
+
+
+	vpxorq	%zmm0,%zmm7,%zmm7
+	vpxorq	%zmm3,%zmm10,%zmm10
+	vpxorq	%zmm4,%zmm11,%zmm11
+	vpxorq	%zmm5,%zmm12,%zmm12
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm7,0(%r10,%rax,1)
+	vmovdqu8	%zmm10,64(%r10,%rax,1)
+	vmovdqu8	%zmm11,128(%r10,%rax,1)
+	vmovdqu8	%zmm12,192(%r10,%rax,1)
+
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+	vmovdqa64	%zmm7,768(%rsp)
+	vmovdqa64	%zmm10,832(%rsp)
+	vmovdqa64	%zmm11,896(%rsp)
+	vmovdqa64	%zmm12,960(%rsp)
+	leaq	96(%rsi),%r12
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_aiuuvfqszdmmDmu
+
+	vmovdqu64	192(%r12),%zmm0
+	vmovdqu64	%zmm0,704(%rsp)
+
+	vmovdqu64	128(%r12),%zmm3
+	vmovdqu64	%zmm3,640(%rsp)
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	64(%r12),%zmm4
+	vmovdqu64	%zmm4,576(%rsp)
+
+	vmovdqu64	0(%r12),%zmm5
+	vmovdqu64	%zmm5,512(%rsp)
+.L_skip_hkeys_precomputation_aiuuvfqszdmmDmu:
+	cmpq	$512,%r8
+	jb	.L_message_below_32_blocks_hsbsCDpixjzhgcA
+
+
+
+	cmpb	$240,%r15b
+	jae	.L_next_16_overflow_inCurFmBDAABDii
+	vpaddd	%zmm28,%zmm2,%zmm7
+	vpaddd	%zmm27,%zmm7,%zmm10
+	vpaddd	%zmm27,%zmm10,%zmm11
+	vpaddd	%zmm27,%zmm11,%zmm12
+	jmp	.L_next_16_ok_inCurFmBDAABDii
+.L_next_16_overflow_inCurFmBDAABDii:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm12
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
+	vpaddd	%zmm12,%zmm7,%zmm10
+	vpaddd	%zmm12,%zmm10,%zmm11
+	vpaddd	%zmm12,%zmm11,%zmm12
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+.L_next_16_ok_inCurFmBDAABDii:
+	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
+	addb	$16,%r15b
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm0
+	vmovdqu8	320(%rcx,%rax,1),%zmm3
+	vmovdqu8	384(%rcx,%rax,1),%zmm4
+	vmovdqu8	448(%rcx,%rax,1),%zmm5
+
+
+	vbroadcastf64x2	0(%rdi),%zmm6
+	vpxorq	%zmm6,%zmm7,%zmm7
+	vpxorq	%zmm6,%zmm10,%zmm10
+	vpxorq	%zmm6,%zmm11,%zmm11
+	vpxorq	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	16(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	32(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	48(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	64(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	80(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	96(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	112(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	128(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	144(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	160(%rdi),%zmm6
+	vaesenclast	%zmm6,%zmm7,%zmm7
+	vaesenclast	%zmm6,%zmm10,%zmm10
+	vaesenclast	%zmm6,%zmm11,%zmm11
+	vaesenclast	%zmm6,%zmm12,%zmm12
+
+
+	vpxorq	%zmm0,%zmm7,%zmm7
+	vpxorq	%zmm3,%zmm10,%zmm10
+	vpxorq	%zmm4,%zmm11,%zmm11
+	vpxorq	%zmm5,%zmm12,%zmm12
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm7,256(%r10,%rax,1)
+	vmovdqu8	%zmm10,320(%r10,%rax,1)
+	vmovdqu8	%zmm11,384(%r10,%rax,1)
+	vmovdqu8	%zmm12,448(%r10,%rax,1)
+
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+	vmovdqa64	%zmm7,1024(%rsp)
+	vmovdqa64	%zmm10,1088(%rsp)
+	vmovdqa64	%zmm11,1152(%rsp)
+	vmovdqa64	%zmm12,1216(%rsp)
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_FrcajbChebCGCzu
+	vmovdqu64	640(%rsp),%zmm3
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	576(%rsp),%zmm4
+	vmovdqu64	512(%rsp),%zmm5
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,256(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,192(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,128(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,64(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,0(%rsp)
+.L_skip_hkeys_precomputation_FrcajbChebCGCzu:
+	movq	$1,%r14
+	addq	$512,%rax
+	subq	$512,%r8
+
+	cmpq	$768,%r8
+	jb	.L_no_more_big_nblocks_hsbsCDpixjzhgcA
+.L_encrypt_big_nblocks_hsbsCDpixjzhgcA:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_EpuczClpbbjueeh
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_EpuczClpbbjueeh
+.L_16_blocks_overflow_EpuczClpbbjueeh:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_EpuczClpbbjueeh:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_yDslhFpEEcfhgiz
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_yDslhFpEEcfhgiz
+.L_16_blocks_overflow_yDslhFpEEcfhgiz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_yDslhFpEEcfhgiz:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	256(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	320(%rsp),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	384(%rsp),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	448(%rsp),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm17
+	vmovdqu8	320(%rcx,%rax,1),%zmm19
+	vmovdqu8	384(%rcx,%rax,1),%zmm20
+	vmovdqu8	448(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,256(%r10,%rax,1)
+	vmovdqu8	%zmm3,320(%r10,%rax,1)
+	vmovdqu8	%zmm4,384(%r10,%rax,1)
+	vmovdqu8	%zmm5,448(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,768(%rsp)
+	vmovdqa64	%zmm3,832(%rsp)
+	vmovdqa64	%zmm4,896(%rsp)
+	vmovdqa64	%zmm5,960(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_huuyFAfzoxtrlcj
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_huuyFAfzoxtrlcj
+.L_16_blocks_overflow_huuyFAfzoxtrlcj:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_huuyFAfzoxtrlcj:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	512(%rcx,%rax,1),%zmm17
+	vmovdqu8	576(%rcx,%rax,1),%zmm19
+	vmovdqu8	640(%rcx,%rax,1),%zmm20
+	vmovdqu8	704(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+
+
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
+	vpxorq	%zmm24,%zmm6,%zmm6
+	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
+	vpxorq	%zmm25,%zmm7,%zmm7
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vextracti64x4	$1,%zmm6,%ymm12
+	vpxorq	%ymm12,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm12
+	vpxorq	%xmm12,%xmm6,%xmm6
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,512(%r10,%rax,1)
+	vmovdqu8	%zmm3,576(%r10,%rax,1)
+	vmovdqu8	%zmm4,640(%r10,%rax,1)
+	vmovdqu8	%zmm5,704(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,1024(%rsp)
+	vmovdqa64	%zmm3,1088(%rsp)
+	vmovdqa64	%zmm4,1152(%rsp)
+	vmovdqa64	%zmm5,1216(%rsp)
+	vmovdqa64	%zmm6,%zmm14
+
+	addq	$768,%rax
+	subq	$768,%r8
+	cmpq	$768,%r8
+	jae	.L_encrypt_big_nblocks_hsbsCDpixjzhgcA
+
+.L_no_more_big_nblocks_hsbsCDpixjzhgcA:
+
+	cmpq	$512,%r8
+	jae	.L_encrypt_32_blocks_hsbsCDpixjzhgcA
+
+	cmpq	$256,%r8
+	jae	.L_encrypt_16_blocks_hsbsCDpixjzhgcA
+.L_encrypt_0_blocks_ghash_32_hsbsCDpixjzhgcA:
+	movl	%r8d,%r10d
+	andl	$~15,%r10d
+	movl	$256,%ebx
+	subl	%r10d,%ebx
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	addl	$256,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_yAGhwpoujyiFmhc
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_yAGhwpoujyiFmhc
+	jb	.L_last_num_blocks_is_7_1_yAGhwpoujyiFmhc
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_yAGhwpoujyiFmhc
+	jb	.L_last_num_blocks_is_11_9_yAGhwpoujyiFmhc
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_yAGhwpoujyiFmhc
+	ja	.L_last_num_blocks_is_16_yAGhwpoujyiFmhc
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_yAGhwpoujyiFmhc
+	jmp	.L_last_num_blocks_is_13_yAGhwpoujyiFmhc
+
+.L_last_num_blocks_is_11_9_yAGhwpoujyiFmhc:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_yAGhwpoujyiFmhc
+	ja	.L_last_num_blocks_is_11_yAGhwpoujyiFmhc
+	jmp	.L_last_num_blocks_is_9_yAGhwpoujyiFmhc
+
+.L_last_num_blocks_is_7_1_yAGhwpoujyiFmhc:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_yAGhwpoujyiFmhc
+	jb	.L_last_num_blocks_is_3_1_yAGhwpoujyiFmhc
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_yAGhwpoujyiFmhc
+	je	.L_last_num_blocks_is_6_yAGhwpoujyiFmhc
+	jmp	.L_last_num_blocks_is_5_yAGhwpoujyiFmhc
+
+.L_last_num_blocks_is_3_1_yAGhwpoujyiFmhc:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_yAGhwpoujyiFmhc
+	je	.L_last_num_blocks_is_2_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_1_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_ztvDkbfcjwbyoda
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_ztvDkbfcjwbyoda
+
+.L_16_blocks_overflow_ztvDkbfcjwbyoda:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_ztvDkbfcjwbyoda:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_obcyprEbbfjsqou
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_obcyprEbbfjsqou
+.L_small_initial_partial_block_obcyprEbbfjsqou:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_obcyprEbbfjsqou
+.L_small_initial_compute_done_obcyprEbbfjsqou:
+.L_after_reduction_obcyprEbbfjsqou:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_2_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_giDteqAcmbtopqE
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_giDteqAcmbtopqE
+
+.L_16_blocks_overflow_giDteqAcmbtopqE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_giDteqAcmbtopqE:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_budbscabjscvdyy
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_budbscabjscvdyy
+.L_small_initial_partial_block_budbscabjscvdyy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_budbscabjscvdyy:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_budbscabjscvdyy
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_budbscabjscvdyy:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_3_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_DFznkcDuyzbxojm
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_DFznkcDuyzbxojm
+
+.L_16_blocks_overflow_DFznkcDuyzbxojm:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_DFznkcDuyzbxojm:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_nDkeesBszssFBAB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_nDkeesBszssFBAB
+.L_small_initial_partial_block_nDkeesBszssFBAB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_nDkeesBszssFBAB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_nDkeesBszssFBAB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_nDkeesBszssFBAB:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_4_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_GxvCujCqEmqCrbm
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_GxvCujCqEmqCrbm
+
+.L_16_blocks_overflow_GxvCujCqEmqCrbm:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_GxvCujCqEmqCrbm:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_bEGtrfolssqnGvg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_bEGtrfolssqnGvg
+.L_small_initial_partial_block_bEGtrfolssqnGvg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_bEGtrfolssqnGvg:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_bEGtrfolssqnGvg
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_bEGtrfolssqnGvg:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_5_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_iEDAhhDvvgeEiBk
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_iEDAhhDvvgeEiBk
+
+.L_16_blocks_overflow_iEDAhhDvvgeEiBk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_iEDAhhDvvgeEiBk:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%xmm29,%xmm3,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BGxorynBidvadcf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BGxorynBidvadcf
+.L_small_initial_partial_block_BGxorynBidvadcf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BGxorynBidvadcf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BGxorynBidvadcf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_BGxorynBidvadcf:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_6_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_krdAtGBmtADdxkd
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_krdAtGBmtADdxkd
+
+.L_16_blocks_overflow_krdAtGBmtADdxkd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_krdAtGBmtADdxkd:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%ymm29,%ymm3,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AymxoBkCdEkCaDn
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AymxoBkCdEkCaDn
+.L_small_initial_partial_block_AymxoBkCdEkCaDn:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AymxoBkCdEkCaDn:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AymxoBkCdEkCaDn
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_AymxoBkCdEkCaDn:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_7_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_zjkbadctnAxFbBf
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_zjkbadctnAxFbBf
+
+.L_16_blocks_overflow_zjkbadctnAxFbBf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_zjkbadctnAxFbBf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_rorgegwgmFfbdgz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_rorgegwgmFfbdgz
+.L_small_initial_partial_block_rorgegwgmFfbdgz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_rorgegwgmFfbdgz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_rorgegwgmFfbdgz
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_rorgegwgmFfbdgz:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_8_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_vjDzeaBksfdjqda
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_vjDzeaBksfdjqda
+
+.L_16_blocks_overflow_vjDzeaBksfdjqda:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_vjDzeaBksfdjqda:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BEsmmDchcrGiGjD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BEsmmDchcrGiGjD
+.L_small_initial_partial_block_BEsmmDchcrGiGjD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BEsmmDchcrGiGjD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BEsmmDchcrGiGjD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_BEsmmDchcrGiGjD:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_9_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_fBbFlGogmmjFtmp
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_fBbFlGogmmjFtmp
+
+.L_16_blocks_overflow_fBbFlGogmmjFtmp:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_fBbFlGogmmjFtmp:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%xmm29,%xmm4,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_FldankniBuhwDrk
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_FldankniBuhwDrk
+.L_small_initial_partial_block_FldankniBuhwDrk:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_FldankniBuhwDrk:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_FldankniBuhwDrk
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_FldankniBuhwDrk:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_10_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_odeletvsEdcpDof
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_odeletvsEdcpDof
+
+.L_16_blocks_overflow_odeletvsEdcpDof:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_odeletvsEdcpDof:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%ymm29,%ymm4,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_hqfmyycwpuAtlcC
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_hqfmyycwpuAtlcC
+.L_small_initial_partial_block_hqfmyycwpuAtlcC:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_hqfmyycwpuAtlcC:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_hqfmyycwpuAtlcC
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_hqfmyycwpuAtlcC:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_11_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_jFgfmfkifwdBsCv
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_jFgfmfkifwdBsCv
+
+.L_16_blocks_overflow_jFgfmfkifwdBsCv:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_jFgfmfkifwdBsCv:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ykdqrBelAvlBcCB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ykdqrBelAvlBcCB
+.L_small_initial_partial_block_ykdqrBelAvlBcCB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ykdqrBelAvlBcCB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ykdqrBelAvlBcCB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ykdqrBelAvlBcCB:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_12_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_FlovvBuecaDFxmd
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_FlovvBuecaDFxmd
+
+.L_16_blocks_overflow_FlovvBuecaDFxmd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_FlovvBuecaDFxmd:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_bvbCbpCGFtagbEc
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_bvbCbpCGFtagbEc
+.L_small_initial_partial_block_bvbCbpCGFtagbEc:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_bvbCbpCGFtagbEc:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_bvbCbpCGFtagbEc
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_bvbCbpCGFtagbEc:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_13_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_rmilhduAhukjqlC
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_rmilhduAhukjqlC
+
+.L_16_blocks_overflow_rmilhduAhukjqlC:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_rmilhduAhukjqlC:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%xmm29,%xmm5,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_qkvBqmfGokBlsfG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_qkvBqmfGokBlsfG
+.L_small_initial_partial_block_qkvBqmfGokBlsfG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_qkvBqmfGokBlsfG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_qkvBqmfGokBlsfG
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_qkvBqmfGokBlsfG:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_14_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_BertbhkkzbfDpAy
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_BertbhkkzbfDpAy
+
+.L_16_blocks_overflow_BertbhkkzbfDpAy:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_BertbhkkzbfDpAy:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%ymm29,%ymm5,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_xlpmBvuofFlEEcx
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_xlpmBvuofFlEEcx
+.L_small_initial_partial_block_xlpmBvuofFlEEcx:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_xlpmBvuofFlEEcx:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_xlpmBvuofFlEEcx
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_xlpmBvuofFlEEcx:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_15_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_pleDFxtafCcrfrp
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_pleDFxtafCcrfrp
+
+.L_16_blocks_overflow_pleDFxtafCcrfrp:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_pleDFxtafCcrfrp:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_emgdDqoDebqdoka
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_emgdDqoDebqdoka
+.L_small_initial_partial_block_emgdDqoDebqdoka:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_emgdDqoDebqdoka:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_emgdDqoDebqdoka
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_emgdDqoDebqdoka:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_16_yAGhwpoujyiFmhc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_pqopypyBcnAqgef
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_pqopypyBcnAqgef
+
+.L_16_blocks_overflow_pqopypyBcnAqgef:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_pqopypyBcnAqgef:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_ufmnqdwdxczzBqA:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ufmnqdwdxczzBqA:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ufmnqdwdxczzBqA:
+	jmp	.L_last_blocks_done_yAGhwpoujyiFmhc
+.L_last_num_blocks_is_0_yAGhwpoujyiFmhc:
+	vmovdqa64	1024(%rsp),%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1088(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1152(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1216(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_yAGhwpoujyiFmhc:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_hsbsCDpixjzhgcA
+.L_encrypt_32_blocks_hsbsCDpixjzhgcA:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_DoCxkfCsqtylqbp
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_DoCxkfCsqtylqbp
+.L_16_blocks_overflow_DoCxkfCsqtylqbp:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_DoCxkfCsqtylqbp:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_nbnwjtAtbiragkD
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_nbnwjtAtbiragkD
+.L_16_blocks_overflow_nbnwjtAtbiragkD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_nbnwjtAtbiragkD:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	256(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	320(%rsp),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	384(%rsp),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	448(%rsp),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm17
+	vmovdqu8	320(%rcx,%rax,1),%zmm19
+	vmovdqu8	384(%rcx,%rax,1),%zmm20
+	vmovdqu8	448(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,256(%r10,%rax,1)
+	vmovdqu8	%zmm3,320(%r10,%rax,1)
+	vmovdqu8	%zmm4,384(%r10,%rax,1)
+	vmovdqu8	%zmm5,448(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,768(%rsp)
+	vmovdqa64	%zmm3,832(%rsp)
+	vmovdqa64	%zmm4,896(%rsp)
+	vmovdqa64	%zmm5,960(%rsp)
+	vmovdqa64	1280(%rsp),%zmm13
+	vmovdqu64	512(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1344(%rsp),%zmm13
+	vmovdqu64	576(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1408(%rsp),%zmm13
+	vmovdqu64	640(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1472(%rsp),%zmm13
+	vmovdqu64	704(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+	subq	$512,%r8
+	addq	$512,%rax
+	movl	%r8d,%r10d
+	andl	$~15,%r10d
+	movl	$512,%ebx
+	subl	%r10d,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_xqzzzizlxhrCCqc
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_xqzzzizlxhrCCqc
+	jb	.L_last_num_blocks_is_7_1_xqzzzizlxhrCCqc
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_xqzzzizlxhrCCqc
+	jb	.L_last_num_blocks_is_11_9_xqzzzizlxhrCCqc
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_xqzzzizlxhrCCqc
+	ja	.L_last_num_blocks_is_16_xqzzzizlxhrCCqc
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_xqzzzizlxhrCCqc
+	jmp	.L_last_num_blocks_is_13_xqzzzizlxhrCCqc
+
+.L_last_num_blocks_is_11_9_xqzzzizlxhrCCqc:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_xqzzzizlxhrCCqc
+	ja	.L_last_num_blocks_is_11_xqzzzizlxhrCCqc
+	jmp	.L_last_num_blocks_is_9_xqzzzizlxhrCCqc
+
+.L_last_num_blocks_is_7_1_xqzzzizlxhrCCqc:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_xqzzzizlxhrCCqc
+	jb	.L_last_num_blocks_is_3_1_xqzzzizlxhrCCqc
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_xqzzzizlxhrCCqc
+	je	.L_last_num_blocks_is_6_xqzzzizlxhrCCqc
+	jmp	.L_last_num_blocks_is_5_xqzzzizlxhrCCqc
+
+.L_last_num_blocks_is_3_1_xqzzzizlxhrCCqc:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_xqzzzizlxhrCCqc
+	je	.L_last_num_blocks_is_2_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_1_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_eqrxnqjpvktxmxs
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_eqrxnqjpvktxmxs
+
+.L_16_blocks_overflow_eqrxnqjpvktxmxs:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_eqrxnqjpvktxmxs:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_xDkiukhhxmpydke
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_xDkiukhhxmpydke
+.L_small_initial_partial_block_xDkiukhhxmpydke:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_xDkiukhhxmpydke
+.L_small_initial_compute_done_xDkiukhhxmpydke:
+.L_after_reduction_xDkiukhhxmpydke:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_2_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_CirlCuEenxrqvcF
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_CirlCuEenxrqvcF
+
+.L_16_blocks_overflow_CirlCuEenxrqvcF:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_CirlCuEenxrqvcF:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_zdyhAqtdlihBeEA
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_zdyhAqtdlihBeEA
+.L_small_initial_partial_block_zdyhAqtdlihBeEA:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zdyhAqtdlihBeEA:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_zdyhAqtdlihBeEA
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_zdyhAqtdlihBeEA:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_3_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_DwtmgxAlfstCish
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_DwtmgxAlfstCish
+
+.L_16_blocks_overflow_DwtmgxAlfstCish:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_DwtmgxAlfstCish:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CfcvewDycErwkBr
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CfcvewDycErwkBr
+.L_small_initial_partial_block_CfcvewDycErwkBr:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CfcvewDycErwkBr:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CfcvewDycErwkBr
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CfcvewDycErwkBr:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_4_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_zdEzefEhiwAqjpf
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_zdEzefEhiwAqjpf
+
+.L_16_blocks_overflow_zdEzefEhiwAqjpf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_zdEzefEhiwAqjpf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_uzsctAfaoCDzaEj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_uzsctAfaoCDzaEj
+.L_small_initial_partial_block_uzsctAfaoCDzaEj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_uzsctAfaoCDzaEj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_uzsctAfaoCDzaEj
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_uzsctAfaoCDzaEj:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_5_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_ejnkocdeCFrdcgk
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_ejnkocdeCFrdcgk
+
+.L_16_blocks_overflow_ejnkocdeCFrdcgk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_ejnkocdeCFrdcgk:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%xmm29,%xmm3,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_rhBhrebnybkClob
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_rhBhrebnybkClob
+.L_small_initial_partial_block_rhBhrebnybkClob:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_rhBhrebnybkClob:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_rhBhrebnybkClob
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_rhBhrebnybkClob:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_6_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_wAmAkkbnwxneuhh
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_wAmAkkbnwxneuhh
+
+.L_16_blocks_overflow_wAmAkkbnwxneuhh:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_wAmAkkbnwxneuhh:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%ymm29,%ymm3,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_iGnyszChjqGffvi
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_iGnyszChjqGffvi
+.L_small_initial_partial_block_iGnyszChjqGffvi:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_iGnyszChjqGffvi:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_iGnyszChjqGffvi
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_iGnyszChjqGffvi:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_7_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_mjimimbaAjDegFD
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_mjimimbaAjDegFD
+
+.L_16_blocks_overflow_mjimimbaAjDegFD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_mjimimbaAjDegFD:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_auyodGekljvqnal
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_auyodGekljvqnal
+.L_small_initial_partial_block_auyodGekljvqnal:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_auyodGekljvqnal:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_auyodGekljvqnal
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_auyodGekljvqnal:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_8_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_jjyaGzdicqaCmif
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_jjyaGzdicqaCmif
+
+.L_16_blocks_overflow_jjyaGzdicqaCmif:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_jjyaGzdicqaCmif:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tsDipkywukdsdEq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tsDipkywukdsdEq
+.L_small_initial_partial_block_tsDipkywukdsdEq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tsDipkywukdsdEq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tsDipkywukdsdEq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tsDipkywukdsdEq:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_9_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_dhFfAqpBExgwAuz
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_dhFfAqpBExgwAuz
+
+.L_16_blocks_overflow_dhFfAqpBExgwAuz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_dhFfAqpBExgwAuz:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%xmm29,%xmm4,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_DajctzvxCcgikkg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_DajctzvxCcgikkg
+.L_small_initial_partial_block_DajctzvxCcgikkg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_DajctzvxCcgikkg:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_DajctzvxCcgikkg
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_DajctzvxCcgikkg:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_10_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_oBhdzalbaDdmGbn
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_oBhdzalbaDdmGbn
+
+.L_16_blocks_overflow_oBhdzalbaDdmGbn:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_oBhdzalbaDdmGbn:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%ymm29,%ymm4,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_uAGwxtBBrnpgrAc
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_uAGwxtBBrnpgrAc
+.L_small_initial_partial_block_uAGwxtBBrnpgrAc:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_uAGwxtBBrnpgrAc:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_uAGwxtBBrnpgrAc
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_uAGwxtBBrnpgrAc:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_11_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_oDouFkiCynbdwgC
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_oDouFkiCynbdwgC
+
+.L_16_blocks_overflow_oDouFkiCynbdwgC:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_oDouFkiCynbdwgC:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_uAhbrGojzkrhdku
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_uAhbrGojzkrhdku
+.L_small_initial_partial_block_uAhbrGojzkrhdku:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_uAhbrGojzkrhdku:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_uAhbrGojzkrhdku
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_uAhbrGojzkrhdku:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_12_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_swmlxbFClGvwobc
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_swmlxbFClGvwobc
+
+.L_16_blocks_overflow_swmlxbFClGvwobc:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_swmlxbFClGvwobc:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_DmGrxjnsstpvlAx
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_DmGrxjnsstpvlAx
+.L_small_initial_partial_block_DmGrxjnsstpvlAx:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_DmGrxjnsstpvlAx:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_DmGrxjnsstpvlAx
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_DmGrxjnsstpvlAx:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_13_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_FyenBnDvgsnqtzb
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_FyenBnDvgsnqtzb
+
+.L_16_blocks_overflow_FyenBnDvgsnqtzb:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_FyenBnDvgsnqtzb:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%xmm29,%xmm5,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_gobyBacqtDnwDyq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_gobyBacqtDnwDyq
+.L_small_initial_partial_block_gobyBacqtDnwDyq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_gobyBacqtDnwDyq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_gobyBacqtDnwDyq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_gobyBacqtDnwDyq:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_14_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_ypEbdEdFEvhguGl
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_ypEbdEdFEvhguGl
+
+.L_16_blocks_overflow_ypEbdEdFEvhguGl:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_ypEbdEdFEvhguGl:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%ymm29,%ymm5,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_faFsqzglCsupyoE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_faFsqzglCsupyoE
+.L_small_initial_partial_block_faFsqzglCsupyoE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_faFsqzglCsupyoE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_faFsqzglCsupyoE
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_faFsqzglCsupyoE:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_15_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_qBtmCznxEfirjwq
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_qBtmCznxEfirjwq
+
+.L_16_blocks_overflow_qBtmCznxEfirjwq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_qBtmCznxEfirjwq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_gwkpifwjlbhqkkq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_gwkpifwjlbhqkkq
+.L_small_initial_partial_block_gwkpifwjlbhqkkq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_gwkpifwjlbhqkkq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_gwkpifwjlbhqkkq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_gwkpifwjlbhqkkq:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_16_xqzzzizlxhrCCqc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_EfBtvBbxlnatgck
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_EfBtvBbxlnatgck
+
+.L_16_blocks_overflow_EfBtvBbxlnatgck:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_EfBtvBbxlnatgck:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_btwksyGdGqvxAqv:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_btwksyGdGqvxAqv:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_btwksyGdGqvxAqv:
+	jmp	.L_last_blocks_done_xqzzzizlxhrCCqc
+.L_last_num_blocks_is_0_xqzzzizlxhrCCqc:
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_xqzzzizlxhrCCqc:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_hsbsCDpixjzhgcA
+.L_encrypt_16_blocks_hsbsCDpixjzhgcA:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_nmfoEmwakBfegfz
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_nmfoEmwakBfegfz
+.L_16_blocks_overflow_nmfoEmwakBfegfz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_nmfoEmwakBfegfz:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	vmovdqa64	1024(%rsp),%zmm13
+	vmovdqu64	256(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1088(%rsp),%zmm13
+	vmovdqu64	320(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1152(%rsp),%zmm13
+	vmovdqu64	384(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1216(%rsp),%zmm13
+	vmovdqu64	448(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	subq	$256,%r8
+	addq	$256,%rax
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_gDmlymcewxciwsg
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_gDmlymcewxciwsg
+	jb	.L_last_num_blocks_is_7_1_gDmlymcewxciwsg
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_gDmlymcewxciwsg
+	jb	.L_last_num_blocks_is_11_9_gDmlymcewxciwsg
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_gDmlymcewxciwsg
+	ja	.L_last_num_blocks_is_16_gDmlymcewxciwsg
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_gDmlymcewxciwsg
+	jmp	.L_last_num_blocks_is_13_gDmlymcewxciwsg
+
+.L_last_num_blocks_is_11_9_gDmlymcewxciwsg:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_gDmlymcewxciwsg
+	ja	.L_last_num_blocks_is_11_gDmlymcewxciwsg
+	jmp	.L_last_num_blocks_is_9_gDmlymcewxciwsg
+
+.L_last_num_blocks_is_7_1_gDmlymcewxciwsg:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_gDmlymcewxciwsg
+	jb	.L_last_num_blocks_is_3_1_gDmlymcewxciwsg
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_gDmlymcewxciwsg
+	je	.L_last_num_blocks_is_6_gDmlymcewxciwsg
+	jmp	.L_last_num_blocks_is_5_gDmlymcewxciwsg
+
+.L_last_num_blocks_is_3_1_gDmlymcewxciwsg:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_gDmlymcewxciwsg
+	je	.L_last_num_blocks_is_2_gDmlymcewxciwsg
+.L_last_num_blocks_is_1_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_oeqeuhitagtjiyd
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_oeqeuhitagtjiyd
+
+.L_16_blocks_overflow_oeqeuhitagtjiyd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_oeqeuhitagtjiyd:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AGahuogrchiGsEo
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AGahuogrchiGsEo
+.L_small_initial_partial_block_AGahuogrchiGsEo:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_AGahuogrchiGsEo
+.L_small_initial_compute_done_AGahuogrchiGsEo:
+.L_after_reduction_AGahuogrchiGsEo:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_2_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_wDfGcfGGDCzEozA
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_wDfGcfGGDCzEozA
+
+.L_16_blocks_overflow_wDfGcfGGDCzEozA:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_wDfGcfGGDCzEozA:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_cshmlFgCmGuucfg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_cshmlFgCmGuucfg
+.L_small_initial_partial_block_cshmlFgCmGuucfg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_cshmlFgCmGuucfg:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_cshmlFgCmGuucfg
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_cshmlFgCmGuucfg:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_3_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_nBmDlilklGtazGe
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_nBmDlilklGtazGe
+
+.L_16_blocks_overflow_nBmDlilklGtazGe:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_nBmDlilklGtazGe:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_EhDtjcgBCydivjB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_EhDtjcgBCydivjB
+.L_small_initial_partial_block_EhDtjcgBCydivjB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_EhDtjcgBCydivjB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_EhDtjcgBCydivjB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_EhDtjcgBCydivjB:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_4_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_EtwmfFamwzGnDBc
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_EtwmfFamwzGnDBc
+
+.L_16_blocks_overflow_EtwmfFamwzGnDBc:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_EtwmfFamwzGnDBc:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_qCjcgElfdofDAoF
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_qCjcgElfdofDAoF
+.L_small_initial_partial_block_qCjcgElfdofDAoF:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_qCjcgElfdofDAoF:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_qCjcgElfdofDAoF
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_qCjcgElfdofDAoF:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_5_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_nDeumdzmAzDbrzi
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_nDeumdzmAzDbrzi
+
+.L_16_blocks_overflow_nDeumdzmAzDbrzi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_nDeumdzmAzDbrzi:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%xmm29,%xmm3,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_oorjmpcfpurCBGr
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_oorjmpcfpurCBGr
+.L_small_initial_partial_block_oorjmpcfpurCBGr:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_oorjmpcfpurCBGr:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_oorjmpcfpurCBGr
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_oorjmpcfpurCBGr:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_6_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_sCAngfhqDotbFmw
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_sCAngfhqDotbFmw
+
+.L_16_blocks_overflow_sCAngfhqDotbFmw:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_sCAngfhqDotbFmw:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%ymm29,%ymm3,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tiwsEpggzxmllaj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tiwsEpggzxmllaj
+.L_small_initial_partial_block_tiwsEpggzxmllaj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tiwsEpggzxmllaj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tiwsEpggzxmllaj
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tiwsEpggzxmllaj:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_7_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_nnnluFvuayFjeCz
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_nnnluFvuayFjeCz
+
+.L_16_blocks_overflow_nnnluFvuayFjeCz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_nnnluFvuayFjeCz:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GbtoDCzbymodlAi
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GbtoDCzbymodlAi
+.L_small_initial_partial_block_GbtoDCzbymodlAi:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GbtoDCzbymodlAi:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GbtoDCzbymodlAi
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GbtoDCzbymodlAi:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_8_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_mAbufzFFCqAipyc
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_mAbufzFFCqAipyc
+
+.L_16_blocks_overflow_mAbufzFFCqAipyc:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_mAbufzFFCqAipyc:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_DixcDwtmGugxcaE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_DixcDwtmGugxcaE
+.L_small_initial_partial_block_DixcDwtmGugxcaE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_DixcDwtmGugxcaE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_DixcDwtmGugxcaE
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_DixcDwtmGugxcaE:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_9_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_fecArFAvspAkvhz
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_fecArFAvspAkvhz
+
+.L_16_blocks_overflow_fecArFAvspAkvhz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_fecArFAvspAkvhz:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%xmm29,%xmm4,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_hychuvCczflwblm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_hychuvCczflwblm
+.L_small_initial_partial_block_hychuvCczflwblm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_hychuvCczflwblm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_hychuvCczflwblm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_hychuvCczflwblm:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_10_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_GopsnDjeiqpErcm
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_GopsnDjeiqpErcm
+
+.L_16_blocks_overflow_GopsnDjeiqpErcm:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_GopsnDjeiqpErcm:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%ymm29,%ymm4,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_nBBredbEFceterz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_nBBredbEFceterz
+.L_small_initial_partial_block_nBBredbEFceterz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_nBBredbEFceterz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_nBBredbEFceterz
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_nBBredbEFceterz:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_11_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_fiwrrgaEkigvzgi
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_fiwrrgaEkigvzgi
+
+.L_16_blocks_overflow_fiwrrgaEkigvzgi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_fiwrrgaEkigvzgi:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_fFhCqDkbnEeqyjy
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_fFhCqDkbnEeqyjy
+.L_small_initial_partial_block_fFhCqDkbnEeqyjy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_fFhCqDkbnEeqyjy:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_fFhCqDkbnEeqyjy
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_fFhCqDkbnEeqyjy:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_12_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_xqDvthaiFDvnByi
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_xqDvthaiFDvnByi
+
+.L_16_blocks_overflow_xqDvthaiFDvnByi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_xqDvthaiFDvnByi:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_rFhaBCebiFufrtu
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_rFhaBCebiFufrtu
+.L_small_initial_partial_block_rFhaBCebiFufrtu:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_rFhaBCebiFufrtu:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_rFhaBCebiFufrtu
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_rFhaBCebiFufrtu:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_13_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_cGzlrgzfiFDzphE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_cGzlrgzfiFDzphE
+
+.L_16_blocks_overflow_cGzlrgzfiFDzphE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_cGzlrgzfiFDzphE:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%xmm29,%xmm5,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ziDpzBvBdmrajkj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ziDpzBvBdmrajkj
+.L_small_initial_partial_block_ziDpzBvBdmrajkj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ziDpzBvBdmrajkj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ziDpzBvBdmrajkj
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ziDpzBvBdmrajkj:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_14_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_FblyetwpzrADaka
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_FblyetwpzrADaka
+
+.L_16_blocks_overflow_FblyetwpzrADaka:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_FblyetwpzrADaka:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%ymm29,%ymm5,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_rgplghgpAisjCbk
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_rgplghgpAisjCbk
+.L_small_initial_partial_block_rgplghgpAisjCbk:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_rgplghgpAisjCbk:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_rgplghgpAisjCbk
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_rgplghgpAisjCbk:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_15_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_uDAnvBmteDnAgub
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_uDAnvBmteDnAgub
+
+.L_16_blocks_overflow_uDAnvBmteDnAgub:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_uDAnvBmteDnAgub:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tjgEedEpusGBbEe
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tjgEedEpusGBbEe
+.L_small_initial_partial_block_tjgEedEpusGBbEe:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tjgEedEpusGBbEe:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tjgEedEpusGBbEe
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tjgEedEpusGBbEe:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_16_gDmlymcewxciwsg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_oqsBvbrnDujAapn
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_oqsBvbrnDujAapn
+
+.L_16_blocks_overflow_oqsBvbrnDujAapn:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_oqsBvbrnDujAapn:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_GcxbyvfGqcEGmAG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GcxbyvfGqcEGmAG:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GcxbyvfGqcEGmAG:
+	jmp	.L_last_blocks_done_gDmlymcewxciwsg
+.L_last_num_blocks_is_0_gDmlymcewxciwsg:
+	vmovdqa64	1280(%rsp),%zmm13
+	vmovdqu64	512(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1344(%rsp),%zmm13
+	vmovdqu64	576(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1408(%rsp),%zmm13
+	vmovdqu64	640(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1472(%rsp),%zmm13
+	vmovdqu64	704(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_gDmlymcewxciwsg:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_hsbsCDpixjzhgcA
+
+.L_message_below_32_blocks_hsbsCDpixjzhgcA:
+
+
+	subq	$256,%r8
+	addq	$256,%rax
+	movl	%r8d,%r10d
+	leaq	96(%rsi),%r12
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_lgzodirsvtxhDar
+	vmovdqu64	640(%rsp),%zmm3
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	576(%rsp),%zmm4
+	vmovdqu64	512(%rsp),%zmm5
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,256(%rsp)
+.L_skip_hkeys_precomputation_lgzodirsvtxhDar:
+	movq	$1,%r14
+	andl	$~15,%r10d
+	movl	$512,%ebx
+	subl	%r10d,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_nfwjedpkwciAtlC
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_nfwjedpkwciAtlC
+	jb	.L_last_num_blocks_is_7_1_nfwjedpkwciAtlC
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_nfwjedpkwciAtlC
+	jb	.L_last_num_blocks_is_11_9_nfwjedpkwciAtlC
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_nfwjedpkwciAtlC
+	ja	.L_last_num_blocks_is_16_nfwjedpkwciAtlC
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_nfwjedpkwciAtlC
+	jmp	.L_last_num_blocks_is_13_nfwjedpkwciAtlC
+
+.L_last_num_blocks_is_11_9_nfwjedpkwciAtlC:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_nfwjedpkwciAtlC
+	ja	.L_last_num_blocks_is_11_nfwjedpkwciAtlC
+	jmp	.L_last_num_blocks_is_9_nfwjedpkwciAtlC
+
+.L_last_num_blocks_is_7_1_nfwjedpkwciAtlC:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_nfwjedpkwciAtlC
+	jb	.L_last_num_blocks_is_3_1_nfwjedpkwciAtlC
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_nfwjedpkwciAtlC
+	je	.L_last_num_blocks_is_6_nfwjedpkwciAtlC
+	jmp	.L_last_num_blocks_is_5_nfwjedpkwciAtlC
+
+.L_last_num_blocks_is_3_1_nfwjedpkwciAtlC:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_nfwjedpkwciAtlC
+	je	.L_last_num_blocks_is_2_nfwjedpkwciAtlC
+.L_last_num_blocks_is_1_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_jvqmepjrBGfzywz
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_jvqmepjrBGfzywz
+
+.L_16_blocks_overflow_jvqmepjrBGfzywz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_jvqmepjrBGfzywz:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_bCmotyhCdqcGdmx
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_bCmotyhCdqcGdmx
+.L_small_initial_partial_block_bCmotyhCdqcGdmx:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_bCmotyhCdqcGdmx
+.L_small_initial_compute_done_bCmotyhCdqcGdmx:
+.L_after_reduction_bCmotyhCdqcGdmx:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_2_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_lgsyvxbCpomwpBd
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_lgsyvxbCpomwpBd
+
+.L_16_blocks_overflow_lgsyvxbCpomwpBd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_lgsyvxbCpomwpBd:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CvohcmmanzcGuww
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CvohcmmanzcGuww
+.L_small_initial_partial_block_CvohcmmanzcGuww:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CvohcmmanzcGuww:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CvohcmmanzcGuww
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CvohcmmanzcGuww:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_3_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_yfFzEqCajezjjEG
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_yfFzEqCajezjjEG
+
+.L_16_blocks_overflow_yfFzEqCajezjjEG:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_yfFzEqCajezjjEG:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ieoCEAgsExoomlB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ieoCEAgsExoomlB
+.L_small_initial_partial_block_ieoCEAgsExoomlB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ieoCEAgsExoomlB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ieoCEAgsExoomlB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ieoCEAgsExoomlB:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_4_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_lhdskhxsnzbqiCE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_lhdskhxsnzbqiCE
+
+.L_16_blocks_overflow_lhdskhxsnzbqiCE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_lhdskhxsnzbqiCE:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_fsgjexrhnDAEroB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_fsgjexrhnDAEroB
+.L_small_initial_partial_block_fsgjexrhnDAEroB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_fsgjexrhnDAEroB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_fsgjexrhnDAEroB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_fsgjexrhnDAEroB:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_5_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_ifmdtlxmbiuncpi
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_ifmdtlxmbiuncpi
+
+.L_16_blocks_overflow_ifmdtlxmbiuncpi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_ifmdtlxmbiuncpi:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%xmm29,%xmm3,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_eEexgttbvnGtvAn
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_eEexgttbvnGtvAn
+.L_small_initial_partial_block_eEexgttbvnGtvAn:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_eEexgttbvnGtvAn:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_eEexgttbvnGtvAn
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_eEexgttbvnGtvAn:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_6_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_DioifFiechlvDDi
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_DioifFiechlvDDi
+
+.L_16_blocks_overflow_DioifFiechlvDDi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_DioifFiechlvDDi:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%ymm29,%ymm3,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_zmCaqublfDfomxD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_zmCaqublfDfomxD
+.L_small_initial_partial_block_zmCaqublfDfomxD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zmCaqublfDfomxD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_zmCaqublfDfomxD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_zmCaqublfDfomxD:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_7_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_wcuvkqelusurbal
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_wcuvkqelusurbal
+
+.L_16_blocks_overflow_wcuvkqelusurbal:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_wcuvkqelusurbal:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tcfGzCpDmckghrw
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tcfGzCpDmckghrw
+.L_small_initial_partial_block_tcfGzCpDmckghrw:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tcfGzCpDmckghrw:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tcfGzCpDmckghrw
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tcfGzCpDmckghrw:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_8_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_bErEfxtmxjfBdGv
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_bErEfxtmxjfBdGv
+
+.L_16_blocks_overflow_bErEfxtmxjfBdGv:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_bErEfxtmxjfBdGv:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_cpcsvaagFbiCesn
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_cpcsvaagFbiCesn
+.L_small_initial_partial_block_cpcsvaagFbiCesn:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_cpcsvaagFbiCesn:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_cpcsvaagFbiCesn
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_cpcsvaagFbiCesn:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_9_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_dGrDGjpblDgcAzF
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_dGrDGjpblDgcAzF
+
+.L_16_blocks_overflow_dGrDGjpblDgcAzF:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_dGrDGjpblDgcAzF:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%xmm29,%xmm4,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ehxEzfvfFolDlDd
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ehxEzfvfFolDlDd
+.L_small_initial_partial_block_ehxEzfvfFolDlDd:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ehxEzfvfFolDlDd:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ehxEzfvfFolDlDd
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ehxEzfvfFolDlDd:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_10_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_igbiBGvgbxbCBdj
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_igbiBGvgbxbCBdj
+
+.L_16_blocks_overflow_igbiBGvgbxbCBdj:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_igbiBGvgbxbCBdj:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%ymm29,%ymm4,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_fjcilpiCvEcynrn
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_fjcilpiCvEcynrn
+.L_small_initial_partial_block_fjcilpiCvEcynrn:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_fjcilpiCvEcynrn:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_fjcilpiCvEcynrn
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_fjcilpiCvEcynrn:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_11_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_kjbfDuobDFkCfkD
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_kjbfDuobDFkCfkD
+
+.L_16_blocks_overflow_kjbfDuobDFkCfkD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_kjbfDuobDFkCfkD:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_EbdxyyChfuosrxp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_EbdxyyChfuosrxp
+.L_small_initial_partial_block_EbdxyyChfuosrxp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_EbdxyyChfuosrxp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_EbdxyyChfuosrxp
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_EbdxyyChfuosrxp:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_12_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_jjrxmmrCfadufdx
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_jjrxmmrCfadufdx
+
+.L_16_blocks_overflow_jjrxmmrCfadufdx:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_jjrxmmrCfadufdx:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tsmwkeCDishyjGx
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tsmwkeCDishyjGx
+.L_small_initial_partial_block_tsmwkeCDishyjGx:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tsmwkeCDishyjGx:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tsmwkeCDishyjGx
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tsmwkeCDishyjGx:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_13_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_oudducjjnnfokjr
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_oudducjjnnfokjr
+
+.L_16_blocks_overflow_oudducjjnnfokjr:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_oudducjjnnfokjr:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%xmm29,%xmm5,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_phFfDyqExEtaxrp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_phFfDyqExEtaxrp
+.L_small_initial_partial_block_phFfDyqExEtaxrp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_phFfDyqExEtaxrp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_phFfDyqExEtaxrp
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_phFfDyqExEtaxrp:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_14_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_xmfmjCdtGeqrEah
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_xmfmjCdtGeqrEah
+
+.L_16_blocks_overflow_xmfmjCdtGeqrEah:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_xmfmjCdtGeqrEah:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%ymm29,%ymm5,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_xrBznaCnFrsnbgA
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_xrBznaCnFrsnbgA
+.L_small_initial_partial_block_xrBznaCnFrsnbgA:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_xrBznaCnFrsnbgA:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_xrBznaCnFrsnbgA
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_xrBznaCnFrsnbgA:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_15_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_kvyDfoitcoxyBFh
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_kvyDfoitcoxyBFh
+
+.L_16_blocks_overflow_kvyDfoitcoxyBFh:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_kvyDfoitcoxyBFh:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dzdpGrjBglxotAD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dzdpGrjBglxotAD
+.L_small_initial_partial_block_dzdpGrjBglxotAD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_dzdpGrjBglxotAD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_dzdpGrjBglxotAD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_dzdpGrjBglxotAD:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_16_nfwjedpkwciAtlC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_esjFjFrzAxvgrxu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_esjFjFrzAxvgrxu
+
+.L_16_blocks_overflow_esjFjFrzAxvgrxu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_esjFjFrzAxvgrxu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_CFgbvhiihpCsmmC:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CFgbvhiihpCsmmC:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CFgbvhiihpCsmmC:
+	jmp	.L_last_blocks_done_nfwjedpkwciAtlC
+.L_last_num_blocks_is_0_nfwjedpkwciAtlC:
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_nfwjedpkwciAtlC:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_hsbsCDpixjzhgcA
+
+.L_message_below_equal_16_blocks_hsbsCDpixjzhgcA:
+
+
+	movl	%r8d,%r12d
+	addl	$15,%r12d
+	shrl	$4,%r12d
+	cmpq	$8,%r12
+	je	.L_small_initial_num_blocks_is_8_vzoGymsmevnxrwm
+	jl	.L_small_initial_num_blocks_is_7_1_vzoGymsmevnxrwm
+
+
+	cmpq	$12,%r12
+	je	.L_small_initial_num_blocks_is_12_vzoGymsmevnxrwm
+	jl	.L_small_initial_num_blocks_is_11_9_vzoGymsmevnxrwm
+
+
+	cmpq	$16,%r12
+	je	.L_small_initial_num_blocks_is_16_vzoGymsmevnxrwm
+	cmpq	$15,%r12
+	je	.L_small_initial_num_blocks_is_15_vzoGymsmevnxrwm
+	cmpq	$14,%r12
+	je	.L_small_initial_num_blocks_is_14_vzoGymsmevnxrwm
+	jmp	.L_small_initial_num_blocks_is_13_vzoGymsmevnxrwm
+
+.L_small_initial_num_blocks_is_11_9_vzoGymsmevnxrwm:
+
+	cmpq	$11,%r12
+	je	.L_small_initial_num_blocks_is_11_vzoGymsmevnxrwm
+	cmpq	$10,%r12
+	je	.L_small_initial_num_blocks_is_10_vzoGymsmevnxrwm
+	jmp	.L_small_initial_num_blocks_is_9_vzoGymsmevnxrwm
+
+.L_small_initial_num_blocks_is_7_1_vzoGymsmevnxrwm:
+	cmpq	$4,%r12
+	je	.L_small_initial_num_blocks_is_4_vzoGymsmevnxrwm
+	jl	.L_small_initial_num_blocks_is_3_1_vzoGymsmevnxrwm
+
+	cmpq	$7,%r12
+	je	.L_small_initial_num_blocks_is_7_vzoGymsmevnxrwm
+	cmpq	$6,%r12
+	je	.L_small_initial_num_blocks_is_6_vzoGymsmevnxrwm
+	jmp	.L_small_initial_num_blocks_is_5_vzoGymsmevnxrwm
+
+.L_small_initial_num_blocks_is_3_1_vzoGymsmevnxrwm:
+
+	cmpq	$3,%r12
+	je	.L_small_initial_num_blocks_is_3_vzoGymsmevnxrwm
+	cmpq	$2,%r12
+	je	.L_small_initial_num_blocks_is_2_vzoGymsmevnxrwm
+
+
+
+
+
+.L_small_initial_num_blocks_is_1_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%xmm29
+	vpaddd	ONE(%rip),%xmm2,%xmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vpshufb	%xmm29,%xmm0,%xmm0
+	vmovdqu8	0(%rcx,%rax,1),%xmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%xmm15,%xmm0,%xmm0
+	vpxorq	%xmm6,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm6
+	vextracti32x4	$0,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_eGeArriAxhloFln
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_eGeArriAxhloFln
+.L_small_initial_partial_block_eGeArriAxhloFln:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm13,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_eGeArriAxhloFln
+.L_small_initial_compute_done_eGeArriAxhloFln:
+.L_after_reduction_eGeArriAxhloFln:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_2_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%ymm29
+	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
+	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vpshufb	%ymm29,%ymm0,%ymm0
+	vmovdqu8	0(%rcx,%rax,1),%ymm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%ymm15,%ymm0,%ymm0
+	vpxorq	%ymm6,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm6
+	vextracti32x4	$1,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AuAGDlpmuwflfqd
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AuAGDlpmuwflfqd
+.L_small_initial_partial_block_AuAGDlpmuwflfqd:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AuAGDlpmuwflfqd:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AuAGDlpmuwflfqd
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_AuAGDlpmuwflfqd:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_3_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vextracti32x4	$2,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_abAldCqawgDhmzi
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_abAldCqawgDhmzi
+.L_small_initial_partial_block_abAldCqawgDhmzi:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_abAldCqawgDhmzi:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_abAldCqawgDhmzi
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_abAldCqawgDhmzi:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_4_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vextracti32x4	$3,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_cDBrgAGymuBkbvr
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_cDBrgAGymuBkbvr
+.L_small_initial_partial_block_cDBrgAGymuBkbvr:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_cDBrgAGymuBkbvr:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_cDBrgAGymuBkbvr
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_cDBrgAGymuBkbvr:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_5_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%xmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%xmm15,%xmm3,%xmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%xmm7,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%xmm29,%xmm3,%xmm7
+	vextracti32x4	$0,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CwflsnofjzjecaE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CwflsnofjzjecaE
+.L_small_initial_partial_block_CwflsnofjzjecaE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CwflsnofjzjecaE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CwflsnofjzjecaE
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_CwflsnofjzjecaE:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_6_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%ymm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%ymm15,%ymm3,%ymm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%ymm7,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%ymm29,%ymm3,%ymm7
+	vextracti32x4	$1,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ktpCsqDfwFbxxFg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ktpCsqDfwFbxxFg
+.L_small_initial_partial_block_ktpCsqDfwFbxxFg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ktpCsqDfwFbxxFg:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ktpCsqDfwFbxxFg
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_ktpCsqDfwFbxxFg:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_7_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vextracti32x4	$2,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CxkdniryrGjaFrj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CxkdniryrGjaFrj
+.L_small_initial_partial_block_CxkdniryrGjaFrj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CxkdniryrGjaFrj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CxkdniryrGjaFrj
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_CxkdniryrGjaFrj:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_8_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vextracti32x4	$3,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ziDAmjpiwDnuyqm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ziDAmjpiwDnuyqm
+.L_small_initial_partial_block_ziDAmjpiwDnuyqm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ziDAmjpiwDnuyqm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ziDAmjpiwDnuyqm
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_ziDAmjpiwDnuyqm:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_9_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%xmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%xmm15,%xmm4,%xmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%xmm10,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%xmm29,%xmm4,%xmm10
+	vextracti32x4	$0,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_mAAeqxtkoFgiAgq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_mAAeqxtkoFgiAgq
+.L_small_initial_partial_block_mAAeqxtkoFgiAgq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_mAAeqxtkoFgiAgq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_mAAeqxtkoFgiAgq
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_mAAeqxtkoFgiAgq:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_10_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%ymm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%ymm15,%ymm4,%ymm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%ymm10,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%ymm29,%ymm4,%ymm10
+	vextracti32x4	$1,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vtcuufeppsyzEqq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vtcuufeppsyzEqq
+.L_small_initial_partial_block_vtcuufeppsyzEqq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vtcuufeppsyzEqq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vtcuufeppsyzEqq
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_vtcuufeppsyzEqq:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_11_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vextracti32x4	$2,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GghpkhDnaBsrAun
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GghpkhDnaBsrAun
+.L_small_initial_partial_block_GghpkhDnaBsrAun:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GghpkhDnaBsrAun:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GghpkhDnaBsrAun
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_GghpkhDnaBsrAun:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_12_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vextracti32x4	$3,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ktDiyaFyqCGdEay
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	64(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ktDiyaFyqCGdEay
+.L_small_initial_partial_block_ktDiyaFyqCGdEay:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ktDiyaFyqCGdEay:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ktDiyaFyqCGdEay
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_ktDiyaFyqCGdEay:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_13_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%xmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%xmm15,%xmm5,%xmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%xmm11,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vpshufb	%xmm29,%xmm5,%xmm11
+	vextracti32x4	$0,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ckBgAbwpxhlsGmp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	48(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ckBgAbwpxhlsGmp
+.L_small_initial_partial_block_ckBgAbwpxhlsGmp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	64(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ckBgAbwpxhlsGmp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ckBgAbwpxhlsGmp
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_ckBgAbwpxhlsGmp:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_14_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%ymm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%ymm15,%ymm5,%ymm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%ymm11,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vpshufb	%ymm29,%ymm5,%ymm11
+	vextracti32x4	$1,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_swCGlfjodadzucj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	32(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_swCGlfjodadzucj
+.L_small_initial_partial_block_swCGlfjodadzucj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	48(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_swCGlfjodadzucj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_swCGlfjodadzucj
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_swCGlfjodadzucj:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_15_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%zmm15,%zmm5,%zmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%zmm11,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vpshufb	%zmm29,%zmm5,%zmm11
+	vextracti32x4	$2,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_mhqvDwjirzthgEp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	16(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_mhqvDwjirzthgEp
+.L_small_initial_partial_block_mhqvDwjirzthgEp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	32(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_mhqvDwjirzthgEp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_mhqvDwjirzthgEp
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_mhqvDwjirzthgEp:
+	jmp	.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm
+.L_small_initial_num_blocks_is_16_vzoGymsmevnxrwm:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%zmm15,%zmm5,%zmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%zmm11,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vpshufb	%zmm29,%zmm5,%zmm11
+	vextracti32x4	$3,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_sFnrddbgkwfrpiG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	16(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_sFnrddbgkwfrpiG:
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_sFnrddbgkwfrpiG:
+.L_small_initial_blocks_encrypted_vzoGymsmevnxrwm:
+.L_ghash_done_hsbsCDpixjzhgcA:
+	vmovdqu64	%xmm2,0(%rsi)
+.L_enc_dec_done_hsbsCDpixjzhgcA:
+
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+	vmovdqu64	%xmm14,64(%rsi)
+.L_enc_dec_abort_hsbsCDpixjzhgcA:
+	jmp	.Lexit_gcm_encrypt
+.align	32
+.Laes_gcm_encrypt_192_avx512:
+	orq	%r8,%r8
+	je	.L_enc_dec_abort_vGFiuFCrdtyEdmj
+	xorq	%r14,%r14
+	vmovdqu64	64(%rsi),%xmm14
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+
+	movl	(%rdx),%eax
+	orq	%rax,%rax
+	je	.L_partial_block_done_Grlydabgcvojtzq
+	movl	$16,%r10d
+	leaq	byte_len_to_mask_table(%rip),%r12
+	cmpq	%r10,%r8
+	cmovcq	%r8,%r10
+	kmovw	(%r12,%r10,2),%k1
+	vmovdqu8	(%rcx),%xmm0{%k1}{z}
+
+	vmovdqu64	16(%rsi),%xmm3
+
+	leaq	96(%rsi),%r10
+	vmovdqu64	240(%r10),%xmm4
+
+
+
+	leaq	SHIFT_MASK(%rip),%r12
+	addq	%rax,%r12
+	vmovdqu64	(%r12),%xmm5
+	vpshufb	%xmm5,%xmm3,%xmm3
+	vpxorq	%xmm0,%xmm3,%xmm3
+
+
+	leaq	(%r8,%rax,1),%r13
+	subq	$16,%r13
+	jge	.L_no_extra_mask_Grlydabgcvojtzq
+	subq	%r13,%r12
+.L_no_extra_mask_Grlydabgcvojtzq:
+
+
+
+	vmovdqu64	16(%r12),%xmm0
+	vpand	%xmm0,%xmm3,%xmm3
+	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
+	vpshufb	%xmm5,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm14,%xmm14
+	cmpq	$0,%r13
+	jl	.L_partial_incomplete_Grlydabgcvojtzq
+
+	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
+	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
+	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
+	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
+	vpxorq	%xmm11,%xmm14,%xmm14
+
+	vpsrldq	$8,%xmm14,%xmm11
+	vpslldq	$8,%xmm14,%xmm14
+	vpxorq	%xmm11,%xmm7,%xmm7
+	vpxorq	%xmm10,%xmm14,%xmm14
+
+
+
+	vmovdqu64	POLY2(%rip),%xmm11
+
+	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
+	vpslldq	$8,%xmm10,%xmm10
+	vpxorq	%xmm10,%xmm14,%xmm14
+
+
+
+	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
+	vpsrldq	$4,%xmm10,%xmm10
+	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+
+	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14
+
+	movl	$0,(%rdx)
+
+	movq	%rax,%r12
+	movq	$16,%rax
+	subq	%r12,%rax
+	jmp	.L_enc_dec_done_Grlydabgcvojtzq
+
+.L_partial_incomplete_Grlydabgcvojtzq:
+	addl	%r8d,(%rdx)
+	movq	%r8,%rax
+
+.L_enc_dec_done_Grlydabgcvojtzq:
+
+
+	leaq	byte_len_to_mask_table(%rip),%r12
+	kmovw	(%r12,%rax,2),%k1
+
+	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
+	vpshufb	%xmm5,%xmm3,%xmm3
+	movq	%r9,%r12
+	vmovdqu8	%xmm3,(%r12){%k1}
+.L_partial_block_done_Grlydabgcvojtzq:
+	vmovdqu64	0(%rsi),%xmm2
+	subq	%rax,%r8
+	je	.L_enc_dec_done_vGFiuFCrdtyEdmj
+	cmpq	$256,%r8
+	jbe	.L_message_below_equal_16_blocks_vGFiuFCrdtyEdmj
+
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
+	vmovdqa64	ddq_addbe_1234(%rip),%zmm28
+
+
+
+
+
+
+	vmovd	%xmm2,%r15d
+	andl	$255,%r15d
+
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpshufb	%zmm29,%zmm2,%zmm2
+
+
+
+	cmpb	$240,%r15b
+	jae	.L_next_16_overflow_zDzqlxmoasDvBwi
+	vpaddd	%zmm28,%zmm2,%zmm7
+	vpaddd	%zmm27,%zmm7,%zmm10
+	vpaddd	%zmm27,%zmm10,%zmm11
+	vpaddd	%zmm27,%zmm11,%zmm12
+	jmp	.L_next_16_ok_zDzqlxmoasDvBwi
+.L_next_16_overflow_zDzqlxmoasDvBwi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm12
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
+	vpaddd	%zmm12,%zmm7,%zmm10
+	vpaddd	%zmm12,%zmm10,%zmm11
+	vpaddd	%zmm12,%zmm11,%zmm12
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+.L_next_16_ok_zDzqlxmoasDvBwi:
+	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
+	addb	$16,%r15b
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm0
+	vmovdqu8	64(%rcx,%rax,1),%zmm3
+	vmovdqu8	128(%rcx,%rax,1),%zmm4
+	vmovdqu8	192(%rcx,%rax,1),%zmm5
+
+
+	vbroadcastf64x2	0(%rdi),%zmm6
+	vpxorq	%zmm6,%zmm7,%zmm7
+	vpxorq	%zmm6,%zmm10,%zmm10
+	vpxorq	%zmm6,%zmm11,%zmm11
+	vpxorq	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	16(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	32(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	48(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	64(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	80(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	96(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	112(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	128(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	144(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	160(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	176(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	192(%rdi),%zmm6
+	vaesenclast	%zmm6,%zmm7,%zmm7
+	vaesenclast	%zmm6,%zmm10,%zmm10
+	vaesenclast	%zmm6,%zmm11,%zmm11
+	vaesenclast	%zmm6,%zmm12,%zmm12
+
+
+	vpxorq	%zmm0,%zmm7,%zmm7
+	vpxorq	%zmm3,%zmm10,%zmm10
+	vpxorq	%zmm4,%zmm11,%zmm11
+	vpxorq	%zmm5,%zmm12,%zmm12
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm7,0(%r10,%rax,1)
+	vmovdqu8	%zmm10,64(%r10,%rax,1)
+	vmovdqu8	%zmm11,128(%r10,%rax,1)
+	vmovdqu8	%zmm12,192(%r10,%rax,1)
+
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+	vmovdqa64	%zmm7,768(%rsp)
+	vmovdqa64	%zmm10,832(%rsp)
+	vmovdqa64	%zmm11,896(%rsp)
+	vmovdqa64	%zmm12,960(%rsp)
+	leaq	96(%rsi),%r12
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_fCCkfmethzGBcsk
+
+	vmovdqu64	192(%r12),%zmm0
+	vmovdqu64	%zmm0,704(%rsp)
+
+	vmovdqu64	128(%r12),%zmm3
+	vmovdqu64	%zmm3,640(%rsp)
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	64(%r12),%zmm4
+	vmovdqu64	%zmm4,576(%rsp)
+
+	vmovdqu64	0(%r12),%zmm5
+	vmovdqu64	%zmm5,512(%rsp)
+.L_skip_hkeys_precomputation_fCCkfmethzGBcsk:
+	cmpq	$512,%r8
+	jb	.L_message_below_32_blocks_vGFiuFCrdtyEdmj
+
+
+
+	cmpb	$240,%r15b
+	jae	.L_next_16_overflow_azdaGpBmqemFsii
+	vpaddd	%zmm28,%zmm2,%zmm7
+	vpaddd	%zmm27,%zmm7,%zmm10
+	vpaddd	%zmm27,%zmm10,%zmm11
+	vpaddd	%zmm27,%zmm11,%zmm12
+	jmp	.L_next_16_ok_azdaGpBmqemFsii
+.L_next_16_overflow_azdaGpBmqemFsii:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm12
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
+	vpaddd	%zmm12,%zmm7,%zmm10
+	vpaddd	%zmm12,%zmm10,%zmm11
+	vpaddd	%zmm12,%zmm11,%zmm12
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+.L_next_16_ok_azdaGpBmqemFsii:
+	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
+	addb	$16,%r15b
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm0
+	vmovdqu8	320(%rcx,%rax,1),%zmm3
+	vmovdqu8	384(%rcx,%rax,1),%zmm4
+	vmovdqu8	448(%rcx,%rax,1),%zmm5
+
+
+	vbroadcastf64x2	0(%rdi),%zmm6
+	vpxorq	%zmm6,%zmm7,%zmm7
+	vpxorq	%zmm6,%zmm10,%zmm10
+	vpxorq	%zmm6,%zmm11,%zmm11
+	vpxorq	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	16(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	32(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	48(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	64(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	80(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	96(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	112(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	128(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	144(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	160(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	176(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	192(%rdi),%zmm6
+	vaesenclast	%zmm6,%zmm7,%zmm7
+	vaesenclast	%zmm6,%zmm10,%zmm10
+	vaesenclast	%zmm6,%zmm11,%zmm11
+	vaesenclast	%zmm6,%zmm12,%zmm12
+
+
+	vpxorq	%zmm0,%zmm7,%zmm7
+	vpxorq	%zmm3,%zmm10,%zmm10
+	vpxorq	%zmm4,%zmm11,%zmm11
+	vpxorq	%zmm5,%zmm12,%zmm12
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm7,256(%r10,%rax,1)
+	vmovdqu8	%zmm10,320(%r10,%rax,1)
+	vmovdqu8	%zmm11,384(%r10,%rax,1)
+	vmovdqu8	%zmm12,448(%r10,%rax,1)
+
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+	vmovdqa64	%zmm7,1024(%rsp)
+	vmovdqa64	%zmm10,1088(%rsp)
+	vmovdqa64	%zmm11,1152(%rsp)
+	vmovdqa64	%zmm12,1216(%rsp)
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_xBfnunwkFayqntE
+	vmovdqu64	640(%rsp),%zmm3
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	576(%rsp),%zmm4
+	vmovdqu64	512(%rsp),%zmm5
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,256(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,192(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,128(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,64(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,0(%rsp)
+.L_skip_hkeys_precomputation_xBfnunwkFayqntE:
+	movq	$1,%r14
+	addq	$512,%rax
+	subq	$512,%r8
+
+	cmpq	$768,%r8
+	jb	.L_no_more_big_nblocks_vGFiuFCrdtyEdmj
+.L_encrypt_big_nblocks_vGFiuFCrdtyEdmj:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_kgrEivxorzgDAtw
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_kgrEivxorzgDAtw
+.L_16_blocks_overflow_kgrEivxorzgDAtw:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_kgrEivxorzgDAtw:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_ygbrdrfvgcxtDsE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_ygbrdrfvgcxtDsE
+.L_16_blocks_overflow_ygbrdrfvgcxtDsE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_ygbrdrfvgcxtDsE:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	256(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	320(%rsp),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	384(%rsp),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	448(%rsp),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm17
+	vmovdqu8	320(%rcx,%rax,1),%zmm19
+	vmovdqu8	384(%rcx,%rax,1),%zmm20
+	vmovdqu8	448(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,256(%r10,%rax,1)
+	vmovdqu8	%zmm3,320(%r10,%rax,1)
+	vmovdqu8	%zmm4,384(%r10,%rax,1)
+	vmovdqu8	%zmm5,448(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,768(%rsp)
+	vmovdqa64	%zmm3,832(%rsp)
+	vmovdqa64	%zmm4,896(%rsp)
+	vmovdqa64	%zmm5,960(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_chzkugnFnmDfqCj
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_chzkugnFnmDfqCj
+.L_16_blocks_overflow_chzkugnFnmDfqCj:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_chzkugnFnmDfqCj:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	512(%rcx,%rax,1),%zmm17
+	vmovdqu8	576(%rcx,%rax,1),%zmm19
+	vmovdqu8	640(%rcx,%rax,1),%zmm20
+	vmovdqu8	704(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+
+
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
+	vpxorq	%zmm24,%zmm6,%zmm6
+	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
+	vpxorq	%zmm25,%zmm7,%zmm7
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vextracti64x4	$1,%zmm6,%ymm12
+	vpxorq	%ymm12,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm12
+	vpxorq	%xmm12,%xmm6,%xmm6
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,512(%r10,%rax,1)
+	vmovdqu8	%zmm3,576(%r10,%rax,1)
+	vmovdqu8	%zmm4,640(%r10,%rax,1)
+	vmovdqu8	%zmm5,704(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,1024(%rsp)
+	vmovdqa64	%zmm3,1088(%rsp)
+	vmovdqa64	%zmm4,1152(%rsp)
+	vmovdqa64	%zmm5,1216(%rsp)
+	vmovdqa64	%zmm6,%zmm14
+
+	addq	$768,%rax
+	subq	$768,%r8
+	cmpq	$768,%r8
+	jae	.L_encrypt_big_nblocks_vGFiuFCrdtyEdmj
+
+.L_no_more_big_nblocks_vGFiuFCrdtyEdmj:
+
+	cmpq	$512,%r8
+	jae	.L_encrypt_32_blocks_vGFiuFCrdtyEdmj
+
+	cmpq	$256,%r8
+	jae	.L_encrypt_16_blocks_vGFiuFCrdtyEdmj
+.L_encrypt_0_blocks_ghash_32_vGFiuFCrdtyEdmj:
+	movl	%r8d,%r10d
+	andl	$~15,%r10d
+	movl	$256,%ebx
+	subl	%r10d,%ebx
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	addl	$256,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_clFmAefifDAutGa
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_clFmAefifDAutGa
+	jb	.L_last_num_blocks_is_7_1_clFmAefifDAutGa
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_clFmAefifDAutGa
+	jb	.L_last_num_blocks_is_11_9_clFmAefifDAutGa
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_clFmAefifDAutGa
+	ja	.L_last_num_blocks_is_16_clFmAefifDAutGa
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_clFmAefifDAutGa
+	jmp	.L_last_num_blocks_is_13_clFmAefifDAutGa
+
+.L_last_num_blocks_is_11_9_clFmAefifDAutGa:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_clFmAefifDAutGa
+	ja	.L_last_num_blocks_is_11_clFmAefifDAutGa
+	jmp	.L_last_num_blocks_is_9_clFmAefifDAutGa
+
+.L_last_num_blocks_is_7_1_clFmAefifDAutGa:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_clFmAefifDAutGa
+	jb	.L_last_num_blocks_is_3_1_clFmAefifDAutGa
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_clFmAefifDAutGa
+	je	.L_last_num_blocks_is_6_clFmAefifDAutGa
+	jmp	.L_last_num_blocks_is_5_clFmAefifDAutGa
+
+.L_last_num_blocks_is_3_1_clFmAefifDAutGa:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_clFmAefifDAutGa
+	je	.L_last_num_blocks_is_2_clFmAefifDAutGa
+.L_last_num_blocks_is_1_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_iclvqofEwujnfDr
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_iclvqofEwujnfDr
+
+.L_16_blocks_overflow_iclvqofEwujnfDr:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_iclvqofEwujnfDr:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_xlhhsmckrrGsbii
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_xlhhsmckrrGsbii
+.L_small_initial_partial_block_xlhhsmckrrGsbii:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_xlhhsmckrrGsbii
+.L_small_initial_compute_done_xlhhsmckrrGsbii:
+.L_after_reduction_xlhhsmckrrGsbii:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_2_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_uahyhdchDGnrzCA
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_uahyhdchDGnrzCA
+
+.L_16_blocks_overflow_uahyhdchDGnrzCA:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_uahyhdchDGnrzCA:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vcsFazFweylfDol
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vcsFazFweylfDol
+.L_small_initial_partial_block_vcsFazFweylfDol:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vcsFazFweylfDol:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vcsFazFweylfDol
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_vcsFazFweylfDol:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_3_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_AFnFDliaCAfvEhu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_AFnFDliaCAfvEhu
+
+.L_16_blocks_overflow_AFnFDliaCAfvEhu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_AFnFDliaCAfvEhu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vjDxuFCjxsarEvj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vjDxuFCjxsarEvj
+.L_small_initial_partial_block_vjDxuFCjxsarEvj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vjDxuFCjxsarEvj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vjDxuFCjxsarEvj
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_vjDxuFCjxsarEvj:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_4_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_bCdyfrrzmACdpaz
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_bCdyfrrzmACdpaz
+
+.L_16_blocks_overflow_bCdyfrrzmACdpaz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_bCdyfrrzmACdpaz:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_hBlAwvlkruntbGg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_hBlAwvlkruntbGg
+.L_small_initial_partial_block_hBlAwvlkruntbGg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_hBlAwvlkruntbGg:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_hBlAwvlkruntbGg
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_hBlAwvlkruntbGg:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_5_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_FBkgBomrocjnnoq
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_FBkgBomrocjnnoq
+
+.L_16_blocks_overflow_FBkgBomrocjnnoq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_FBkgBomrocjnnoq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%xmm29,%xmm3,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_DxadGchnoenwDGf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_DxadGchnoenwDGf
+.L_small_initial_partial_block_DxadGchnoenwDGf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_DxadGchnoenwDGf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_DxadGchnoenwDGf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_DxadGchnoenwDGf:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_6_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_avAFzmCstdCudlx
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_avAFzmCstdCudlx
+
+.L_16_blocks_overflow_avAFzmCstdCudlx:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_avAFzmCstdCudlx:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%ymm29,%ymm3,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wEpEgdanagoFiGh
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wEpEgdanagoFiGh
+.L_small_initial_partial_block_wEpEgdanagoFiGh:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_wEpEgdanagoFiGh:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_wEpEgdanagoFiGh
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_wEpEgdanagoFiGh:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_7_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_qEvgDehztggbdmA
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_qEvgDehztggbdmA
+
+.L_16_blocks_overflow_qEvgDehztggbdmA:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_qEvgDehztggbdmA:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_yzfygbgiCwxdabk
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_yzfygbgiCwxdabk
+.L_small_initial_partial_block_yzfygbgiCwxdabk:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_yzfygbgiCwxdabk:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_yzfygbgiCwxdabk
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_yzfygbgiCwxdabk:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_8_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_BGdhGCcszpdtCac
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_BGdhGCcszpdtCac
+
+.L_16_blocks_overflow_BGdhGCcszpdtCac:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_BGdhGCcszpdtCac:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_lizyAiurAsnmqbD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_lizyAiurAsnmqbD
+.L_small_initial_partial_block_lizyAiurAsnmqbD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_lizyAiurAsnmqbD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_lizyAiurAsnmqbD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_lizyAiurAsnmqbD:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_9_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_jejbvetjBvmkhBk
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_jejbvetjBvmkhBk
+
+.L_16_blocks_overflow_jejbvetjBvmkhBk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_jejbvetjBvmkhBk:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%xmm29,%xmm4,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_slktlABvkyAwtcm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_slktlABvkyAwtcm
+.L_small_initial_partial_block_slktlABvkyAwtcm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_slktlABvkyAwtcm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_slktlABvkyAwtcm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_slktlABvkyAwtcm:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_10_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_kuxqcFyCjCdzypj
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_kuxqcFyCjCdzypj
+
+.L_16_blocks_overflow_kuxqcFyCjCdzypj:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_kuxqcFyCjCdzypj:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%ymm29,%ymm4,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_kzjpBlzqjjDobcm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_kzjpBlzqjjDobcm
+.L_small_initial_partial_block_kzjpBlzqjjDobcm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_kzjpBlzqjjDobcm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_kzjpBlzqjjDobcm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_kzjpBlzqjjDobcm:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_11_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_twcvqekuBjgEqFy
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_twcvqekuBjgEqFy
+
+.L_16_blocks_overflow_twcvqekuBjgEqFy:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_twcvqekuBjgEqFy:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_yEftxDedcsuuExz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_yEftxDedcsuuExz
+.L_small_initial_partial_block_yEftxDedcsuuExz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_yEftxDedcsuuExz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_yEftxDedcsuuExz
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_yEftxDedcsuuExz:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_12_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_sapwttaxExwojqq
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_sapwttaxExwojqq
+
+.L_16_blocks_overflow_sapwttaxExwojqq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_sapwttaxExwojqq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_zklroEhfuzheeuf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_zklroEhfuzheeuf
+.L_small_initial_partial_block_zklroEhfuzheeuf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zklroEhfuzheeuf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_zklroEhfuzheeuf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_zklroEhfuzheeuf:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_13_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_yoyctpavfkGjAsi
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_yoyctpavfkGjAsi
+
+.L_16_blocks_overflow_yoyctpavfkGjAsi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_yoyctpavfkGjAsi:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%xmm29,%xmm5,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_blksqAlnAokexsD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_blksqAlnAokexsD
+.L_small_initial_partial_block_blksqAlnAokexsD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_blksqAlnAokexsD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_blksqAlnAokexsD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_blksqAlnAokexsD:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_14_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_DsCpbewBdfukyhl
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_DsCpbewBdfukyhl
+
+.L_16_blocks_overflow_DsCpbewBdfukyhl:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_DsCpbewBdfukyhl:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%ymm29,%ymm5,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_fBEycylymsEvvrm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_fBEycylymsEvvrm
+.L_small_initial_partial_block_fBEycylymsEvvrm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_fBEycylymsEvvrm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_fBEycylymsEvvrm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_fBEycylymsEvvrm:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_15_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_uEsngarrelCnrFy
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_uEsngarrelCnrFy
+
+.L_16_blocks_overflow_uEsngarrelCnrFy:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_uEsngarrelCnrFy:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BfrhukhdBbguaGk
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BfrhukhdBbguaGk
+.L_small_initial_partial_block_BfrhukhdBbguaGk:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BfrhukhdBbguaGk:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BfrhukhdBbguaGk
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_BfrhukhdBbguaGk:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_16_clFmAefifDAutGa:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_oqgiqyBihcCsmrf
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_oqgiqyBihcCsmrf
+
+.L_16_blocks_overflow_oqgiqyBihcCsmrf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_oqgiqyBihcCsmrf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_FodEmoybrzBzqbi:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_FodEmoybrzBzqbi:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_FodEmoybrzBzqbi:
+	jmp	.L_last_blocks_done_clFmAefifDAutGa
+.L_last_num_blocks_is_0_clFmAefifDAutGa:
+	vmovdqa64	1024(%rsp),%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1088(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1152(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1216(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_clFmAefifDAutGa:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_vGFiuFCrdtyEdmj
+.L_encrypt_32_blocks_vGFiuFCrdtyEdmj:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_uqgGnjqnvAlidpb
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_uqgGnjqnvAlidpb
+.L_16_blocks_overflow_uqgGnjqnvAlidpb:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_uqgGnjqnvAlidpb:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_wAdCqwDncdFvimA
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_wAdCqwDncdFvimA
+.L_16_blocks_overflow_wAdCqwDncdFvimA:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_wAdCqwDncdFvimA:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	256(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	320(%rsp),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	384(%rsp),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	448(%rsp),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm17
+	vmovdqu8	320(%rcx,%rax,1),%zmm19
+	vmovdqu8	384(%rcx,%rax,1),%zmm20
+	vmovdqu8	448(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,256(%r10,%rax,1)
+	vmovdqu8	%zmm3,320(%r10,%rax,1)
+	vmovdqu8	%zmm4,384(%r10,%rax,1)
+	vmovdqu8	%zmm5,448(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,768(%rsp)
+	vmovdqa64	%zmm3,832(%rsp)
+	vmovdqa64	%zmm4,896(%rsp)
+	vmovdqa64	%zmm5,960(%rsp)
+	vmovdqa64	1280(%rsp),%zmm13
+	vmovdqu64	512(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1344(%rsp),%zmm13
+	vmovdqu64	576(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1408(%rsp),%zmm13
+	vmovdqu64	640(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1472(%rsp),%zmm13
+	vmovdqu64	704(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+	subq	$512,%r8
+	addq	$512,%rax
+	movl	%r8d,%r10d
+	andl	$~15,%r10d
+	movl	$512,%ebx
+	subl	%r10d,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_iFegCclwArrudus
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_iFegCclwArrudus
+	jb	.L_last_num_blocks_is_7_1_iFegCclwArrudus
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_iFegCclwArrudus
+	jb	.L_last_num_blocks_is_11_9_iFegCclwArrudus
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_iFegCclwArrudus
+	ja	.L_last_num_blocks_is_16_iFegCclwArrudus
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_iFegCclwArrudus
+	jmp	.L_last_num_blocks_is_13_iFegCclwArrudus
+
+.L_last_num_blocks_is_11_9_iFegCclwArrudus:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_iFegCclwArrudus
+	ja	.L_last_num_blocks_is_11_iFegCclwArrudus
+	jmp	.L_last_num_blocks_is_9_iFegCclwArrudus
+
+.L_last_num_blocks_is_7_1_iFegCclwArrudus:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_iFegCclwArrudus
+	jb	.L_last_num_blocks_is_3_1_iFegCclwArrudus
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_iFegCclwArrudus
+	je	.L_last_num_blocks_is_6_iFegCclwArrudus
+	jmp	.L_last_num_blocks_is_5_iFegCclwArrudus
+
+.L_last_num_blocks_is_3_1_iFegCclwArrudus:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_iFegCclwArrudus
+	je	.L_last_num_blocks_is_2_iFegCclwArrudus
+.L_last_num_blocks_is_1_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_DciAyBdsaEcBtBm
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_DciAyBdsaEcBtBm
+
+.L_16_blocks_overflow_DciAyBdsaEcBtBm:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_DciAyBdsaEcBtBm:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_edfbaAaCfDpCchg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_edfbaAaCfDpCchg
+.L_small_initial_partial_block_edfbaAaCfDpCchg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_edfbaAaCfDpCchg
+.L_small_initial_compute_done_edfbaAaCfDpCchg:
+.L_after_reduction_edfbaAaCfDpCchg:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_2_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_GpifoAjmktCwnmp
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_GpifoAjmktCwnmp
+
+.L_16_blocks_overflow_GpifoAjmktCwnmp:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_GpifoAjmktCwnmp:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ygylBoGCwsrcfgf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ygylBoGCwsrcfgf
+.L_small_initial_partial_block_ygylBoGCwsrcfgf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ygylBoGCwsrcfgf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ygylBoGCwsrcfgf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ygylBoGCwsrcfgf:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_3_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_hGnClozauwdDuEg
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_hGnClozauwdDuEg
+
+.L_16_blocks_overflow_hGnClozauwdDuEg:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_hGnClozauwdDuEg:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dzkbuGemjGkqEuC
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dzkbuGemjGkqEuC
+.L_small_initial_partial_block_dzkbuGemjGkqEuC:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_dzkbuGemjGkqEuC:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_dzkbuGemjGkqEuC
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_dzkbuGemjGkqEuC:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_4_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_ArcrbBitdBqBudr
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_ArcrbBitdBqBudr
+
+.L_16_blocks_overflow_ArcrbBitdBqBudr:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_ArcrbBitdBqBudr:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_kdxuBafgrtzeaDF
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_kdxuBafgrtzeaDF
+.L_small_initial_partial_block_kdxuBafgrtzeaDF:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_kdxuBafgrtzeaDF:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_kdxuBafgrtzeaDF
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_kdxuBafgrtzeaDF:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_5_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_bBbqnaGanslpFtn
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_bBbqnaGanslpFtn
+
+.L_16_blocks_overflow_bBbqnaGanslpFtn:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_bBbqnaGanslpFtn:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%xmm29,%xmm3,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BhfhlqiGsgnpcaj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BhfhlqiGsgnpcaj
+.L_small_initial_partial_block_BhfhlqiGsgnpcaj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BhfhlqiGsgnpcaj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BhfhlqiGsgnpcaj
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_BhfhlqiGsgnpcaj:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_6_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_ipxuEtpdvFaEGBo
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_ipxuEtpdvFaEGBo
+
+.L_16_blocks_overflow_ipxuEtpdvFaEGBo:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_ipxuEtpdvFaEGBo:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%ymm29,%ymm3,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AFrofwilBgFibdp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AFrofwilBgFibdp
+.L_small_initial_partial_block_AFrofwilBgFibdp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AFrofwilBgFibdp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AFrofwilBgFibdp
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_AFrofwilBgFibdp:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_7_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_rxsGziziFEsFehd
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_rxsGziziFEsFehd
+
+.L_16_blocks_overflow_rxsGziziFEsFehd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_rxsGziziFEsFehd:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_olwFrxofqbimEDe
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_olwFrxofqbimEDe
+.L_small_initial_partial_block_olwFrxofqbimEDe:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_olwFrxofqbimEDe:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_olwFrxofqbimEDe
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_olwFrxofqbimEDe:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_8_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_lEyymxxpxBizosp
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_lEyymxxpxBizosp
+
+.L_16_blocks_overflow_lEyymxxpxBizosp:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_lEyymxxpxBizosp:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CexFFEjwFgatiyf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CexFFEjwFgatiyf
+.L_small_initial_partial_block_CexFFEjwFgatiyf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CexFFEjwFgatiyf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CexFFEjwFgatiyf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CexFFEjwFgatiyf:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_9_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_sovxAnjDpkyFEjr
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_sovxAnjDpkyFEjr
+
+.L_16_blocks_overflow_sovxAnjDpkyFEjr:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_sovxAnjDpkyFEjr:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%xmm29,%xmm4,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_rFBmfCxhznkefCs
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_rFBmfCxhznkefCs
+.L_small_initial_partial_block_rFBmfCxhznkefCs:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_rFBmfCxhznkefCs:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_rFBmfCxhznkefCs
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_rFBmfCxhznkefCs:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_10_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_jEwqdlhmuzbphAs
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_jEwqdlhmuzbphAs
+
+.L_16_blocks_overflow_jEwqdlhmuzbphAs:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_jEwqdlhmuzbphAs:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%ymm29,%ymm4,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AtsqidaBixiaFfh
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AtsqidaBixiaFfh
+.L_small_initial_partial_block_AtsqidaBixiaFfh:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AtsqidaBixiaFfh:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AtsqidaBixiaFfh
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_AtsqidaBixiaFfh:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_11_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_EoGhehEagxjCvfD
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_EoGhehEagxjCvfD
+
+.L_16_blocks_overflow_EoGhehEagxjCvfD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_EoGhehEagxjCvfD:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wFxBqGsmxhrFiAy
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wFxBqGsmxhrFiAy
+.L_small_initial_partial_block_wFxBqGsmxhrFiAy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_wFxBqGsmxhrFiAy:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_wFxBqGsmxhrFiAy
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_wFxBqGsmxhrFiAy:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_12_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_DlGqiEaynindGDj
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_DlGqiEaynindGDj
+
+.L_16_blocks_overflow_DlGqiEaynindGDj:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_DlGqiEaynindGDj:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AudthzjrrnBxrrs
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AudthzjrrnBxrrs
+.L_small_initial_partial_block_AudthzjrrnBxrrs:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AudthzjrrnBxrrs:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AudthzjrrnBxrrs
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_AudthzjrrnBxrrs:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_13_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_jAzFCbqBGBmivAt
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_jAzFCbqBGBmivAt
+
+.L_16_blocks_overflow_jAzFCbqBGBmivAt:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_jAzFCbqBGBmivAt:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%xmm29,%xmm5,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_bacorulAinEEDtq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_bacorulAinEEDtq
+.L_small_initial_partial_block_bacorulAinEEDtq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_bacorulAinEEDtq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_bacorulAinEEDtq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_bacorulAinEEDtq:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_14_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_owzbrbGGxkCBFmd
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_owzbrbGGxkCBFmd
+
+.L_16_blocks_overflow_owzbrbGGxkCBFmd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_owzbrbGGxkCBFmd:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%ymm29,%ymm5,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_DwAsCrtAvwjzeAy
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_DwAsCrtAvwjzeAy
+.L_small_initial_partial_block_DwAsCrtAvwjzeAy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_DwAsCrtAvwjzeAy:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_DwAsCrtAvwjzeAy
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_DwAsCrtAvwjzeAy:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_15_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_namleveffCjgugz
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_namleveffCjgugz
+
+.L_16_blocks_overflow_namleveffCjgugz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_namleveffCjgugz:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_iljtdpviChDcmGl
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_iljtdpviChDcmGl
+.L_small_initial_partial_block_iljtdpviChDcmGl:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_iljtdpviChDcmGl:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_iljtdpviChDcmGl
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_iljtdpviChDcmGl:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_16_iFegCclwArrudus:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_wrhtGoykvlyvCGz
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_wrhtGoykvlyvCGz
+
+.L_16_blocks_overflow_wrhtGoykvlyvCGz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_wrhtGoykvlyvCGz:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_gtecnkibudebduF:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_gtecnkibudebduF:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_gtecnkibudebduF:
+	jmp	.L_last_blocks_done_iFegCclwArrudus
+.L_last_num_blocks_is_0_iFegCclwArrudus:
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_iFegCclwArrudus:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_vGFiuFCrdtyEdmj
+.L_encrypt_16_blocks_vGFiuFCrdtyEdmj:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_nscohsgEDrGwqyj
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_nscohsgEDrGwqyj
+.L_16_blocks_overflow_nscohsgEDrGwqyj:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_nscohsgEDrGwqyj:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	vmovdqa64	1024(%rsp),%zmm13
+	vmovdqu64	256(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1088(%rsp),%zmm13
+	vmovdqu64	320(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1152(%rsp),%zmm13
+	vmovdqu64	384(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1216(%rsp),%zmm13
+	vmovdqu64	448(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	subq	$256,%r8
+	addq	$256,%rax
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_pnwrAqdaFijfhwq
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_pnwrAqdaFijfhwq
+	jb	.L_last_num_blocks_is_7_1_pnwrAqdaFijfhwq
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_pnwrAqdaFijfhwq
+	jb	.L_last_num_blocks_is_11_9_pnwrAqdaFijfhwq
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_pnwrAqdaFijfhwq
+	ja	.L_last_num_blocks_is_16_pnwrAqdaFijfhwq
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_pnwrAqdaFijfhwq
+	jmp	.L_last_num_blocks_is_13_pnwrAqdaFijfhwq
+
+.L_last_num_blocks_is_11_9_pnwrAqdaFijfhwq:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_pnwrAqdaFijfhwq
+	ja	.L_last_num_blocks_is_11_pnwrAqdaFijfhwq
+	jmp	.L_last_num_blocks_is_9_pnwrAqdaFijfhwq
+
+.L_last_num_blocks_is_7_1_pnwrAqdaFijfhwq:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_pnwrAqdaFijfhwq
+	jb	.L_last_num_blocks_is_3_1_pnwrAqdaFijfhwq
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_pnwrAqdaFijfhwq
+	je	.L_last_num_blocks_is_6_pnwrAqdaFijfhwq
+	jmp	.L_last_num_blocks_is_5_pnwrAqdaFijfhwq
+
+.L_last_num_blocks_is_3_1_pnwrAqdaFijfhwq:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_pnwrAqdaFijfhwq
+	je	.L_last_num_blocks_is_2_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_1_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_cwdjmazipeiwlyE
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_cwdjmazipeiwlyE
+
+.L_16_blocks_overflow_cwdjmazipeiwlyE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_cwdjmazipeiwlyE:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wyxbylxqppbEaDm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wyxbylxqppbEaDm
+.L_small_initial_partial_block_wyxbylxqppbEaDm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_wyxbylxqppbEaDm
+.L_small_initial_compute_done_wyxbylxqppbEaDm:
+.L_after_reduction_wyxbylxqppbEaDm:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_2_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_lgkBbhdgFuchBGr
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_lgkBbhdgFuchBGr
+
+.L_16_blocks_overflow_lgkBbhdgFuchBGr:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_lgkBbhdgFuchBGr:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ykEGjvluAnFavsz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ykEGjvluAnFavsz
+.L_small_initial_partial_block_ykEGjvluAnFavsz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ykEGjvluAnFavsz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ykEGjvluAnFavsz
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ykEGjvluAnFavsz:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_3_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_zgAghoFkpGrEkFD
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_zgAghoFkpGrEkFD
+
+.L_16_blocks_overflow_zgAghoFkpGrEkFD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_zgAghoFkpGrEkFD:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_jFtlpubgeCmuinB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_jFtlpubgeCmuinB
+.L_small_initial_partial_block_jFtlpubgeCmuinB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_jFtlpubgeCmuinB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_jFtlpubgeCmuinB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_jFtlpubgeCmuinB:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_4_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_rEhEtxiCwGEzsEC
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_rEhEtxiCwGEzsEC
+
+.L_16_blocks_overflow_rEhEtxiCwGEzsEC:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_rEhEtxiCwGEzsEC:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_nrCusbrqDqdrzzw
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_nrCusbrqDqdrzzw
+.L_small_initial_partial_block_nrCusbrqDqdrzzw:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_nrCusbrqDqdrzzw:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_nrCusbrqDqdrzzw
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_nrCusbrqDqdrzzw:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_5_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_ouejkusocFxwGmd
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_ouejkusocFxwGmd
+
+.L_16_blocks_overflow_ouejkusocFxwGmd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_ouejkusocFxwGmd:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%xmm29,%xmm3,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_qzpgbxFAbplvkca
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_qzpgbxFAbplvkca
+.L_small_initial_partial_block_qzpgbxFAbplvkca:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_qzpgbxFAbplvkca:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_qzpgbxFAbplvkca
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_qzpgbxFAbplvkca:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_6_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_xcbuueBpuhEasuk
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_xcbuueBpuhEasuk
+
+.L_16_blocks_overflow_xcbuueBpuhEasuk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_xcbuueBpuhEasuk:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%ymm29,%ymm3,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tweGzvtekmiydot
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tweGzvtekmiydot
+.L_small_initial_partial_block_tweGzvtekmiydot:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tweGzvtekmiydot:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tweGzvtekmiydot
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tweGzvtekmiydot:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_7_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_vngircosEcaCGdp
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_vngircosEcaCGdp
+
+.L_16_blocks_overflow_vngircosEcaCGdp:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_vngircosEcaCGdp:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_DwvdkgxddxvccEc
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_DwvdkgxddxvccEc
+.L_small_initial_partial_block_DwvdkgxddxvccEc:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_DwvdkgxddxvccEc:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_DwvdkgxddxvccEc
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_DwvdkgxddxvccEc:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_8_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_zjeGfhgkfwqxvGp
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_zjeGfhgkfwqxvGp
+
+.L_16_blocks_overflow_zjeGfhgkfwqxvGp:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_zjeGfhgkfwqxvGp:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_goxqbCznCxkqiGr
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_goxqbCznCxkqiGr
+.L_small_initial_partial_block_goxqbCznCxkqiGr:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_goxqbCznCxkqiGr:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_goxqbCznCxkqiGr
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_goxqbCznCxkqiGr:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_9_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_ACyGxxGuBABjEwe
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_ACyGxxGuBABjEwe
+
+.L_16_blocks_overflow_ACyGxxGuBABjEwe:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_ACyGxxGuBABjEwe:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%xmm29,%xmm4,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ovsGChjshzklfjo
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ovsGChjshzklfjo
+.L_small_initial_partial_block_ovsGChjshzklfjo:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ovsGChjshzklfjo:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ovsGChjshzklfjo
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ovsGChjshzklfjo:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_10_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_hGkmGescaebrGln
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_hGkmGescaebrGln
+
+.L_16_blocks_overflow_hGkmGescaebrGln:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_hGkmGescaebrGln:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%ymm29,%ymm4,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CyjxjlwdepqaApA
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CyjxjlwdepqaApA
+.L_small_initial_partial_block_CyjxjlwdepqaApA:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CyjxjlwdepqaApA:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CyjxjlwdepqaApA
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CyjxjlwdepqaApA:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_11_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_xbmsxFnbrGjefaG
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_xbmsxFnbrGjefaG
+
+.L_16_blocks_overflow_xbmsxFnbrGjefaG:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_xbmsxFnbrGjefaG:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BzFbyqBAFGsECpg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BzFbyqBAFGsECpg
+.L_small_initial_partial_block_BzFbyqBAFGsECpg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BzFbyqBAFGsECpg:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BzFbyqBAFGsECpg
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_BzFbyqBAFGsECpg:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_12_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_qmmwGhDzlkuaqFG
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_qmmwGhDzlkuaqFG
+
+.L_16_blocks_overflow_qmmwGhDzlkuaqFG:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_qmmwGhDzlkuaqFG:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_xcrebvvducinnuu
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_xcrebvvducinnuu
+.L_small_initial_partial_block_xcrebvvducinnuu:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_xcrebvvducinnuu:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_xcrebvvducinnuu
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_xcrebvvducinnuu:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_13_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_gfianvcbrxfzilf
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_gfianvcbrxfzilf
+
+.L_16_blocks_overflow_gfianvcbrxfzilf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_gfianvcbrxfzilf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%xmm29,%xmm5,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GexyfxbntoCAqzs
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GexyfxbntoCAqzs
+.L_small_initial_partial_block_GexyfxbntoCAqzs:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GexyfxbntoCAqzs:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GexyfxbntoCAqzs
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GexyfxbntoCAqzs:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_14_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_GkgfAflGxevlfeG
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_GkgfAflGxevlfeG
+
+.L_16_blocks_overflow_GkgfAflGxevlfeG:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_GkgfAflGxevlfeG:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%ymm29,%ymm5,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AbGomDlqmDBlnnB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AbGomDlqmDBlnnB
+.L_small_initial_partial_block_AbGomDlqmDBlnnB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AbGomDlqmDBlnnB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AbGomDlqmDBlnnB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_AbGomDlqmDBlnnB:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_15_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_yamzqtsogEDBipE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_yamzqtsogEDBipE
+
+.L_16_blocks_overflow_yamzqtsogEDBipE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_yamzqtsogEDBipE:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AoqoeuDdahkhAax
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AoqoeuDdahkhAax
+.L_small_initial_partial_block_AoqoeuDdahkhAax:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AoqoeuDdahkhAax:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AoqoeuDdahkhAax
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_AoqoeuDdahkhAax:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_16_pnwrAqdaFijfhwq:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_xsDmpFibtvalgsD
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_xsDmpFibtvalgsD
+
+.L_16_blocks_overflow_xsDmpFibtvalgsD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_xsDmpFibtvalgsD:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_EbtzxmyhzGyfzeq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_EbtzxmyhzGyfzeq:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_EbtzxmyhzGyfzeq:
+	jmp	.L_last_blocks_done_pnwrAqdaFijfhwq
+.L_last_num_blocks_is_0_pnwrAqdaFijfhwq:
+	vmovdqa64	1280(%rsp),%zmm13
+	vmovdqu64	512(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1344(%rsp),%zmm13
+	vmovdqu64	576(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1408(%rsp),%zmm13
+	vmovdqu64	640(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1472(%rsp),%zmm13
+	vmovdqu64	704(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_pnwrAqdaFijfhwq:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_vGFiuFCrdtyEdmj
+
+.L_message_below_32_blocks_vGFiuFCrdtyEdmj:
+
+
+	subq	$256,%r8
+	addq	$256,%rax
+	movl	%r8d,%r10d
+	leaq	96(%rsi),%r12
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_AAiskeiBCbfmoay
+	vmovdqu64	640(%rsp),%zmm3
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	576(%rsp),%zmm4
+	vmovdqu64	512(%rsp),%zmm5
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,256(%rsp)
+.L_skip_hkeys_precomputation_AAiskeiBCbfmoay:
+	movq	$1,%r14
+	andl	$~15,%r10d
+	movl	$512,%ebx
+	subl	%r10d,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_EszkovqhgzpokiC
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_EszkovqhgzpokiC
+	jb	.L_last_num_blocks_is_7_1_EszkovqhgzpokiC
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_EszkovqhgzpokiC
+	jb	.L_last_num_blocks_is_11_9_EszkovqhgzpokiC
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_EszkovqhgzpokiC
+	ja	.L_last_num_blocks_is_16_EszkovqhgzpokiC
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_EszkovqhgzpokiC
+	jmp	.L_last_num_blocks_is_13_EszkovqhgzpokiC
+
+.L_last_num_blocks_is_11_9_EszkovqhgzpokiC:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_EszkovqhgzpokiC
+	ja	.L_last_num_blocks_is_11_EszkovqhgzpokiC
+	jmp	.L_last_num_blocks_is_9_EszkovqhgzpokiC
+
+.L_last_num_blocks_is_7_1_EszkovqhgzpokiC:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_EszkovqhgzpokiC
+	jb	.L_last_num_blocks_is_3_1_EszkovqhgzpokiC
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_EszkovqhgzpokiC
+	je	.L_last_num_blocks_is_6_EszkovqhgzpokiC
+	jmp	.L_last_num_blocks_is_5_EszkovqhgzpokiC
+
+.L_last_num_blocks_is_3_1_EszkovqhgzpokiC:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_EszkovqhgzpokiC
+	je	.L_last_num_blocks_is_2_EszkovqhgzpokiC
+.L_last_num_blocks_is_1_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_wipsdAvuluGCAFD
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_wipsdAvuluGCAFD
+
+.L_16_blocks_overflow_wipsdAvuluGCAFD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_wipsdAvuluGCAFD:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ndirycjqicacyDj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ndirycjqicacyDj
+.L_small_initial_partial_block_ndirycjqicacyDj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_ndirycjqicacyDj
+.L_small_initial_compute_done_ndirycjqicacyDj:
+.L_after_reduction_ndirycjqicacyDj:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_2_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_CvystBlheqGlykh
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_CvystBlheqGlykh
+
+.L_16_blocks_overflow_CvystBlheqGlykh:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_CvystBlheqGlykh:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tDwnymdoykecBag
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tDwnymdoykecBag
+.L_small_initial_partial_block_tDwnymdoykecBag:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tDwnymdoykecBag:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tDwnymdoykecBag
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tDwnymdoykecBag:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_3_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_DCdxoipEpzCgdBw
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_DCdxoipEpzCgdBw
+
+.L_16_blocks_overflow_DCdxoipEpzCgdBw:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_DCdxoipEpzCgdBw:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_oDevGmGibiCwrco
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_oDevGmGibiCwrco
+.L_small_initial_partial_block_oDevGmGibiCwrco:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_oDevGmGibiCwrco:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_oDevGmGibiCwrco
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_oDevGmGibiCwrco:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_4_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_hvhzvcpvsDCyhqn
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_hvhzvcpvsDCyhqn
+
+.L_16_blocks_overflow_hvhzvcpvsDCyhqn:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_hvhzvcpvsDCyhqn:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_zBqwdbEvyDEeGth
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_zBqwdbEvyDEeGth
+.L_small_initial_partial_block_zBqwdbEvyDEeGth:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zBqwdbEvyDEeGth:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_zBqwdbEvyDEeGth
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_zBqwdbEvyDEeGth:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_5_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_lgowmkgCbdAgvlu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_lgowmkgCbdAgvlu
+
+.L_16_blocks_overflow_lgowmkgCbdAgvlu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_lgowmkgCbdAgvlu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%xmm29,%xmm3,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_lkpAdzbDyGohojn
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_lkpAdzbDyGohojn
+.L_small_initial_partial_block_lkpAdzbDyGohojn:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_lkpAdzbDyGohojn:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_lkpAdzbDyGohojn
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_lkpAdzbDyGohojn:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_6_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_kzwbrCapnfBwCir
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_kzwbrCapnfBwCir
+
+.L_16_blocks_overflow_kzwbrCapnfBwCir:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_kzwbrCapnfBwCir:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%ymm29,%ymm3,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_udGBmzCGmnGxEww
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_udGBmzCGmnGxEww
+.L_small_initial_partial_block_udGBmzCGmnGxEww:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_udGBmzCGmnGxEww:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_udGBmzCGmnGxEww
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_udGBmzCGmnGxEww:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_7_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_gjElsDnfkkmCuDk
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_gjElsDnfkkmCuDk
+
+.L_16_blocks_overflow_gjElsDnfkkmCuDk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_gjElsDnfkkmCuDk:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_kuprArunyftgobC
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_kuprArunyftgobC
+.L_small_initial_partial_block_kuprArunyftgobC:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_kuprArunyftgobC:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_kuprArunyftgobC
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_kuprArunyftgobC:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_8_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_DDniadwduioizsc
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_DDniadwduioizsc
+
+.L_16_blocks_overflow_DDniadwduioizsc:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_DDniadwduioizsc:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_EpcCErEleehbwhm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_EpcCErEleehbwhm
+.L_small_initial_partial_block_EpcCErEleehbwhm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_EpcCErEleehbwhm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_EpcCErEleehbwhm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_EpcCErEleehbwhm:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_9_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_CejjFoiAoBrDbCk
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_CejjFoiAoBrDbCk
+
+.L_16_blocks_overflow_CejjFoiAoBrDbCk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_CejjFoiAoBrDbCk:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%xmm29,%xmm4,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_oCukAdGGloxAcfj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_oCukAdGGloxAcfj
+.L_small_initial_partial_block_oCukAdGGloxAcfj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_oCukAdGGloxAcfj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_oCukAdGGloxAcfj
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_oCukAdGGloxAcfj:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_10_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_mbamaoBaEACrrcu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_mbamaoBaEACrrcu
+
+.L_16_blocks_overflow_mbamaoBaEACrrcu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_mbamaoBaEACrrcu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%ymm29,%ymm4,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ielbedjygpCAFDc
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ielbedjygpCAFDc
+.L_small_initial_partial_block_ielbedjygpCAFDc:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ielbedjygpCAFDc:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ielbedjygpCAFDc
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ielbedjygpCAFDc:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_11_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_kxfokeAAhgdzwuu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_kxfokeAAhgdzwuu
+
+.L_16_blocks_overflow_kxfokeAAhgdzwuu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_kxfokeAAhgdzwuu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_lhehudnGFbaBseA
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_lhehudnGFbaBseA
+.L_small_initial_partial_block_lhehudnGFbaBseA:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_lhehudnGFbaBseA:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_lhehudnGFbaBseA
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_lhehudnGFbaBseA:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_12_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_ndbFCkfjbEFCkmt
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_ndbFCkfjbEFCkmt
+
+.L_16_blocks_overflow_ndbFCkfjbEFCkmt:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_ndbFCkfjbEFCkmt:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_bgFyuGgCblefFiE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_bgFyuGgCblefFiE
+.L_small_initial_partial_block_bgFyuGgCblefFiE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_bgFyuGgCblefFiE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_bgFyuGgCblefFiE
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_bgFyuGgCblefFiE:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_13_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_pgCloowzoeDojro
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_pgCloowzoeDojro
+
+.L_16_blocks_overflow_pgCloowzoeDojro:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_pgCloowzoeDojro:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%xmm29,%xmm5,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_zrbdgvlAwahgmzm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_zrbdgvlAwahgmzm
+.L_small_initial_partial_block_zrbdgvlAwahgmzm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zrbdgvlAwahgmzm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_zrbdgvlAwahgmzm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_zrbdgvlAwahgmzm:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_14_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_dAzgdDkxtunwwdi
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_dAzgdDkxtunwwdi
+
+.L_16_blocks_overflow_dAzgdDkxtunwwdi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_dAzgdDkxtunwwdi:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%ymm29,%ymm5,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_afhwFylouqsGGfz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_afhwFylouqsGGfz
+.L_small_initial_partial_block_afhwFylouqsGGfz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_afhwFylouqsGGfz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_afhwFylouqsGGfz
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_afhwFylouqsGGfz:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_15_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_jhgAsmqlufhpFvc
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_jhgAsmqlufhpFvc
+
+.L_16_blocks_overflow_jhgAsmqlufhpFvc:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_jhgAsmqlufhpFvc:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tjmFkbFyGhDkGat
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tjmFkbFyGhDkGat
+.L_small_initial_partial_block_tjmFkbFyGhDkGat:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tjmFkbFyGhDkGat:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tjmFkbFyGhDkGat
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tjmFkbFyGhDkGat:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_16_EszkovqhgzpokiC:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_uvAhgqnmrycjaCk
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_uvAhgqnmrycjaCk
+
+.L_16_blocks_overflow_uvAhgqnmrycjaCk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_uvAhgqnmrycjaCk:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_ozEgscpzdpanFkn:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ozEgscpzdpanFkn:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ozEgscpzdpanFkn:
+	jmp	.L_last_blocks_done_EszkovqhgzpokiC
+.L_last_num_blocks_is_0_EszkovqhgzpokiC:
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_EszkovqhgzpokiC:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_vGFiuFCrdtyEdmj
+
+.L_message_below_equal_16_blocks_vGFiuFCrdtyEdmj:
+
+
+	movl	%r8d,%r12d
+	addl	$15,%r12d
+	shrl	$4,%r12d
+	cmpq	$8,%r12
+	je	.L_small_initial_num_blocks_is_8_vFnGvjfgreazEcF
+	jl	.L_small_initial_num_blocks_is_7_1_vFnGvjfgreazEcF
+
+
+	cmpq	$12,%r12
+	je	.L_small_initial_num_blocks_is_12_vFnGvjfgreazEcF
+	jl	.L_small_initial_num_blocks_is_11_9_vFnGvjfgreazEcF
+
+
+	cmpq	$16,%r12
+	je	.L_small_initial_num_blocks_is_16_vFnGvjfgreazEcF
+	cmpq	$15,%r12
+	je	.L_small_initial_num_blocks_is_15_vFnGvjfgreazEcF
+	cmpq	$14,%r12
+	je	.L_small_initial_num_blocks_is_14_vFnGvjfgreazEcF
+	jmp	.L_small_initial_num_blocks_is_13_vFnGvjfgreazEcF
+
+.L_small_initial_num_blocks_is_11_9_vFnGvjfgreazEcF:
+
+	cmpq	$11,%r12
+	je	.L_small_initial_num_blocks_is_11_vFnGvjfgreazEcF
+	cmpq	$10,%r12
+	je	.L_small_initial_num_blocks_is_10_vFnGvjfgreazEcF
+	jmp	.L_small_initial_num_blocks_is_9_vFnGvjfgreazEcF
+
+.L_small_initial_num_blocks_is_7_1_vFnGvjfgreazEcF:
+	cmpq	$4,%r12
+	je	.L_small_initial_num_blocks_is_4_vFnGvjfgreazEcF
+	jl	.L_small_initial_num_blocks_is_3_1_vFnGvjfgreazEcF
+
+	cmpq	$7,%r12
+	je	.L_small_initial_num_blocks_is_7_vFnGvjfgreazEcF
+	cmpq	$6,%r12
+	je	.L_small_initial_num_blocks_is_6_vFnGvjfgreazEcF
+	jmp	.L_small_initial_num_blocks_is_5_vFnGvjfgreazEcF
+
+.L_small_initial_num_blocks_is_3_1_vFnGvjfgreazEcF:
+
+	cmpq	$3,%r12
+	je	.L_small_initial_num_blocks_is_3_vFnGvjfgreazEcF
+	cmpq	$2,%r12
+	je	.L_small_initial_num_blocks_is_2_vFnGvjfgreazEcF
+
+
+
+
+
+.L_small_initial_num_blocks_is_1_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%xmm29
+	vpaddd	ONE(%rip),%xmm2,%xmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vpshufb	%xmm29,%xmm0,%xmm0
+	vmovdqu8	0(%rcx,%rax,1),%xmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%xmm15,%xmm0,%xmm0
+	vpxorq	%xmm6,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm6
+	vextracti32x4	$0,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_EylajApwEfhjxgB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_EylajApwEfhjxgB
+.L_small_initial_partial_block_EylajApwEfhjxgB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm13,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_EylajApwEfhjxgB
+.L_small_initial_compute_done_EylajApwEfhjxgB:
+.L_after_reduction_EylajApwEfhjxgB:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_2_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%ymm29
+	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
+	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vpshufb	%ymm29,%ymm0,%ymm0
+	vmovdqu8	0(%rcx,%rax,1),%ymm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%ymm15,%ymm0,%ymm0
+	vpxorq	%ymm6,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm6
+	vextracti32x4	$1,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_jlpvylvbhteGygo
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_jlpvylvbhteGygo
+.L_small_initial_partial_block_jlpvylvbhteGygo:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_jlpvylvbhteGygo:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_jlpvylvbhteGygo
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_jlpvylvbhteGygo:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_3_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vextracti32x4	$2,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_liElhlapqqbqFpC
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_liElhlapqqbqFpC
+.L_small_initial_partial_block_liElhlapqqbqFpC:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_liElhlapqqbqFpC:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_liElhlapqqbqFpC
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_liElhlapqqbqFpC:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_4_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vextracti32x4	$3,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_khrakvxpADurEpt
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_khrakvxpADurEpt
+.L_small_initial_partial_block_khrakvxpADurEpt:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_khrakvxpADurEpt:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_khrakvxpADurEpt
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_khrakvxpADurEpt:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_5_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%xmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%xmm15,%xmm3,%xmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%xmm7,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%xmm29,%xmm3,%xmm7
+	vextracti32x4	$0,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_idvtsyqmawtzhpa
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_idvtsyqmawtzhpa
+.L_small_initial_partial_block_idvtsyqmawtzhpa:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_idvtsyqmawtzhpa:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_idvtsyqmawtzhpa
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_idvtsyqmawtzhpa:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_6_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%ymm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%ymm15,%ymm3,%ymm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%ymm7,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%ymm29,%ymm3,%ymm7
+	vextracti32x4	$1,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ByvsCixuylGFukn
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ByvsCixuylGFukn
+.L_small_initial_partial_block_ByvsCixuylGFukn:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ByvsCixuylGFukn:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ByvsCixuylGFukn
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_ByvsCixuylGFukn:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_7_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vextracti32x4	$2,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_igzsovckDCsmEGF
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_igzsovckDCsmEGF
+.L_small_initial_partial_block_igzsovckDCsmEGF:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_igzsovckDCsmEGF:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_igzsovckDCsmEGF
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_igzsovckDCsmEGF:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_8_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vextracti32x4	$3,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ldldDBictdBdFsD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ldldDBictdBdFsD
+.L_small_initial_partial_block_ldldDBictdBdFsD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ldldDBictdBdFsD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ldldDBictdBdFsD
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_ldldDBictdBdFsD:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_9_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%xmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%xmm15,%xmm4,%xmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%xmm10,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%xmm29,%xmm4,%xmm10
+	vextracti32x4	$0,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vijaDlwufEjvmfB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vijaDlwufEjvmfB
+.L_small_initial_partial_block_vijaDlwufEjvmfB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vijaDlwufEjvmfB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vijaDlwufEjvmfB
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_vijaDlwufEjvmfB:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_10_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%ymm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%ymm15,%ymm4,%ymm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%ymm10,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%ymm29,%ymm4,%ymm10
+	vextracti32x4	$1,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ppznafkycEsgEhf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ppznafkycEsgEhf
+.L_small_initial_partial_block_ppznafkycEsgEhf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ppznafkycEsgEhf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ppznafkycEsgEhf
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_ppznafkycEsgEhf:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_11_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vextracti32x4	$2,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GDhvsFtpBrxjhFi
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GDhvsFtpBrxjhFi
+.L_small_initial_partial_block_GDhvsFtpBrxjhFi:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GDhvsFtpBrxjhFi:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GDhvsFtpBrxjhFi
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_GDhvsFtpBrxjhFi:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_12_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vextracti32x4	$3,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vuczpzfqkFEbifz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	64(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vuczpzfqkFEbifz
+.L_small_initial_partial_block_vuczpzfqkFEbifz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vuczpzfqkFEbifz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vuczpzfqkFEbifz
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_vuczpzfqkFEbifz:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_13_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%xmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%xmm15,%xmm5,%xmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%xmm11,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vpshufb	%xmm29,%xmm5,%xmm11
+	vextracti32x4	$0,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tfesdkFpcxjpdCx
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	48(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tfesdkFpcxjpdCx
+.L_small_initial_partial_block_tfesdkFpcxjpdCx:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	64(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tfesdkFpcxjpdCx:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tfesdkFpcxjpdCx
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_tfesdkFpcxjpdCx:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_14_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%ymm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%ymm15,%ymm5,%ymm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%ymm11,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vpshufb	%ymm29,%ymm5,%ymm11
+	vextracti32x4	$1,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_rshlqcnfwbnoADd
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	32(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_rshlqcnfwbnoADd
+.L_small_initial_partial_block_rshlqcnfwbnoADd:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	48(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_rshlqcnfwbnoADd:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_rshlqcnfwbnoADd
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_rshlqcnfwbnoADd:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_15_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%zmm15,%zmm5,%zmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%zmm11,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vpshufb	%zmm29,%zmm5,%zmm11
+	vextracti32x4	$2,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_pFFxGDocmcrsCiu
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	16(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_pFFxGDocmcrsCiu
+.L_small_initial_partial_block_pFFxGDocmcrsCiu:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	32(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_pFFxGDocmcrsCiu:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_pFFxGDocmcrsCiu
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_pFFxGDocmcrsCiu:
+	jmp	.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF
+.L_small_initial_num_blocks_is_16_vFnGvjfgreazEcF:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%zmm15,%zmm5,%zmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%zmm11,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vpshufb	%zmm29,%zmm5,%zmm11
+	vextracti32x4	$3,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_pxgxlAFuDFzEyFg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	16(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_pxgxlAFuDFzEyFg:
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_pxgxlAFuDFzEyFg:
+.L_small_initial_blocks_encrypted_vFnGvjfgreazEcF:
+.L_ghash_done_vGFiuFCrdtyEdmj:
+	vmovdqu64	%xmm2,0(%rsi)
+.L_enc_dec_done_vGFiuFCrdtyEdmj:
+
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+	vmovdqu64	%xmm14,64(%rsi)
+.L_enc_dec_abort_vGFiuFCrdtyEdmj:
+	jmp	.Lexit_gcm_encrypt
+.align	32
+.Laes_gcm_encrypt_256_avx512:
+	orq	%r8,%r8
+	je	.L_enc_dec_abort_vswthpvgnuoqDhk
+	xorq	%r14,%r14
+	vmovdqu64	64(%rsi),%xmm14
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+
+	movl	(%rdx),%eax
+	orq	%rax,%rax
+	je	.L_partial_block_done_mjithqdhnBgwyld
+	movl	$16,%r10d
+	leaq	byte_len_to_mask_table(%rip),%r12
+	cmpq	%r10,%r8
+	cmovcq	%r8,%r10
+	kmovw	(%r12,%r10,2),%k1
+	vmovdqu8	(%rcx),%xmm0{%k1}{z}
+
+	vmovdqu64	16(%rsi),%xmm3
+
+	leaq	96(%rsi),%r10
+	vmovdqu64	240(%r10),%xmm4
+
+
+
+	leaq	SHIFT_MASK(%rip),%r12
+	addq	%rax,%r12
+	vmovdqu64	(%r12),%xmm5
+	vpshufb	%xmm5,%xmm3,%xmm3
+	vpxorq	%xmm0,%xmm3,%xmm3
+
+
+	leaq	(%r8,%rax,1),%r13
+	subq	$16,%r13
+	jge	.L_no_extra_mask_mjithqdhnBgwyld
+	subq	%r13,%r12
+.L_no_extra_mask_mjithqdhnBgwyld:
+
+
+
+	vmovdqu64	16(%r12),%xmm0
+	vpand	%xmm0,%xmm3,%xmm3
+	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
+	vpshufb	%xmm5,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm14,%xmm14
+	cmpq	$0,%r13
+	jl	.L_partial_incomplete_mjithqdhnBgwyld
+
+	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
+	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
+	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
+	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
+	vpxorq	%xmm11,%xmm14,%xmm14
+
+	vpsrldq	$8,%xmm14,%xmm11
+	vpslldq	$8,%xmm14,%xmm14
+	vpxorq	%xmm11,%xmm7,%xmm7
+	vpxorq	%xmm10,%xmm14,%xmm14
+
+
+
+	vmovdqu64	POLY2(%rip),%xmm11
+
+	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
+	vpslldq	$8,%xmm10,%xmm10
+	vpxorq	%xmm10,%xmm14,%xmm14
+
+
+
+	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
+	vpsrldq	$4,%xmm10,%xmm10
+	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+
+	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14
+
+	movl	$0,(%rdx)
+
+	movq	%rax,%r12
+	movq	$16,%rax
+	subq	%r12,%rax
+	jmp	.L_enc_dec_done_mjithqdhnBgwyld
+
+.L_partial_incomplete_mjithqdhnBgwyld:
+	addl	%r8d,(%rdx)
+	movq	%r8,%rax
+
+.L_enc_dec_done_mjithqdhnBgwyld:
+
+
+	leaq	byte_len_to_mask_table(%rip),%r12
+	kmovw	(%r12,%rax,2),%k1
+
+	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
+	vpshufb	%xmm5,%xmm3,%xmm3
+	movq	%r9,%r12
+	vmovdqu8	%xmm3,(%r12){%k1}
+.L_partial_block_done_mjithqdhnBgwyld:
+	vmovdqu64	0(%rsi),%xmm2
+	subq	%rax,%r8
+	je	.L_enc_dec_done_vswthpvgnuoqDhk
+	cmpq	$256,%r8
+	jbe	.L_message_below_equal_16_blocks_vswthpvgnuoqDhk
+
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
+	vmovdqa64	ddq_addbe_1234(%rip),%zmm28
+
+
+
+
+
+
+	vmovd	%xmm2,%r15d
+	andl	$255,%r15d
+
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpshufb	%zmm29,%zmm2,%zmm2
+
+
+
+	cmpb	$240,%r15b
+	jae	.L_next_16_overflow_vkvqFaakoGcFhgj
+	vpaddd	%zmm28,%zmm2,%zmm7
+	vpaddd	%zmm27,%zmm7,%zmm10
+	vpaddd	%zmm27,%zmm10,%zmm11
+	vpaddd	%zmm27,%zmm11,%zmm12
+	jmp	.L_next_16_ok_vkvqFaakoGcFhgj
+.L_next_16_overflow_vkvqFaakoGcFhgj:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm12
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
+	vpaddd	%zmm12,%zmm7,%zmm10
+	vpaddd	%zmm12,%zmm10,%zmm11
+	vpaddd	%zmm12,%zmm11,%zmm12
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+.L_next_16_ok_vkvqFaakoGcFhgj:
+	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
+	addb	$16,%r15b
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm0
+	vmovdqu8	64(%rcx,%rax,1),%zmm3
+	vmovdqu8	128(%rcx,%rax,1),%zmm4
+	vmovdqu8	192(%rcx,%rax,1),%zmm5
+
+
+	vbroadcastf64x2	0(%rdi),%zmm6
+	vpxorq	%zmm6,%zmm7,%zmm7
+	vpxorq	%zmm6,%zmm10,%zmm10
+	vpxorq	%zmm6,%zmm11,%zmm11
+	vpxorq	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	16(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	32(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	48(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	64(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	80(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	96(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	112(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	128(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	144(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	160(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	176(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	192(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	208(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	224(%rdi),%zmm6
+	vaesenclast	%zmm6,%zmm7,%zmm7
+	vaesenclast	%zmm6,%zmm10,%zmm10
+	vaesenclast	%zmm6,%zmm11,%zmm11
+	vaesenclast	%zmm6,%zmm12,%zmm12
+
+
+	vpxorq	%zmm0,%zmm7,%zmm7
+	vpxorq	%zmm3,%zmm10,%zmm10
+	vpxorq	%zmm4,%zmm11,%zmm11
+	vpxorq	%zmm5,%zmm12,%zmm12
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm7,0(%r10,%rax,1)
+	vmovdqu8	%zmm10,64(%r10,%rax,1)
+	vmovdqu8	%zmm11,128(%r10,%rax,1)
+	vmovdqu8	%zmm12,192(%r10,%rax,1)
+
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+	vmovdqa64	%zmm7,768(%rsp)
+	vmovdqa64	%zmm10,832(%rsp)
+	vmovdqa64	%zmm11,896(%rsp)
+	vmovdqa64	%zmm12,960(%rsp)
+	leaq	96(%rsi),%r12
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_bmmFrxeGegDdFmg
+
+	vmovdqu64	192(%r12),%zmm0
+	vmovdqu64	%zmm0,704(%rsp)
+
+	vmovdqu64	128(%r12),%zmm3
+	vmovdqu64	%zmm3,640(%rsp)
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	64(%r12),%zmm4
+	vmovdqu64	%zmm4,576(%rsp)
+
+	vmovdqu64	0(%r12),%zmm5
+	vmovdqu64	%zmm5,512(%rsp)
+.L_skip_hkeys_precomputation_bmmFrxeGegDdFmg:
+	cmpq	$512,%r8
+	jb	.L_message_below_32_blocks_vswthpvgnuoqDhk
+
+
+
+	cmpb	$240,%r15b
+	jae	.L_next_16_overflow_lnAfDEojfpnoxeq
+	vpaddd	%zmm28,%zmm2,%zmm7
+	vpaddd	%zmm27,%zmm7,%zmm10
+	vpaddd	%zmm27,%zmm10,%zmm11
+	vpaddd	%zmm27,%zmm11,%zmm12
+	jmp	.L_next_16_ok_lnAfDEojfpnoxeq
+.L_next_16_overflow_lnAfDEojfpnoxeq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm12
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
+	vpaddd	%zmm12,%zmm7,%zmm10
+	vpaddd	%zmm12,%zmm10,%zmm11
+	vpaddd	%zmm12,%zmm11,%zmm12
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+.L_next_16_ok_lnAfDEojfpnoxeq:
+	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
+	addb	$16,%r15b
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm0
+	vmovdqu8	320(%rcx,%rax,1),%zmm3
+	vmovdqu8	384(%rcx,%rax,1),%zmm4
+	vmovdqu8	448(%rcx,%rax,1),%zmm5
+
+
+	vbroadcastf64x2	0(%rdi),%zmm6
+	vpxorq	%zmm6,%zmm7,%zmm7
+	vpxorq	%zmm6,%zmm10,%zmm10
+	vpxorq	%zmm6,%zmm11,%zmm11
+	vpxorq	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	16(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	32(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	48(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	64(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	80(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	96(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	112(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	128(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	144(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	160(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	176(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	192(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	208(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	224(%rdi),%zmm6
+	vaesenclast	%zmm6,%zmm7,%zmm7
+	vaesenclast	%zmm6,%zmm10,%zmm10
+	vaesenclast	%zmm6,%zmm11,%zmm11
+	vaesenclast	%zmm6,%zmm12,%zmm12
+
+
+	vpxorq	%zmm0,%zmm7,%zmm7
+	vpxorq	%zmm3,%zmm10,%zmm10
+	vpxorq	%zmm4,%zmm11,%zmm11
+	vpxorq	%zmm5,%zmm12,%zmm12
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm7,256(%r10,%rax,1)
+	vmovdqu8	%zmm10,320(%r10,%rax,1)
+	vmovdqu8	%zmm11,384(%r10,%rax,1)
+	vmovdqu8	%zmm12,448(%r10,%rax,1)
+
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+	vmovdqa64	%zmm7,1024(%rsp)
+	vmovdqa64	%zmm10,1088(%rsp)
+	vmovdqa64	%zmm11,1152(%rsp)
+	vmovdqa64	%zmm12,1216(%rsp)
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_ngGbfuByhxgtvAm
+	vmovdqu64	640(%rsp),%zmm3
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	576(%rsp),%zmm4
+	vmovdqu64	512(%rsp),%zmm5
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,256(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,192(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,128(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,64(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,0(%rsp)
+.L_skip_hkeys_precomputation_ngGbfuByhxgtvAm:
+	movq	$1,%r14
+	addq	$512,%rax
+	subq	$512,%r8
+
+	cmpq	$768,%r8
+	jb	.L_no_more_big_nblocks_vswthpvgnuoqDhk
+.L_encrypt_big_nblocks_vswthpvgnuoqDhk:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_okyBBFviGprGAzb
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_okyBBFviGprGAzb
+.L_16_blocks_overflow_okyBBFviGprGAzb:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_okyBBFviGprGAzb:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_xfweypgbbDviwin
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_xfweypgbbDviwin
+.L_16_blocks_overflow_xfweypgbbDviwin:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_xfweypgbbDviwin:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	256(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	320(%rsp),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	384(%rsp),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	448(%rsp),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm17
+	vmovdqu8	320(%rcx,%rax,1),%zmm19
+	vmovdqu8	384(%rcx,%rax,1),%zmm20
+	vmovdqu8	448(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,256(%r10,%rax,1)
+	vmovdqu8	%zmm3,320(%r10,%rax,1)
+	vmovdqu8	%zmm4,384(%r10,%rax,1)
+	vmovdqu8	%zmm5,448(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,768(%rsp)
+	vmovdqa64	%zmm3,832(%rsp)
+	vmovdqa64	%zmm4,896(%rsp)
+	vmovdqa64	%zmm5,960(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_xDcpxobnbtEEwei
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_xDcpxobnbtEEwei
+.L_16_blocks_overflow_xDcpxobnbtEEwei:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_xDcpxobnbtEEwei:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	512(%rcx,%rax,1),%zmm17
+	vmovdqu8	576(%rcx,%rax,1),%zmm19
+	vmovdqu8	640(%rcx,%rax,1),%zmm20
+	vmovdqu8	704(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+
+
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
+	vpxorq	%zmm24,%zmm6,%zmm6
+	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
+	vpxorq	%zmm25,%zmm7,%zmm7
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vextracti64x4	$1,%zmm6,%ymm12
+	vpxorq	%ymm12,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm12
+	vpxorq	%xmm12,%xmm6,%xmm6
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,512(%r10,%rax,1)
+	vmovdqu8	%zmm3,576(%r10,%rax,1)
+	vmovdqu8	%zmm4,640(%r10,%rax,1)
+	vmovdqu8	%zmm5,704(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,1024(%rsp)
+	vmovdqa64	%zmm3,1088(%rsp)
+	vmovdqa64	%zmm4,1152(%rsp)
+	vmovdqa64	%zmm5,1216(%rsp)
+	vmovdqa64	%zmm6,%zmm14
+
+	addq	$768,%rax
+	subq	$768,%r8
+	cmpq	$768,%r8
+	jae	.L_encrypt_big_nblocks_vswthpvgnuoqDhk
+
+.L_no_more_big_nblocks_vswthpvgnuoqDhk:
+
+	cmpq	$512,%r8
+	jae	.L_encrypt_32_blocks_vswthpvgnuoqDhk
+
+	cmpq	$256,%r8
+	jae	.L_encrypt_16_blocks_vswthpvgnuoqDhk
+.L_encrypt_0_blocks_ghash_32_vswthpvgnuoqDhk:
+	movl	%r8d,%r10d
+	andl	$~15,%r10d
+	movl	$256,%ebx
+	subl	%r10d,%ebx
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	addl	$256,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_bzjdBqvElwspgfd
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_bzjdBqvElwspgfd
+	jb	.L_last_num_blocks_is_7_1_bzjdBqvElwspgfd
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_bzjdBqvElwspgfd
+	jb	.L_last_num_blocks_is_11_9_bzjdBqvElwspgfd
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_bzjdBqvElwspgfd
+	ja	.L_last_num_blocks_is_16_bzjdBqvElwspgfd
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_bzjdBqvElwspgfd
+	jmp	.L_last_num_blocks_is_13_bzjdBqvElwspgfd
+
+.L_last_num_blocks_is_11_9_bzjdBqvElwspgfd:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_bzjdBqvElwspgfd
+	ja	.L_last_num_blocks_is_11_bzjdBqvElwspgfd
+	jmp	.L_last_num_blocks_is_9_bzjdBqvElwspgfd
+
+.L_last_num_blocks_is_7_1_bzjdBqvElwspgfd:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_bzjdBqvElwspgfd
+	jb	.L_last_num_blocks_is_3_1_bzjdBqvElwspgfd
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_bzjdBqvElwspgfd
+	je	.L_last_num_blocks_is_6_bzjdBqvElwspgfd
+	jmp	.L_last_num_blocks_is_5_bzjdBqvElwspgfd
+
+.L_last_num_blocks_is_3_1_bzjdBqvElwspgfd:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_bzjdBqvElwspgfd
+	je	.L_last_num_blocks_is_2_bzjdBqvElwspgfd
+.L_last_num_blocks_is_1_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_ckFElbzfacsvgnB
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_ckFElbzfacsvgnB
+
+.L_16_blocks_overflow_ckFElbzfacsvgnB:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_ckFElbzfacsvgnB:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_mACeAksrjpsEnzt
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_mACeAksrjpsEnzt
+.L_small_initial_partial_block_mACeAksrjpsEnzt:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_mACeAksrjpsEnzt
+.L_small_initial_compute_done_mACeAksrjpsEnzt:
+.L_after_reduction_mACeAksrjpsEnzt:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_2_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_cmwbiDCqlrwAFAv
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_cmwbiDCqlrwAFAv
+
+.L_16_blocks_overflow_cmwbiDCqlrwAFAv:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_cmwbiDCqlrwAFAv:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GABlrDfimxbyqtD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GABlrDfimxbyqtD
+.L_small_initial_partial_block_GABlrDfimxbyqtD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GABlrDfimxbyqtD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GABlrDfimxbyqtD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GABlrDfimxbyqtD:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_3_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_qmpbpuyhyxryChv
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_qmpbpuyhyxryChv
+
+.L_16_blocks_overflow_qmpbpuyhyxryChv:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_qmpbpuyhyxryChv:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_rmDdtkcdlxdfhiy
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_rmDdtkcdlxdfhiy
+.L_small_initial_partial_block_rmDdtkcdlxdfhiy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_rmDdtkcdlxdfhiy:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_rmDdtkcdlxdfhiy
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_rmDdtkcdlxdfhiy:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_4_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_uwwspnjjCdrpvlu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_uwwspnjjCdrpvlu
+
+.L_16_blocks_overflow_uwwspnjjCdrpvlu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_uwwspnjjCdrpvlu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_hyfnddqzzoGBBnf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_hyfnddqzzoGBBnf
+.L_small_initial_partial_block_hyfnddqzzoGBBnf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_hyfnddqzzoGBBnf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_hyfnddqzzoGBBnf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_hyfnddqzzoGBBnf:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_5_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_aaitAziorlewjhc
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_aaitAziorlewjhc
+
+.L_16_blocks_overflow_aaitAziorlewjhc:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_aaitAziorlewjhc:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%xmm29,%xmm3,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_pdusdscnCitkmpp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_pdusdscnCitkmpp
+.L_small_initial_partial_block_pdusdscnCitkmpp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_pdusdscnCitkmpp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_pdusdscnCitkmpp
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_pdusdscnCitkmpp:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_6_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_iAvDhFAGtAByBgw
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_iAvDhFAGtAByBgw
+
+.L_16_blocks_overflow_iAvDhFAGtAByBgw:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_iAvDhFAGtAByBgw:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%ymm29,%ymm3,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_fxqeutdmDalxaoE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_fxqeutdmDalxaoE
+.L_small_initial_partial_block_fxqeutdmDalxaoE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_fxqeutdmDalxaoE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_fxqeutdmDalxaoE
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_fxqeutdmDalxaoE:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_7_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_FqbmuAupFbDtgxw
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_FqbmuAupFbDtgxw
+
+.L_16_blocks_overflow_FqbmuAupFbDtgxw:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_FqbmuAupFbDtgxw:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AiqugjxjhlAGGyB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AiqugjxjhlAGGyB
+.L_small_initial_partial_block_AiqugjxjhlAGGyB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AiqugjxjhlAGGyB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AiqugjxjhlAGGyB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_AiqugjxjhlAGGyB:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_8_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_mecuathjsAwqrbt
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_mecuathjsAwqrbt
+
+.L_16_blocks_overflow_mecuathjsAwqrbt:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_mecuathjsAwqrbt:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ofEoBsuxjCzndEq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ofEoBsuxjCzndEq
+.L_small_initial_partial_block_ofEoBsuxjCzndEq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ofEoBsuxjCzndEq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ofEoBsuxjCzndEq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ofEoBsuxjCzndEq:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_9_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_EgmGmrAztsrsroo
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_EgmGmrAztsrsroo
+
+.L_16_blocks_overflow_EgmGmrAztsrsroo:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_EgmGmrAztsrsroo:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%xmm29,%xmm4,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_bbkcufcthtmlDnl
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_bbkcufcthtmlDnl
+.L_small_initial_partial_block_bbkcufcthtmlDnl:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_bbkcufcthtmlDnl:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_bbkcufcthtmlDnl
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_bbkcufcthtmlDnl:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_10_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_vpwaixFopvklBcy
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_vpwaixFopvklBcy
+
+.L_16_blocks_overflow_vpwaixFopvklBcy:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_vpwaixFopvklBcy:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%ymm29,%ymm4,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_rjxrtDgEfozesnf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_rjxrtDgEfozesnf
+.L_small_initial_partial_block_rjxrtDgEfozesnf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_rjxrtDgEfozesnf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_rjxrtDgEfozesnf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_rjxrtDgEfozesnf:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_11_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_GaEbdiphiEkEjcn
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_GaEbdiphiEkEjcn
+
+.L_16_blocks_overflow_GaEbdiphiEkEjcn:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_GaEbdiphiEkEjcn:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_rcGwwBoAcqreqgB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_rcGwwBoAcqreqgB
+.L_small_initial_partial_block_rcGwwBoAcqreqgB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_rcGwwBoAcqreqgB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_rcGwwBoAcqreqgB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_rcGwwBoAcqreqgB:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_12_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_kCnoqlkqoBqkoye
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_kCnoqlkqoBqkoye
+
+.L_16_blocks_overflow_kCnoqlkqoBqkoye:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_kCnoqlkqoBqkoye:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_oBEjGdpmqdGhmlc
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_oBEjGdpmqdGhmlc
+.L_small_initial_partial_block_oBEjGdpmqdGhmlc:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_oBEjGdpmqdGhmlc:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_oBEjGdpmqdGhmlc
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_oBEjGdpmqdGhmlc:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_13_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_xrfjFeymBEEnesE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_xrfjFeymBEEnesE
+
+.L_16_blocks_overflow_xrfjFeymBEEnesE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_xrfjFeymBEEnesE:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%xmm29,%xmm5,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_yyEwlCBypmwbFzq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_yyEwlCBypmwbFzq
+.L_small_initial_partial_block_yyEwlCBypmwbFzq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_yyEwlCBypmwbFzq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_yyEwlCBypmwbFzq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_yyEwlCBypmwbFzq:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_14_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_DzqkrubrbFlGqcF
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_DzqkrubrbFlGqcF
+
+.L_16_blocks_overflow_DzqkrubrbFlGqcF:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_DzqkrubrbFlGqcF:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%ymm29,%ymm5,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_uCaohccpsdhqvEa
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_uCaohccpsdhqvEa
+.L_small_initial_partial_block_uCaohccpsdhqvEa:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_uCaohccpsdhqvEa:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_uCaohccpsdhqvEa
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_uCaohccpsdhqvEa:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_15_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_tFGqEfaDCawwwzs
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_tFGqEfaDCawwwzs
+
+.L_16_blocks_overflow_tFGqEfaDCawwwzs:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_tFGqEfaDCawwwzs:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_pdmazkjCcmhuhEs
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_pdmazkjCcmhuhEs
+.L_small_initial_partial_block_pdmazkjCcmhuhEs:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_pdmazkjCcmhuhEs:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_pdmazkjCcmhuhEs
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_pdmazkjCcmhuhEs:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_16_bzjdBqvElwspgfd:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_DzAuzcFtatilkhb
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_DzAuzcFtatilkhb
+
+.L_16_blocks_overflow_DzAuzcFtatilkhb:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_DzAuzcFtatilkhb:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_eAeDBhzfhoiCDEy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_eAeDBhzfhoiCDEy:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_eAeDBhzfhoiCDEy:
+	jmp	.L_last_blocks_done_bzjdBqvElwspgfd
+.L_last_num_blocks_is_0_bzjdBqvElwspgfd:
+	vmovdqa64	1024(%rsp),%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1088(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1152(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1216(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_bzjdBqvElwspgfd:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_vswthpvgnuoqDhk
+.L_encrypt_32_blocks_vswthpvgnuoqDhk:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_ghzjAgueesGhlod
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_ghzjAgueesGhlod
+.L_16_blocks_overflow_ghzjAgueesGhlod:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_ghzjAgueesGhlod:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_wqkyylcvssdrcvg
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_wqkyylcvssdrcvg
+.L_16_blocks_overflow_wqkyylcvssdrcvg:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_wqkyylcvssdrcvg:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	256(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	320(%rsp),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	384(%rsp),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	448(%rsp),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm17
+	vmovdqu8	320(%rcx,%rax,1),%zmm19
+	vmovdqu8	384(%rcx,%rax,1),%zmm20
+	vmovdqu8	448(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,256(%r10,%rax,1)
+	vmovdqu8	%zmm3,320(%r10,%rax,1)
+	vmovdqu8	%zmm4,384(%r10,%rax,1)
+	vmovdqu8	%zmm5,448(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,768(%rsp)
+	vmovdqa64	%zmm3,832(%rsp)
+	vmovdqa64	%zmm4,896(%rsp)
+	vmovdqa64	%zmm5,960(%rsp)
+	vmovdqa64	1280(%rsp),%zmm13
+	vmovdqu64	512(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1344(%rsp),%zmm13
+	vmovdqu64	576(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1408(%rsp),%zmm13
+	vmovdqu64	640(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1472(%rsp),%zmm13
+	vmovdqu64	704(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+	subq	$512,%r8
+	addq	$512,%rax
+	movl	%r8d,%r10d
+	andl	$~15,%r10d
+	movl	$512,%ebx
+	subl	%r10d,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_exhzzhjdlrDFwuG
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_exhzzhjdlrDFwuG
+	jb	.L_last_num_blocks_is_7_1_exhzzhjdlrDFwuG
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_exhzzhjdlrDFwuG
+	jb	.L_last_num_blocks_is_11_9_exhzzhjdlrDFwuG
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_exhzzhjdlrDFwuG
+	ja	.L_last_num_blocks_is_16_exhzzhjdlrDFwuG
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_exhzzhjdlrDFwuG
+	jmp	.L_last_num_blocks_is_13_exhzzhjdlrDFwuG
+
+.L_last_num_blocks_is_11_9_exhzzhjdlrDFwuG:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_exhzzhjdlrDFwuG
+	ja	.L_last_num_blocks_is_11_exhzzhjdlrDFwuG
+	jmp	.L_last_num_blocks_is_9_exhzzhjdlrDFwuG
+
+.L_last_num_blocks_is_7_1_exhzzhjdlrDFwuG:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_exhzzhjdlrDFwuG
+	jb	.L_last_num_blocks_is_3_1_exhzzhjdlrDFwuG
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_exhzzhjdlrDFwuG
+	je	.L_last_num_blocks_is_6_exhzzhjdlrDFwuG
+	jmp	.L_last_num_blocks_is_5_exhzzhjdlrDFwuG
+
+.L_last_num_blocks_is_3_1_exhzzhjdlrDFwuG:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_exhzzhjdlrDFwuG
+	je	.L_last_num_blocks_is_2_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_1_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_tnaypqozsnopmzb
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_tnaypqozsnopmzb
+
+.L_16_blocks_overflow_tnaypqozsnopmzb:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_tnaypqozsnopmzb:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GjCggvabEhvmxtl
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GjCggvabEhvmxtl
+.L_small_initial_partial_block_GjCggvabEhvmxtl:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_GjCggvabEhvmxtl
+.L_small_initial_compute_done_GjCggvabEhvmxtl:
+.L_after_reduction_GjCggvabEhvmxtl:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_2_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_fzCaybtraCBoiim
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_fzCaybtraCBoiim
+
+.L_16_blocks_overflow_fzCaybtraCBoiim:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_fzCaybtraCBoiim:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GodDeapifureglz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GodDeapifureglz
+.L_small_initial_partial_block_GodDeapifureglz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GodDeapifureglz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GodDeapifureglz
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GodDeapifureglz:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_3_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_sDvCaeysoBcBGuF
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_sDvCaeysoBcBGuF
+
+.L_16_blocks_overflow_sDvCaeysoBcBGuF:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_sDvCaeysoBcBGuF:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_EigDfofgsuyCaka
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_EigDfofgsuyCaka
+.L_small_initial_partial_block_EigDfofgsuyCaka:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_EigDfofgsuyCaka:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_EigDfofgsuyCaka
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_EigDfofgsuyCaka:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_4_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_ykunmkDCdzAFemk
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_ykunmkDCdzAFemk
+
+.L_16_blocks_overflow_ykunmkDCdzAFemk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_ykunmkDCdzAFemk:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BzDatprdEebErxs
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BzDatprdEebErxs
+.L_small_initial_partial_block_BzDatprdEebErxs:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BzDatprdEebErxs:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BzDatprdEebErxs
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_BzDatprdEebErxs:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_5_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_EuFutgulygjlztA
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_EuFutgulygjlztA
+
+.L_16_blocks_overflow_EuFutgulygjlztA:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_EuFutgulygjlztA:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%xmm29,%xmm3,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_gwymbgejyptbrfA
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_gwymbgejyptbrfA
+.L_small_initial_partial_block_gwymbgejyptbrfA:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_gwymbgejyptbrfA:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_gwymbgejyptbrfA
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_gwymbgejyptbrfA:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_6_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_htmfsqxmfbozlGk
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_htmfsqxmfbozlGk
+
+.L_16_blocks_overflow_htmfsqxmfbozlGk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_htmfsqxmfbozlGk:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%ymm29,%ymm3,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ywwejygpbyozDzv
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ywwejygpbyozDzv
+.L_small_initial_partial_block_ywwejygpbyozDzv:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ywwejygpbyozDzv:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ywwejygpbyozDzv
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ywwejygpbyozDzv:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_7_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_loAlxvvuhexFufg
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_loAlxvvuhexFufg
+
+.L_16_blocks_overflow_loAlxvvuhexFufg:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_loAlxvvuhexFufg:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CaCqelBtGCGmnqf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CaCqelBtGCGmnqf
+.L_small_initial_partial_block_CaCqelBtGCGmnqf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CaCqelBtGCGmnqf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CaCqelBtGCGmnqf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CaCqelBtGCGmnqf:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_8_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_eDceGAtagmBjblu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_eDceGAtagmBjblu
+
+.L_16_blocks_overflow_eDceGAtagmBjblu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_eDceGAtagmBjblu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_zkcEeFGgdbbadgn
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_zkcEeFGgdbbadgn
+.L_small_initial_partial_block_zkcEeFGgdbbadgn:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zkcEeFGgdbbadgn:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_zkcEeFGgdbbadgn
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_zkcEeFGgdbbadgn:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_9_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_eopAxeEmdgjgntC
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_eopAxeEmdgjgntC
+
+.L_16_blocks_overflow_eopAxeEmdgjgntC:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_eopAxeEmdgjgntC:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%xmm29,%xmm4,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GggdlsbwyeAowFC
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GggdlsbwyeAowFC
+.L_small_initial_partial_block_GggdlsbwyeAowFC:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GggdlsbwyeAowFC:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GggdlsbwyeAowFC
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GggdlsbwyeAowFC:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_10_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_wDFhytvmrenjqCp
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_wDFhytvmrenjqCp
+
+.L_16_blocks_overflow_wDFhytvmrenjqCp:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_wDFhytvmrenjqCp:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%ymm29,%ymm4,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_pEBjpbgiaezxeqv
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_pEBjpbgiaezxeqv
+.L_small_initial_partial_block_pEBjpbgiaezxeqv:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_pEBjpbgiaezxeqv:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_pEBjpbgiaezxeqv
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_pEBjpbgiaezxeqv:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_11_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_brmEChEoldppBqi
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_brmEChEoldppBqi
+
+.L_16_blocks_overflow_brmEChEoldppBqi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_brmEChEoldppBqi:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_bcjlgstjdqenBau
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_bcjlgstjdqenBau
+.L_small_initial_partial_block_bcjlgstjdqenBau:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_bcjlgstjdqenBau:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_bcjlgstjdqenBau
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_bcjlgstjdqenBau:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_12_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_ECivbzszDirwxpG
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_ECivbzszDirwxpG
+
+.L_16_blocks_overflow_ECivbzszDirwxpG:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_ECivbzszDirwxpG:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ggBmepFwFbdskFv
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ggBmepFwFbdskFv
+.L_small_initial_partial_block_ggBmepFwFbdskFv:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ggBmepFwFbdskFv:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ggBmepFwFbdskFv
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ggBmepFwFbdskFv:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_13_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_ceCyzdkqrptBacE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_ceCyzdkqrptBacE
+
+.L_16_blocks_overflow_ceCyzdkqrptBacE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_ceCyzdkqrptBacE:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%xmm29,%xmm5,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_yoDwtukAFyFtsgf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_yoDwtukAFyFtsgf
+.L_small_initial_partial_block_yoDwtukAFyFtsgf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_yoDwtukAFyFtsgf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_yoDwtukAFyFtsgf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_yoDwtukAFyFtsgf:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_14_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_oFmlayCgcunawrA
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_oFmlayCgcunawrA
+
+.L_16_blocks_overflow_oFmlayCgcunawrA:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_oFmlayCgcunawrA:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%ymm29,%ymm5,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_unbmgGyzftriACf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_unbmgGyzftriACf
+.L_small_initial_partial_block_unbmgGyzftriACf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_unbmgGyzftriACf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_unbmgGyzftriACf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_unbmgGyzftriACf:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_15_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_utixiaAnbhhwjxr
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_utixiaAnbhhwjxr
+
+.L_16_blocks_overflow_utixiaAnbhhwjxr:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_utixiaAnbhhwjxr:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wzgGbsxazuonpgi
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wzgGbsxazuonpgi
+.L_small_initial_partial_block_wzgGbsxazuonpgi:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_wzgGbsxazuonpgi:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_wzgGbsxazuonpgi
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_wzgGbsxazuonpgi:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_16_exhzzhjdlrDFwuG:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_wmwGnzfCcuwbnqh
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_wmwGnzfCcuwbnqh
+
+.L_16_blocks_overflow_wmwGnzfCcuwbnqh:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_wmwGnzfCcuwbnqh:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_cvdtygcsasnsjlE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_cvdtygcsasnsjlE:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_cvdtygcsasnsjlE:
+	jmp	.L_last_blocks_done_exhzzhjdlrDFwuG
+.L_last_num_blocks_is_0_exhzzhjdlrDFwuG:
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_exhzzhjdlrDFwuG:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_vswthpvgnuoqDhk
+.L_encrypt_16_blocks_vswthpvgnuoqDhk:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_ArBzbGGBkouxsmz
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_ArBzbGGBkouxsmz
+.L_16_blocks_overflow_ArBzbGGBkouxsmz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_ArBzbGGBkouxsmz:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	vmovdqa64	1024(%rsp),%zmm13
+	vmovdqu64	256(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1088(%rsp),%zmm13
+	vmovdqu64	320(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1152(%rsp),%zmm13
+	vmovdqu64	384(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1216(%rsp),%zmm13
+	vmovdqu64	448(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	subq	$256,%r8
+	addq	$256,%rax
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_ABAqseoyifqBCxD
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_ABAqseoyifqBCxD
+	jb	.L_last_num_blocks_is_7_1_ABAqseoyifqBCxD
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_ABAqseoyifqBCxD
+	jb	.L_last_num_blocks_is_11_9_ABAqseoyifqBCxD
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_ABAqseoyifqBCxD
+	ja	.L_last_num_blocks_is_16_ABAqseoyifqBCxD
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_ABAqseoyifqBCxD
+	jmp	.L_last_num_blocks_is_13_ABAqseoyifqBCxD
+
+.L_last_num_blocks_is_11_9_ABAqseoyifqBCxD:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_ABAqseoyifqBCxD
+	ja	.L_last_num_blocks_is_11_ABAqseoyifqBCxD
+	jmp	.L_last_num_blocks_is_9_ABAqseoyifqBCxD
+
+.L_last_num_blocks_is_7_1_ABAqseoyifqBCxD:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_ABAqseoyifqBCxD
+	jb	.L_last_num_blocks_is_3_1_ABAqseoyifqBCxD
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_ABAqseoyifqBCxD
+	je	.L_last_num_blocks_is_6_ABAqseoyifqBCxD
+	jmp	.L_last_num_blocks_is_5_ABAqseoyifqBCxD
+
+.L_last_num_blocks_is_3_1_ABAqseoyifqBCxD:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_ABAqseoyifqBCxD
+	je	.L_last_num_blocks_is_2_ABAqseoyifqBCxD
+.L_last_num_blocks_is_1_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_krpnmbxhxwBolch
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_krpnmbxhxwBolch
+
+.L_16_blocks_overflow_krpnmbxhxwBolch:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_krpnmbxhxwBolch:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_EynaftlqEegfdFf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_EynaftlqEegfdFf
+.L_small_initial_partial_block_EynaftlqEegfdFf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_EynaftlqEegfdFf
+.L_small_initial_compute_done_EynaftlqEegfdFf:
+.L_after_reduction_EynaftlqEegfdFf:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_2_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_DlEEafdDwjquiDv
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_DlEEafdDwjquiDv
+
+.L_16_blocks_overflow_DlEEafdDwjquiDv:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_DlEEafdDwjquiDv:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_defjtwAhtflmkgo
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_defjtwAhtflmkgo
+.L_small_initial_partial_block_defjtwAhtflmkgo:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_defjtwAhtflmkgo:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_defjtwAhtflmkgo
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_defjtwAhtflmkgo:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_3_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_txAinfnGiqxwcuC
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_txAinfnGiqxwcuC
+
+.L_16_blocks_overflow_txAinfnGiqxwcuC:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_txAinfnGiqxwcuC:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GEDsdukCznbxleu
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GEDsdukCznbxleu
+.L_small_initial_partial_block_GEDsdukCznbxleu:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GEDsdukCznbxleu:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GEDsdukCznbxleu
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GEDsdukCznbxleu:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_4_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_EejFcEduDdGDmic
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_EejFcEduDdGDmic
+
+.L_16_blocks_overflow_EejFcEduDdGDmic:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_EejFcEduDdGDmic:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_hbwnsmqujrzfjdF
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_hbwnsmqujrzfjdF
+.L_small_initial_partial_block_hbwnsmqujrzfjdF:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_hbwnsmqujrzfjdF:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_hbwnsmqujrzfjdF
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_hbwnsmqujrzfjdF:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_5_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_BfqdfdrDfAkivbf
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_BfqdfdrDfAkivbf
+
+.L_16_blocks_overflow_BfqdfdrDfAkivbf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_BfqdfdrDfAkivbf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%xmm29,%xmm3,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tCjqboCtmBhbioq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tCjqboCtmBhbioq
+.L_small_initial_partial_block_tCjqboCtmBhbioq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tCjqboCtmBhbioq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tCjqboCtmBhbioq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tCjqboCtmBhbioq:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_6_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_cnhccCodqgvsDhm
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_cnhccCodqgvsDhm
+
+.L_16_blocks_overflow_cnhccCodqgvsDhm:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_cnhccCodqgvsDhm:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%ymm29,%ymm3,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wEoquAkwjwBDAbf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wEoquAkwjwBDAbf
+.L_small_initial_partial_block_wEoquAkwjwBDAbf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_wEoquAkwjwBDAbf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_wEoquAkwjwBDAbf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_wEoquAkwjwBDAbf:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_7_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_eyiECaBodsFbFqo
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_eyiECaBodsFbFqo
+
+.L_16_blocks_overflow_eyiECaBodsFbFqo:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_eyiECaBodsFbFqo:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_lyyhtlkrrFlvCBu
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_lyyhtlkrrFlvCBu
+.L_small_initial_partial_block_lyyhtlkrrFlvCBu:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_lyyhtlkrrFlvCBu:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_lyyhtlkrrFlvCBu
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_lyyhtlkrrFlvCBu:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_8_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_kxBxpghnbdmabbm
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_kxBxpghnbdmabbm
+
+.L_16_blocks_overflow_kxBxpghnbdmabbm:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_kxBxpghnbdmabbm:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_kBjtFjzfcuiGseF
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_kBjtFjzfcuiGseF
+.L_small_initial_partial_block_kBjtFjzfcuiGseF:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_kBjtFjzfcuiGseF:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_kBjtFjzfcuiGseF
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_kBjtFjzfcuiGseF:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_9_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_uizDsazcoEmDrep
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_uizDsazcoEmDrep
+
+.L_16_blocks_overflow_uizDsazcoEmDrep:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_uizDsazcoEmDrep:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%xmm29,%xmm4,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_anwByGwsumcvoBm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_anwByGwsumcvoBm
+.L_small_initial_partial_block_anwByGwsumcvoBm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_anwByGwsumcvoBm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_anwByGwsumcvoBm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_anwByGwsumcvoBm:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_10_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_brtGrCkFqDzcrim
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_brtGrCkFqDzcrim
+
+.L_16_blocks_overflow_brtGrCkFqDzcrim:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_brtGrCkFqDzcrim:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%ymm29,%ymm4,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_hykFaoolDdGhEdy
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_hykFaoolDdGhEdy
+.L_small_initial_partial_block_hykFaoolDdGhEdy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_hykFaoolDdGhEdy:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_hykFaoolDdGhEdy
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_hykFaoolDdGhEdy:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_11_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_hEdEgzkmxygFdte
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_hEdEgzkmxygFdte
+
+.L_16_blocks_overflow_hEdEgzkmxygFdte:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_hEdEgzkmxygFdte:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BdwGvfylayyqgck
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BdwGvfylayyqgck
+.L_small_initial_partial_block_BdwGvfylayyqgck:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BdwGvfylayyqgck:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BdwGvfylayyqgck
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_BdwGvfylayyqgck:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_12_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_xAjziptBqrfppyo
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_xAjziptBqrfppyo
+
+.L_16_blocks_overflow_xAjziptBqrfppyo:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_xAjziptBqrfppyo:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_gxzAzkrnnGgfDoz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_gxzAzkrnnGgfDoz
+.L_small_initial_partial_block_gxzAzkrnnGgfDoz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_gxzAzkrnnGgfDoz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_gxzAzkrnnGgfDoz
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_gxzAzkrnnGgfDoz:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_13_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_gpsEvnydqpEdnfB
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_gpsEvnydqpEdnfB
+
+.L_16_blocks_overflow_gpsEvnydqpEdnfB:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_gpsEvnydqpEdnfB:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%xmm29,%xmm5,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_FFjcoijrcAkBDhv
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_FFjcoijrcAkBDhv
+.L_small_initial_partial_block_FFjcoijrcAkBDhv:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_FFjcoijrcAkBDhv:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_FFjcoijrcAkBDhv
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_FFjcoijrcAkBDhv:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_14_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_iDfoqFwydGavabn
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_iDfoqFwydGavabn
+
+.L_16_blocks_overflow_iDfoqFwydGavabn:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_iDfoqFwydGavabn:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%ymm29,%ymm5,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_woEcwyCxvtuxbeq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_woEcwyCxvtuxbeq
+.L_small_initial_partial_block_woEcwyCxvtuxbeq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_woEcwyCxvtuxbeq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_woEcwyCxvtuxbeq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_woEcwyCxvtuxbeq:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_15_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_zzvqsiijalEwwxa
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_zzvqsiijalEwwxa
+
+.L_16_blocks_overflow_zzvqsiijalEwwxa:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_zzvqsiijalEwwxa:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_jybvjumEnvCvubq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_jybvjumEnvCvubq
+.L_small_initial_partial_block_jybvjumEnvCvubq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_jybvjumEnvCvubq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_jybvjumEnvCvubq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_jybvjumEnvCvubq:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_16_ABAqseoyifqBCxD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_Amvdvdgzwfrukik
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_Amvdvdgzwfrukik
+
+.L_16_blocks_overflow_Amvdvdgzwfrukik:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_Amvdvdgzwfrukik:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_sgehhxdsunCkyqe:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_sgehhxdsunCkyqe:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_sgehhxdsunCkyqe:
+	jmp	.L_last_blocks_done_ABAqseoyifqBCxD
+.L_last_num_blocks_is_0_ABAqseoyifqBCxD:
+	vmovdqa64	1280(%rsp),%zmm13
+	vmovdqu64	512(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1344(%rsp),%zmm13
+	vmovdqu64	576(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1408(%rsp),%zmm13
+	vmovdqu64	640(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1472(%rsp),%zmm13
+	vmovdqu64	704(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_ABAqseoyifqBCxD:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_vswthpvgnuoqDhk
+
+.L_message_below_32_blocks_vswthpvgnuoqDhk:
+
+
+	subq	$256,%r8
+	addq	$256,%rax
+	movl	%r8d,%r10d
+	leaq	96(%rsi),%r12
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_oxbcCGwdArGieal
+	vmovdqu64	640(%rsp),%zmm3
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	576(%rsp),%zmm4
+	vmovdqu64	512(%rsp),%zmm5
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,256(%rsp)
+.L_skip_hkeys_precomputation_oxbcCGwdArGieal:
+	movq	$1,%r14
+	andl	$~15,%r10d
+	movl	$512,%ebx
+	subl	%r10d,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_gntjwhlgsqvClFh
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_gntjwhlgsqvClFh
+	jb	.L_last_num_blocks_is_7_1_gntjwhlgsqvClFh
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_gntjwhlgsqvClFh
+	jb	.L_last_num_blocks_is_11_9_gntjwhlgsqvClFh
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_gntjwhlgsqvClFh
+	ja	.L_last_num_blocks_is_16_gntjwhlgsqvClFh
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_gntjwhlgsqvClFh
+	jmp	.L_last_num_blocks_is_13_gntjwhlgsqvClFh
+
+.L_last_num_blocks_is_11_9_gntjwhlgsqvClFh:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_gntjwhlgsqvClFh
+	ja	.L_last_num_blocks_is_11_gntjwhlgsqvClFh
+	jmp	.L_last_num_blocks_is_9_gntjwhlgsqvClFh
+
+.L_last_num_blocks_is_7_1_gntjwhlgsqvClFh:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_gntjwhlgsqvClFh
+	jb	.L_last_num_blocks_is_3_1_gntjwhlgsqvClFh
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_gntjwhlgsqvClFh
+	je	.L_last_num_blocks_is_6_gntjwhlgsqvClFh
+	jmp	.L_last_num_blocks_is_5_gntjwhlgsqvClFh
+
+.L_last_num_blocks_is_3_1_gntjwhlgsqvClFh:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_gntjwhlgsqvClFh
+	je	.L_last_num_blocks_is_2_gntjwhlgsqvClFh
+.L_last_num_blocks_is_1_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_ucgmGiriDaegCjp
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_ucgmGiriDaegCjp
+
+.L_16_blocks_overflow_ucgmGiriDaegCjp:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_ucgmGiriDaegCjp:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AfabvcDBwqBzEjk
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AfabvcDBwqBzEjk
+.L_small_initial_partial_block_AfabvcDBwqBzEjk:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_AfabvcDBwqBzEjk
+.L_small_initial_compute_done_AfabvcDBwqBzEjk:
+.L_after_reduction_AfabvcDBwqBzEjk:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_2_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_dzDrcdEbjiseubB
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_dzDrcdEbjiseubB
+
+.L_16_blocks_overflow_dzDrcdEbjiseubB:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_dzDrcdEbjiseubB:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dkGEvkzDAewapmf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dkGEvkzDAewapmf
+.L_small_initial_partial_block_dkGEvkzDAewapmf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_dkGEvkzDAewapmf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_dkGEvkzDAewapmf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_dkGEvkzDAewapmf:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_3_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_CgDcreEbuGrlxGk
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_CgDcreEbuGrlxGk
+
+.L_16_blocks_overflow_CgDcreEbuGrlxGk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_CgDcreEbuGrlxGk:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GgAuGtxwcqfhfxk
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GgAuGtxwcqfhfxk
+.L_small_initial_partial_block_GgAuGtxwcqfhfxk:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GgAuGtxwcqfhfxk:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GgAuGtxwcqfhfxk
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GgAuGtxwcqfhfxk:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_4_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_smAurufvezqzeob
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_smAurufvezqzeob
+
+.L_16_blocks_overflow_smAurufvezqzeob:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_smAurufvezqzeob:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_jpCkesjoonmBncj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_jpCkesjoonmBncj
+.L_small_initial_partial_block_jpCkesjoonmBncj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_jpCkesjoonmBncj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_jpCkesjoonmBncj
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_jpCkesjoonmBncj:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_5_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_buekwBovxgalqne
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_buekwBovxgalqne
+
+.L_16_blocks_overflow_buekwBovxgalqne:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_buekwBovxgalqne:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%xmm29,%xmm3,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_DwCpEAedzntcccb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_DwCpEAedzntcccb
+.L_small_initial_partial_block_DwCpEAedzntcccb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_DwCpEAedzntcccb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_DwCpEAedzntcccb
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_DwCpEAedzntcccb:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_6_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_stkwmtBEFGpBBma
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_stkwmtBEFGpBBma
+
+.L_16_blocks_overflow_stkwmtBEFGpBBma:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_stkwmtBEFGpBBma:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%ymm29,%ymm3,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_aouvBhztatfkEgm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_aouvBhztatfkEgm
+.L_small_initial_partial_block_aouvBhztatfkEgm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_aouvBhztatfkEgm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_aouvBhztatfkEgm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_aouvBhztatfkEgm:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_7_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_weEfrDEebekjmfA
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_weEfrDEebekjmfA
+
+.L_16_blocks_overflow_weEfrDEebekjmfA:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_weEfrDEebekjmfA:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_stgGtCBdtaeChAq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_stgGtCBdtaeChAq
+.L_small_initial_partial_block_stgGtCBdtaeChAq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_stgGtCBdtaeChAq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_stgGtCBdtaeChAq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_stgGtCBdtaeChAq:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_8_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_DmuutvsdDsoeubG
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_DmuutvsdDsoeubG
+
+.L_16_blocks_overflow_DmuutvsdDsoeubG:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_DmuutvsdDsoeubG:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_zEdnhAmEtfubnlD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_zEdnhAmEtfubnlD
+.L_small_initial_partial_block_zEdnhAmEtfubnlD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zEdnhAmEtfubnlD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_zEdnhAmEtfubnlD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_zEdnhAmEtfubnlD:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_9_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_GGxavnwnEwlABey
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_GGxavnwnEwlABey
+
+.L_16_blocks_overflow_GGxavnwnEwlABey:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_GGxavnwnEwlABey:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%xmm29,%xmm4,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_izBrxAlictncdiA
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_izBrxAlictncdiA
+.L_small_initial_partial_block_izBrxAlictncdiA:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_izBrxAlictncdiA:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_izBrxAlictncdiA
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_izBrxAlictncdiA:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_10_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_xddomFygmszpAmA
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_xddomFygmszpAmA
+
+.L_16_blocks_overflow_xddomFygmszpAmA:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_xddomFygmszpAmA:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%ymm29,%ymm4,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_hgtnEyjrhsnAdig
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_hgtnEyjrhsnAdig
+.L_small_initial_partial_block_hgtnEyjrhsnAdig:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_hgtnEyjrhsnAdig:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_hgtnEyjrhsnAdig
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_hgtnEyjrhsnAdig:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_11_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_cFuCFbDsusecmya
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_cFuCFbDsusecmya
+
+.L_16_blocks_overflow_cFuCFbDsusecmya:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_cFuCFbDsusecmya:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_nApnfelDpFxglty
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_nApnfelDpFxglty
+.L_small_initial_partial_block_nApnfelDpFxglty:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_nApnfelDpFxglty:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_nApnfelDpFxglty
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_nApnfelDpFxglty:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_12_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_sAvlxCAvlvsleAf
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_sAvlxCAvlvsleAf
+
+.L_16_blocks_overflow_sAvlxCAvlvsleAf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_sAvlxCAvlvsleAf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dcFrGkjEfrqacDo
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dcFrGkjEfrqacDo
+.L_small_initial_partial_block_dcFrGkjEfrqacDo:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_dcFrGkjEfrqacDo:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_dcFrGkjEfrqacDo
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_dcFrGkjEfrqacDo:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_13_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_mEsafjacwbhlAne
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_mEsafjacwbhlAne
+
+.L_16_blocks_overflow_mEsafjacwbhlAne:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_mEsafjacwbhlAne:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%xmm29,%xmm5,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_zadocpeAihykieg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_zadocpeAihykieg
+.L_small_initial_partial_block_zadocpeAihykieg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zadocpeAihykieg:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_zadocpeAihykieg
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_zadocpeAihykieg:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_14_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_ealFarwqtjzaptD
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_ealFarwqtjzaptD
+
+.L_16_blocks_overflow_ealFarwqtjzaptD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_ealFarwqtjzaptD:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%ymm29,%ymm5,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_xwbcGpmasnletau
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_xwbcGpmasnletau
+.L_small_initial_partial_block_xwbcGpmasnletau:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_xwbcGpmasnletau:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_xwbcGpmasnletau
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_xwbcGpmasnletau:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_15_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_abrgFjcxisgrldy
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_abrgFjcxisgrldy
+
+.L_16_blocks_overflow_abrgFjcxisgrldy:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_abrgFjcxisgrldy:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_bizxcFfFgfkvbde
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_bizxcFfFgfkvbde
+.L_small_initial_partial_block_bizxcFfFgfkvbde:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_bizxcFfFgfkvbde:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_bizxcFfFgfkvbde
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_bizxcFfFgfkvbde:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_16_gntjwhlgsqvClFh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_CtfDvkxduzaFxhz
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_CtfDvkxduzaFxhz
+
+.L_16_blocks_overflow_CtfDvkxduzaFxhz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_CtfDvkxduzaFxhz:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm17
+	vpshufb	%zmm29,%zmm3,%zmm19
+	vpshufb	%zmm29,%zmm4,%zmm20
+	vpshufb	%zmm29,%zmm5,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_nzntswrmBahlADf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_nzntswrmBahlADf:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_nzntswrmBahlADf:
+	jmp	.L_last_blocks_done_gntjwhlgsqvClFh
+.L_last_num_blocks_is_0_gntjwhlgsqvClFh:
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_gntjwhlgsqvClFh:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_vswthpvgnuoqDhk
+
+.L_message_below_equal_16_blocks_vswthpvgnuoqDhk:
+
+
+	movl	%r8d,%r12d
+	addl	$15,%r12d
+	shrl	$4,%r12d
+	cmpq	$8,%r12
+	je	.L_small_initial_num_blocks_is_8_nuilneqyeasoeFh
+	jl	.L_small_initial_num_blocks_is_7_1_nuilneqyeasoeFh
+
+
+	cmpq	$12,%r12
+	je	.L_small_initial_num_blocks_is_12_nuilneqyeasoeFh
+	jl	.L_small_initial_num_blocks_is_11_9_nuilneqyeasoeFh
+
+
+	cmpq	$16,%r12
+	je	.L_small_initial_num_blocks_is_16_nuilneqyeasoeFh
+	cmpq	$15,%r12
+	je	.L_small_initial_num_blocks_is_15_nuilneqyeasoeFh
+	cmpq	$14,%r12
+	je	.L_small_initial_num_blocks_is_14_nuilneqyeasoeFh
+	jmp	.L_small_initial_num_blocks_is_13_nuilneqyeasoeFh
+
+.L_small_initial_num_blocks_is_11_9_nuilneqyeasoeFh:
+
+	cmpq	$11,%r12
+	je	.L_small_initial_num_blocks_is_11_nuilneqyeasoeFh
+	cmpq	$10,%r12
+	je	.L_small_initial_num_blocks_is_10_nuilneqyeasoeFh
+	jmp	.L_small_initial_num_blocks_is_9_nuilneqyeasoeFh
+
+.L_small_initial_num_blocks_is_7_1_nuilneqyeasoeFh:
+	cmpq	$4,%r12
+	je	.L_small_initial_num_blocks_is_4_nuilneqyeasoeFh
+	jl	.L_small_initial_num_blocks_is_3_1_nuilneqyeasoeFh
+
+	cmpq	$7,%r12
+	je	.L_small_initial_num_blocks_is_7_nuilneqyeasoeFh
+	cmpq	$6,%r12
+	je	.L_small_initial_num_blocks_is_6_nuilneqyeasoeFh
+	jmp	.L_small_initial_num_blocks_is_5_nuilneqyeasoeFh
+
+.L_small_initial_num_blocks_is_3_1_nuilneqyeasoeFh:
+
+	cmpq	$3,%r12
+	je	.L_small_initial_num_blocks_is_3_nuilneqyeasoeFh
+	cmpq	$2,%r12
+	je	.L_small_initial_num_blocks_is_2_nuilneqyeasoeFh
+
+
+
+
+
+.L_small_initial_num_blocks_is_1_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%xmm29
+	vpaddd	ONE(%rip),%xmm2,%xmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vpshufb	%xmm29,%xmm0,%xmm0
+	vmovdqu8	0(%rcx,%rax,1),%xmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%xmm15,%xmm0,%xmm0
+	vpxorq	%xmm6,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm0,%xmm6
+	vextracti32x4	$0,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_mGCFfFqEyhgaEpA
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_mGCFfFqEyhgaEpA
+.L_small_initial_partial_block_mGCFfFqEyhgaEpA:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm13,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_mGCFfFqEyhgaEpA
+.L_small_initial_compute_done_mGCFfFqEyhgaEpA:
+.L_after_reduction_mGCFfFqEyhgaEpA:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_2_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%ymm29
+	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
+	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vpshufb	%ymm29,%ymm0,%ymm0
+	vmovdqu8	0(%rcx,%rax,1),%ymm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%ymm15,%ymm0,%ymm0
+	vpxorq	%ymm6,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm0,%ymm6
+	vextracti32x4	$1,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_umEBArkuBvyGCrA
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_umEBArkuBvyGCrA
+.L_small_initial_partial_block_umEBArkuBvyGCrA:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_umEBArkuBvyGCrA:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_umEBArkuBvyGCrA
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_umEBArkuBvyGCrA:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_3_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vextracti32x4	$2,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_earitpDccqqbEai
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_earitpDccqqbEai
+.L_small_initial_partial_block_earitpDccqqbEai:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_earitpDccqqbEai:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_earitpDccqqbEai
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_earitpDccqqbEai:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_4_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vextracti32x4	$3,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_hoqrriwBhwFgnqD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_hoqrriwBhwFgnqD
+.L_small_initial_partial_block_hoqrriwBhwFgnqD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_hoqrriwBhwFgnqD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_hoqrriwBhwFgnqD
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_hoqrriwBhwFgnqD:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_5_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%xmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%xmm15,%xmm3,%xmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%xmm7,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%xmm29,%xmm3,%xmm7
+	vextracti32x4	$0,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_slwjAFxmAEsnpad
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_slwjAFxmAEsnpad
+.L_small_initial_partial_block_slwjAFxmAEsnpad:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_slwjAFxmAEsnpad:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_slwjAFxmAEsnpad
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_slwjAFxmAEsnpad:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_6_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%ymm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%ymm15,%ymm3,%ymm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%ymm7,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%ymm29,%ymm3,%ymm7
+	vextracti32x4	$1,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_DdygoqgAkuptfgy
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_DdygoqgAkuptfgy
+.L_small_initial_partial_block_DdygoqgAkuptfgy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_DdygoqgAkuptfgy:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_DdygoqgAkuptfgy
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_DdygoqgAkuptfgy:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_7_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vextracti32x4	$2,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tnCbkomijCDfned
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tnCbkomijCDfned
+.L_small_initial_partial_block_tnCbkomijCDfned:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tnCbkomijCDfned:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tnCbkomijCDfned
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_tnCbkomijCDfned:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_8_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vextracti32x4	$3,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_fwkkfxEznDozhaz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_fwkkfxEznDozhaz
+.L_small_initial_partial_block_fwkkfxEznDozhaz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_fwkkfxEznDozhaz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_fwkkfxEznDozhaz
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_fwkkfxEznDozhaz:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_9_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%xmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%xmm15,%xmm4,%xmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%xmm10,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%xmm29,%xmm4,%xmm10
+	vextracti32x4	$0,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_pAktyhctqoxhsBs
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_pAktyhctqoxhsBs
+.L_small_initial_partial_block_pAktyhctqoxhsBs:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_pAktyhctqoxhsBs:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_pAktyhctqoxhsBs
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_pAktyhctqoxhsBs:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_10_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%ymm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%ymm15,%ymm4,%ymm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%ymm10,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%ymm29,%ymm4,%ymm10
+	vextracti32x4	$1,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_zBlfjfGBariooDj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_zBlfjfGBariooDj
+.L_small_initial_partial_block_zBlfjfGBariooDj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zBlfjfGBariooDj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_zBlfjfGBariooDj
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_zBlfjfGBariooDj:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_11_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vextracti32x4	$2,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_pecxlfigylwCjvq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_pecxlfigylwCjvq
+.L_small_initial_partial_block_pecxlfigylwCjvq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_pecxlfigylwCjvq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_pecxlfigylwCjvq
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_pecxlfigylwCjvq:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_12_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vextracti32x4	$3,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_hplhdDrqaiwqgEa
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	64(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_hplhdDrqaiwqgEa
+.L_small_initial_partial_block_hplhdDrqaiwqgEa:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_hplhdDrqaiwqgEa:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_hplhdDrqaiwqgEa
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_hplhdDrqaiwqgEa:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_13_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%xmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%xmm15,%xmm5,%xmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%xmm11,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vpshufb	%xmm29,%xmm5,%xmm11
+	vextracti32x4	$0,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_nACAlCvkchFwkld
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	48(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_nACAlCvkchFwkld
+.L_small_initial_partial_block_nACAlCvkchFwkld:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	64(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_nACAlCvkchFwkld:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_nACAlCvkchFwkld
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_nACAlCvkchFwkld:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_14_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%ymm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%ymm15,%ymm5,%ymm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%ymm11,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vpshufb	%ymm29,%ymm5,%ymm11
+	vextracti32x4	$1,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ebsjvhxqppdtagG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	32(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ebsjvhxqppdtagG
+.L_small_initial_partial_block_ebsjvhxqppdtagG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	48(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ebsjvhxqppdtagG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ebsjvhxqppdtagG
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_ebsjvhxqppdtagG:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_15_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%zmm15,%zmm5,%zmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%zmm11,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vpshufb	%zmm29,%zmm5,%zmm11
+	vextracti32x4	$2,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_phgxqynCfwBkgnC
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	16(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_phgxqynCfwBkgnC
+.L_small_initial_partial_block_phgxqynCfwBkgnC:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	32(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_phgxqynCfwBkgnC:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_phgxqynCfwBkgnC
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_phgxqynCfwBkgnC:
+	jmp	.L_small_initial_blocks_encrypted_nuilneqyeasoeFh
+.L_small_initial_num_blocks_is_16_nuilneqyeasoeFh:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%zmm15,%zmm5,%zmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%zmm11,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm0,%zmm6
+	vpshufb	%zmm29,%zmm3,%zmm7
+	vpshufb	%zmm29,%zmm4,%zmm10
+	vpshufb	%zmm29,%zmm5,%zmm11
+	vextracti32x4	$3,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_pgpjmqkaAFCuyGj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	16(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_pgpjmqkaAFCuyGj:
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_pgpjmqkaAFCuyGj:
+.L_small_initial_blocks_encrypted_nuilneqyeasoeFh:
+.L_ghash_done_vswthpvgnuoqDhk:
+	vmovdqu64	%xmm2,0(%rsi)
+.L_enc_dec_done_vswthpvgnuoqDhk:
+
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+	vmovdqu64	%xmm14,64(%rsi)
+.L_enc_dec_abort_vswthpvgnuoqDhk:
+	jmp	.Lexit_gcm_encrypt
+.Lexit_gcm_encrypt:
+	cmpq	$256,%r8
+	jbe	.Lskip_hkeys_cleanup_huFEdfqBBGgeuml
+	vpxor	%xmm0,%xmm0,%xmm0
+	vmovdqa64	%zmm0,0(%rsp)
+	vmovdqa64	%zmm0,64(%rsp)
+	vmovdqa64	%zmm0,128(%rsp)
+	vmovdqa64	%zmm0,192(%rsp)
+	vmovdqa64	%zmm0,256(%rsp)
+	vmovdqa64	%zmm0,320(%rsp)
+	vmovdqa64	%zmm0,384(%rsp)
+	vmovdqa64	%zmm0,448(%rsp)
+	vmovdqa64	%zmm0,512(%rsp)
+	vmovdqa64	%zmm0,576(%rsp)
+	vmovdqa64	%zmm0,640(%rsp)
+	vmovdqa64	%zmm0,704(%rsp)
+.Lskip_hkeys_cleanup_huFEdfqBBGgeuml:
+	vzeroupper
+	leaq	(%rbp),%rsp
+.cfi_def_cfa_register	%rsp
+	popq	%r15
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r15
+	popq	%r14
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r14
+	popq	%r13
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r13
+	popq	%r12
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r12
+	popq	%rbp
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%rbp
+	popq	%rbx
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%rbx
+	.byte	0xf3,0xc3
+.Lencrypt_seh_end:
+.cfi_endproc	
+.size	aes_gcm_encrypt_avx512, .-aes_gcm_encrypt_avx512
+.globl	aes_gcm_decrypt_avx512
+.hidden aes_gcm_decrypt_avx512
+.type	aes_gcm_decrypt_avx512,@function
+.align	32
+aes_gcm_decrypt_avx512:
+.cfi_startproc	
+.Ldecrypt_seh_begin:
+.byte	243,15,30,250
+	pushq	%rbx
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%rbx,-16
+.Ldecrypt_seh_push_rbx:
+	pushq	%rbp
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%rbp,-24
+.Ldecrypt_seh_push_rbp:
+	pushq	%r12
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r12,-32
+.Ldecrypt_seh_push_r12:
+	pushq	%r13
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r13,-40
+.Ldecrypt_seh_push_r13:
+	pushq	%r14
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r14,-48
+.Ldecrypt_seh_push_r14:
+	pushq	%r15
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%r15,-56
+.Ldecrypt_seh_push_r15:
+
+
+
+
+
+
+
+
+
+
+	leaq	0(%rsp),%rbp
+.cfi_def_cfa_register	%rbp
+.Ldecrypt_seh_setfp:
+
+.Ldecrypt_seh_prolog_end:
+	subq	$1588,%rsp
+	andq	$(-64),%rsp
+
+
+	movl	240(%rdi),%eax
+	cmpl	$9,%eax
+	je	.Laes_gcm_decrypt_128_avx512
+	cmpl	$11,%eax
+	je	.Laes_gcm_decrypt_192_avx512
+	cmpl	$13,%eax
+	je	.Laes_gcm_decrypt_256_avx512
+	xorl	%eax,%eax
+	jmp	.Lexit_gcm_decrypt
+.align	32
+.Laes_gcm_decrypt_128_avx512:
+	orq	%r8,%r8
+	je	.L_enc_dec_abort_vbsGndgytpgfqCd
+	xorq	%r14,%r14
+	vmovdqu64	64(%rsi),%xmm14
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+
+	movl	(%rdx),%eax
+	orq	%rax,%rax
+	je	.L_partial_block_done_ervsgiCBeqysgzG
+	movl	$16,%r10d
+	leaq	byte_len_to_mask_table(%rip),%r12
+	cmpq	%r10,%r8
+	cmovcq	%r8,%r10
+	kmovw	(%r12,%r10,2),%k1
+	vmovdqu8	(%rcx),%xmm0{%k1}{z}
+
+	vmovdqu64	16(%rsi),%xmm3
+
+	leaq	96(%rsi),%r10
+	vmovdqu64	240(%r10),%xmm4
+
+
+
+	leaq	SHIFT_MASK(%rip),%r12
+	addq	%rax,%r12
+	vmovdqu64	(%r12),%xmm5
+	vpshufb	%xmm5,%xmm3,%xmm3
+
+	vmovdqa64	%xmm0,%xmm6
+	vpxorq	%xmm0,%xmm3,%xmm3
+
+
+	leaq	(%r8,%rax,1),%r13
+	subq	$16,%r13
+	jge	.L_no_extra_mask_ervsgiCBeqysgzG
+	subq	%r13,%r12
+.L_no_extra_mask_ervsgiCBeqysgzG:
+
+
+
+	vmovdqu64	16(%r12),%xmm0
+	vpand	%xmm0,%xmm3,%xmm3
+	vpand	%xmm0,%xmm6,%xmm6
+	vpshufb	SHUF_MASK(%rip),%xmm6,%xmm6
+	vpshufb	%xmm5,%xmm6,%xmm6
+	vpxorq	%xmm6,%xmm14,%xmm14
+	cmpq	$0,%r13
+	jl	.L_partial_incomplete_ervsgiCBeqysgzG
+
+	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
+	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
+	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
+	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
+	vpxorq	%xmm11,%xmm14,%xmm14
+
+	vpsrldq	$8,%xmm14,%xmm11
+	vpslldq	$8,%xmm14,%xmm14
+	vpxorq	%xmm11,%xmm7,%xmm7
+	vpxorq	%xmm10,%xmm14,%xmm14
+
+
+
+	vmovdqu64	POLY2(%rip),%xmm11
+
+	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
+	vpslldq	$8,%xmm10,%xmm10
+	vpxorq	%xmm10,%xmm14,%xmm14
+
+
+
+	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
+	vpsrldq	$4,%xmm10,%xmm10
+	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+
+	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14
+
+	movl	$0,(%rdx)
+
+	movq	%rax,%r12
+	movq	$16,%rax
+	subq	%r12,%rax
+	jmp	.L_enc_dec_done_ervsgiCBeqysgzG
+
+.L_partial_incomplete_ervsgiCBeqysgzG:
+	addl	%r8d,(%rdx)
+	movq	%r8,%rax
+
+.L_enc_dec_done_ervsgiCBeqysgzG:
+
+
+	leaq	byte_len_to_mask_table(%rip),%r12
+	kmovw	(%r12,%rax,2),%k1
+	movq	%r9,%r12
+	vmovdqu8	%xmm3,(%r12){%k1}
+.L_partial_block_done_ervsgiCBeqysgzG:
+	vmovdqu64	0(%rsi),%xmm2
+	subq	%rax,%r8
+	je	.L_enc_dec_done_vbsGndgytpgfqCd
+	cmpq	$256,%r8
+	jbe	.L_message_below_equal_16_blocks_vbsGndgytpgfqCd
+
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
+	vmovdqa64	ddq_addbe_1234(%rip),%zmm28
+
+
+
+
+
+
+	vmovd	%xmm2,%r15d
+	andl	$255,%r15d
+
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpshufb	%zmm29,%zmm2,%zmm2
+
+
+
+	cmpb	$240,%r15b
+	jae	.L_next_16_overflow_GDxCxbvehieesqG
+	vpaddd	%zmm28,%zmm2,%zmm7
+	vpaddd	%zmm27,%zmm7,%zmm10
+	vpaddd	%zmm27,%zmm10,%zmm11
+	vpaddd	%zmm27,%zmm11,%zmm12
+	jmp	.L_next_16_ok_GDxCxbvehieesqG
+.L_next_16_overflow_GDxCxbvehieesqG:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm12
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
+	vpaddd	%zmm12,%zmm7,%zmm10
+	vpaddd	%zmm12,%zmm10,%zmm11
+	vpaddd	%zmm12,%zmm11,%zmm12
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+.L_next_16_ok_GDxCxbvehieesqG:
+	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
+	addb	$16,%r15b
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm0
+	vmovdqu8	64(%rcx,%rax,1),%zmm3
+	vmovdqu8	128(%rcx,%rax,1),%zmm4
+	vmovdqu8	192(%rcx,%rax,1),%zmm5
+
+
+	vbroadcastf64x2	0(%rdi),%zmm6
+	vpxorq	%zmm6,%zmm7,%zmm7
+	vpxorq	%zmm6,%zmm10,%zmm10
+	vpxorq	%zmm6,%zmm11,%zmm11
+	vpxorq	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	16(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	32(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	48(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	64(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	80(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	96(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	112(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	128(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	144(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	160(%rdi),%zmm6
+	vaesenclast	%zmm6,%zmm7,%zmm7
+	vaesenclast	%zmm6,%zmm10,%zmm10
+	vaesenclast	%zmm6,%zmm11,%zmm11
+	vaesenclast	%zmm6,%zmm12,%zmm12
+
+
+	vpxorq	%zmm0,%zmm7,%zmm7
+	vpxorq	%zmm3,%zmm10,%zmm10
+	vpxorq	%zmm4,%zmm11,%zmm11
+	vpxorq	%zmm5,%zmm12,%zmm12
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm7,0(%r10,%rax,1)
+	vmovdqu8	%zmm10,64(%r10,%rax,1)
+	vmovdqu8	%zmm11,128(%r10,%rax,1)
+	vmovdqu8	%zmm12,192(%r10,%rax,1)
+
+	vpshufb	%zmm29,%zmm0,%zmm7
+	vpshufb	%zmm29,%zmm3,%zmm10
+	vpshufb	%zmm29,%zmm4,%zmm11
+	vpshufb	%zmm29,%zmm5,%zmm12
+	vmovdqa64	%zmm7,768(%rsp)
+	vmovdqa64	%zmm10,832(%rsp)
+	vmovdqa64	%zmm11,896(%rsp)
+	vmovdqa64	%zmm12,960(%rsp)
+	leaq	96(%rsi),%r12
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_zjxrsreEazopzxn
+
+	vmovdqu64	192(%r12),%zmm0
+	vmovdqu64	%zmm0,704(%rsp)
+
+	vmovdqu64	128(%r12),%zmm3
+	vmovdqu64	%zmm3,640(%rsp)
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	64(%r12),%zmm4
+	vmovdqu64	%zmm4,576(%rsp)
+
+	vmovdqu64	0(%r12),%zmm5
+	vmovdqu64	%zmm5,512(%rsp)
+.L_skip_hkeys_precomputation_zjxrsreEazopzxn:
+	cmpq	$512,%r8
+	jb	.L_message_below_32_blocks_vbsGndgytpgfqCd
+
+
+
+	cmpb	$240,%r15b
+	jae	.L_next_16_overflow_sciictgemzjksoB
+	vpaddd	%zmm28,%zmm2,%zmm7
+	vpaddd	%zmm27,%zmm7,%zmm10
+	vpaddd	%zmm27,%zmm10,%zmm11
+	vpaddd	%zmm27,%zmm11,%zmm12
+	jmp	.L_next_16_ok_sciictgemzjksoB
+.L_next_16_overflow_sciictgemzjksoB:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm12
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
+	vpaddd	%zmm12,%zmm7,%zmm10
+	vpaddd	%zmm12,%zmm10,%zmm11
+	vpaddd	%zmm12,%zmm11,%zmm12
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+.L_next_16_ok_sciictgemzjksoB:
+	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
+	addb	$16,%r15b
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm0
+	vmovdqu8	320(%rcx,%rax,1),%zmm3
+	vmovdqu8	384(%rcx,%rax,1),%zmm4
+	vmovdqu8	448(%rcx,%rax,1),%zmm5
+
+
+	vbroadcastf64x2	0(%rdi),%zmm6
+	vpxorq	%zmm6,%zmm7,%zmm7
+	vpxorq	%zmm6,%zmm10,%zmm10
+	vpxorq	%zmm6,%zmm11,%zmm11
+	vpxorq	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	16(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	32(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	48(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	64(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	80(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	96(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	112(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	128(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	144(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	160(%rdi),%zmm6
+	vaesenclast	%zmm6,%zmm7,%zmm7
+	vaesenclast	%zmm6,%zmm10,%zmm10
+	vaesenclast	%zmm6,%zmm11,%zmm11
+	vaesenclast	%zmm6,%zmm12,%zmm12
+
+
+	vpxorq	%zmm0,%zmm7,%zmm7
+	vpxorq	%zmm3,%zmm10,%zmm10
+	vpxorq	%zmm4,%zmm11,%zmm11
+	vpxorq	%zmm5,%zmm12,%zmm12
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm7,256(%r10,%rax,1)
+	vmovdqu8	%zmm10,320(%r10,%rax,1)
+	vmovdqu8	%zmm11,384(%r10,%rax,1)
+	vmovdqu8	%zmm12,448(%r10,%rax,1)
+
+	vpshufb	%zmm29,%zmm0,%zmm7
+	vpshufb	%zmm29,%zmm3,%zmm10
+	vpshufb	%zmm29,%zmm4,%zmm11
+	vpshufb	%zmm29,%zmm5,%zmm12
+	vmovdqa64	%zmm7,1024(%rsp)
+	vmovdqa64	%zmm10,1088(%rsp)
+	vmovdqa64	%zmm11,1152(%rsp)
+	vmovdqa64	%zmm12,1216(%rsp)
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_gDqynhyDbklAwqC
+	vmovdqu64	640(%rsp),%zmm3
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	576(%rsp),%zmm4
+	vmovdqu64	512(%rsp),%zmm5
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,256(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,192(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,128(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,64(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,0(%rsp)
+.L_skip_hkeys_precomputation_gDqynhyDbklAwqC:
+	movq	$1,%r14
+	addq	$512,%rax
+	subq	$512,%r8
+
+	cmpq	$768,%r8
+	jb	.L_no_more_big_nblocks_vbsGndgytpgfqCd
+.L_encrypt_big_nblocks_vbsGndgytpgfqCd:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_hgjiBjFmlngGulF
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_hgjiBjFmlngGulF
+.L_16_blocks_overflow_hgjiBjFmlngGulF:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_hgjiBjFmlngGulF:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_ADbflFodpwAjaCi
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_ADbflFodpwAjaCi
+.L_16_blocks_overflow_ADbflFodpwAjaCi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_ADbflFodpwAjaCi:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	256(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	320(%rsp),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	384(%rsp),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	448(%rsp),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm17
+	vmovdqu8	320(%rcx,%rax,1),%zmm19
+	vmovdqu8	384(%rcx,%rax,1),%zmm20
+	vmovdqu8	448(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,256(%r10,%rax,1)
+	vmovdqu8	%zmm3,320(%r10,%rax,1)
+	vmovdqu8	%zmm4,384(%r10,%rax,1)
+	vmovdqu8	%zmm5,448(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,768(%rsp)
+	vmovdqa64	%zmm3,832(%rsp)
+	vmovdqa64	%zmm4,896(%rsp)
+	vmovdqa64	%zmm5,960(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_twtzdBAFyjkhafi
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_twtzdBAFyjkhafi
+.L_16_blocks_overflow_twtzdBAFyjkhafi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_twtzdBAFyjkhafi:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	512(%rcx,%rax,1),%zmm17
+	vmovdqu8	576(%rcx,%rax,1),%zmm19
+	vmovdqu8	640(%rcx,%rax,1),%zmm20
+	vmovdqu8	704(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+
+
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
+	vpxorq	%zmm24,%zmm6,%zmm6
+	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
+	vpxorq	%zmm25,%zmm7,%zmm7
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vextracti64x4	$1,%zmm6,%ymm12
+	vpxorq	%ymm12,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm12
+	vpxorq	%xmm12,%xmm6,%xmm6
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,512(%r10,%rax,1)
+	vmovdqu8	%zmm3,576(%r10,%rax,1)
+	vmovdqu8	%zmm4,640(%r10,%rax,1)
+	vmovdqu8	%zmm5,704(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,1024(%rsp)
+	vmovdqa64	%zmm3,1088(%rsp)
+	vmovdqa64	%zmm4,1152(%rsp)
+	vmovdqa64	%zmm5,1216(%rsp)
+	vmovdqa64	%zmm6,%zmm14
+
+	addq	$768,%rax
+	subq	$768,%r8
+	cmpq	$768,%r8
+	jae	.L_encrypt_big_nblocks_vbsGndgytpgfqCd
+
+.L_no_more_big_nblocks_vbsGndgytpgfqCd:
+
+	cmpq	$512,%r8
+	jae	.L_encrypt_32_blocks_vbsGndgytpgfqCd
+
+	cmpq	$256,%r8
+	jae	.L_encrypt_16_blocks_vbsGndgytpgfqCd
+.L_encrypt_0_blocks_ghash_32_vbsGndgytpgfqCd:
+	movl	%r8d,%r10d
+	andl	$~15,%r10d
+	movl	$256,%ebx
+	subl	%r10d,%ebx
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	addl	$256,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_dmCDklApknoFfip
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_dmCDklApknoFfip
+	jb	.L_last_num_blocks_is_7_1_dmCDklApknoFfip
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_dmCDklApknoFfip
+	jb	.L_last_num_blocks_is_11_9_dmCDklApknoFfip
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_dmCDklApknoFfip
+	ja	.L_last_num_blocks_is_16_dmCDklApknoFfip
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_dmCDklApknoFfip
+	jmp	.L_last_num_blocks_is_13_dmCDklApknoFfip
+
+.L_last_num_blocks_is_11_9_dmCDklApknoFfip:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_dmCDklApknoFfip
+	ja	.L_last_num_blocks_is_11_dmCDklApknoFfip
+	jmp	.L_last_num_blocks_is_9_dmCDklApknoFfip
+
+.L_last_num_blocks_is_7_1_dmCDklApknoFfip:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_dmCDklApknoFfip
+	jb	.L_last_num_blocks_is_3_1_dmCDklApknoFfip
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_dmCDklApknoFfip
+	je	.L_last_num_blocks_is_6_dmCDklApknoFfip
+	jmp	.L_last_num_blocks_is_5_dmCDklApknoFfip
+
+.L_last_num_blocks_is_3_1_dmCDklApknoFfip:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_dmCDklApknoFfip
+	je	.L_last_num_blocks_is_2_dmCDklApknoFfip
+.L_last_num_blocks_is_1_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_giGAGGlnlocmuxc
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_giGAGGlnlocmuxc
+
+.L_16_blocks_overflow_giGAGGlnlocmuxc:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_giGAGGlnlocmuxc:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%xmm29,%xmm17,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_FztcsxjweCmtzDf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_FztcsxjweCmtzDf
+.L_small_initial_partial_block_FztcsxjweCmtzDf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_FztcsxjweCmtzDf
+.L_small_initial_compute_done_FztcsxjweCmtzDf:
+.L_after_reduction_FztcsxjweCmtzDf:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_2_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_mEoxvBbnadCiEtv
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_mEoxvBbnadCiEtv
+
+.L_16_blocks_overflow_mEoxvBbnadCiEtv:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_mEoxvBbnadCiEtv:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%ymm29,%ymm17,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_iECcywpGexyouvn
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_iECcywpGexyouvn
+.L_small_initial_partial_block_iECcywpGexyouvn:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_iECcywpGexyouvn:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_iECcywpGexyouvn
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_iECcywpGexyouvn:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_3_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_nomvpfuDGjrytud
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_nomvpfuDGjrytud
+
+.L_16_blocks_overflow_nomvpfuDGjrytud:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_nomvpfuDGjrytud:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_yxvxDobpywhuAua
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_yxvxDobpywhuAua
+.L_small_initial_partial_block_yxvxDobpywhuAua:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_yxvxDobpywhuAua:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_yxvxDobpywhuAua
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_yxvxDobpywhuAua:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_4_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_pcjupEmfwpxcfqu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_pcjupEmfwpxcfqu
+
+.L_16_blocks_overflow_pcjupEmfwpxcfqu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_pcjupEmfwpxcfqu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_gCEadkildbrCyor
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_gCEadkildbrCyor
+.L_small_initial_partial_block_gCEadkildbrCyor:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_gCEadkildbrCyor:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_gCEadkildbrCyor
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_gCEadkildbrCyor:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_5_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_aiodxzzrzgaknuv
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_aiodxzzrzgaknuv
+
+.L_16_blocks_overflow_aiodxzzrzgaknuv:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_aiodxzzrzgaknuv:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%xmm29,%xmm19,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AqEcmpczzACrFGB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AqEcmpczzACrFGB
+.L_small_initial_partial_block_AqEcmpczzACrFGB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AqEcmpczzACrFGB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AqEcmpczzACrFGB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_AqEcmpczzACrFGB:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_6_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_vnmjtxBEBhAegvC
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_vnmjtxBEBhAegvC
+
+.L_16_blocks_overflow_vnmjtxBEBhAegvC:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_vnmjtxBEBhAegvC:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%ymm29,%ymm19,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_epwkGkxluronyen
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_epwkGkxluronyen
+.L_small_initial_partial_block_epwkGkxluronyen:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_epwkGkxluronyen:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_epwkGkxluronyen
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_epwkGkxluronyen:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_7_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_zfFifxjCfpEvFpf
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_zfFifxjCfpEvFpf
+
+.L_16_blocks_overflow_zfFifxjCfpEvFpf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_zfFifxjCfpEvFpf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BrurehlropGezFw
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BrurehlropGezFw
+.L_small_initial_partial_block_BrurehlropGezFw:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BrurehlropGezFw:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BrurehlropGezFw
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_BrurehlropGezFw:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_8_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_GyeBgsalubjBbdB
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_GyeBgsalubjBbdB
+
+.L_16_blocks_overflow_GyeBgsalubjBbdB:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_GyeBgsalubjBbdB:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dpCdchlpavEustg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dpCdchlpavEustg
+.L_small_initial_partial_block_dpCdchlpavEustg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_dpCdchlpavEustg:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_dpCdchlpavEustg
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_dpCdchlpavEustg:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_9_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_ycqoattEtxDknFC
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_ycqoattEtxDknFC
+
+.L_16_blocks_overflow_ycqoattEtxDknFC:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_ycqoattEtxDknFC:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%xmm29,%xmm20,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AApqwaocpFcEziB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AApqwaocpFcEziB
+.L_small_initial_partial_block_AApqwaocpFcEziB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AApqwaocpFcEziB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AApqwaocpFcEziB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_AApqwaocpFcEziB:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_10_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_hjjicyfzGkwjefd
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_hjjicyfzGkwjefd
+
+.L_16_blocks_overflow_hjjicyfzGkwjefd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_hjjicyfzGkwjefd:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%ymm29,%ymm20,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_asblydgkaAsemEq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_asblydgkaAsemEq
+.L_small_initial_partial_block_asblydgkaAsemEq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_asblydgkaAsemEq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_asblydgkaAsemEq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_asblydgkaAsemEq:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_11_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_segfDhiczjsBsgv
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_segfDhiczjsBsgv
+
+.L_16_blocks_overflow_segfDhiczjsBsgv:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_segfDhiczjsBsgv:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_cdCiatdaDtvxGwa
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_cdCiatdaDtvxGwa
+.L_small_initial_partial_block_cdCiatdaDtvxGwa:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_cdCiatdaDtvxGwa:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_cdCiatdaDtvxGwa
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_cdCiatdaDtvxGwa:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_12_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_pwAaqnuaeFcjtCq
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_pwAaqnuaeFcjtCq
+
+.L_16_blocks_overflow_pwAaqnuaeFcjtCq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_pwAaqnuaeFcjtCq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_jtrFatfhgAubvAb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_jtrFatfhgAubvAb
+.L_small_initial_partial_block_jtrFatfhgAubvAb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_jtrFatfhgAubvAb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_jtrFatfhgAubvAb
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_jtrFatfhgAubvAb:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_13_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_lokkCvxpfBloGfk
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_lokkCvxpfBloGfk
+
+.L_16_blocks_overflow_lokkCvxpfBloGfk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_lokkCvxpfBloGfk:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%xmm29,%xmm21,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dsuoyEdBnCotuDf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dsuoyEdBnCotuDf
+.L_small_initial_partial_block_dsuoyEdBnCotuDf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_dsuoyEdBnCotuDf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_dsuoyEdBnCotuDf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_dsuoyEdBnCotuDf:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_14_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_FFqpkDkxeqDkhfp
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_FFqpkDkxeqDkhfp
+
+.L_16_blocks_overflow_FFqpkDkxeqDkhfp:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_FFqpkDkxeqDkhfp:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%ymm29,%ymm21,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AmFuAxcazdGwyly
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AmFuAxcazdGwyly
+.L_small_initial_partial_block_AmFuAxcazdGwyly:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AmFuAxcazdGwyly:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AmFuAxcazdGwyly
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_AmFuAxcazdGwyly:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_15_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_zmccbzClpmvoakx
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_zmccbzClpmvoakx
+
+.L_16_blocks_overflow_zmccbzClpmvoakx:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_zmccbzClpmvoakx:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_yDqGowisFwnfCpe
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_yDqGowisFwnfCpe
+.L_small_initial_partial_block_yDqGowisFwnfCpe:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_yDqGowisFwnfCpe:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_yDqGowisFwnfCpe
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_yDqGowisFwnfCpe:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_16_dmCDklApknoFfip:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_AGpohuEiuDgCeaE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_AGpohuEiuDgCeaE
+
+.L_16_blocks_overflow_AGpohuEiuDgCeaE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_AGpohuEiuDgCeaE:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_ydzoexFllhtbbhg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ydzoexFllhtbbhg:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ydzoexFllhtbbhg:
+	jmp	.L_last_blocks_done_dmCDklApknoFfip
+.L_last_num_blocks_is_0_dmCDklApknoFfip:
+	vmovdqa64	1024(%rsp),%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1088(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1152(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1216(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_dmCDklApknoFfip:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_vbsGndgytpgfqCd
+.L_encrypt_32_blocks_vbsGndgytpgfqCd:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_GsamywonbgmElzl
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_GsamywonbgmElzl
+.L_16_blocks_overflow_GsamywonbgmElzl:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_GsamywonbgmElzl:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_sogFkizhetEaGAc
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_sogFkizhetEaGAc
+.L_16_blocks_overflow_sogFkizhetEaGAc:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_sogFkizhetEaGAc:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	256(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	320(%rsp),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	384(%rsp),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	448(%rsp),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm17
+	vmovdqu8	320(%rcx,%rax,1),%zmm19
+	vmovdqu8	384(%rcx,%rax,1),%zmm20
+	vmovdqu8	448(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,256(%r10,%rax,1)
+	vmovdqu8	%zmm3,320(%r10,%rax,1)
+	vmovdqu8	%zmm4,384(%r10,%rax,1)
+	vmovdqu8	%zmm5,448(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,768(%rsp)
+	vmovdqa64	%zmm3,832(%rsp)
+	vmovdqa64	%zmm4,896(%rsp)
+	vmovdqa64	%zmm5,960(%rsp)
+	vmovdqa64	1280(%rsp),%zmm13
+	vmovdqu64	512(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1344(%rsp),%zmm13
+	vmovdqu64	576(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1408(%rsp),%zmm13
+	vmovdqu64	640(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1472(%rsp),%zmm13
+	vmovdqu64	704(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+	subq	$512,%r8
+	addq	$512,%rax
+	movl	%r8d,%r10d
+	andl	$~15,%r10d
+	movl	$512,%ebx
+	subl	%r10d,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_rcyyvEibaiwEmll
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_rcyyvEibaiwEmll
+	jb	.L_last_num_blocks_is_7_1_rcyyvEibaiwEmll
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_rcyyvEibaiwEmll
+	jb	.L_last_num_blocks_is_11_9_rcyyvEibaiwEmll
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_rcyyvEibaiwEmll
+	ja	.L_last_num_blocks_is_16_rcyyvEibaiwEmll
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_rcyyvEibaiwEmll
+	jmp	.L_last_num_blocks_is_13_rcyyvEibaiwEmll
+
+.L_last_num_blocks_is_11_9_rcyyvEibaiwEmll:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_rcyyvEibaiwEmll
+	ja	.L_last_num_blocks_is_11_rcyyvEibaiwEmll
+	jmp	.L_last_num_blocks_is_9_rcyyvEibaiwEmll
+
+.L_last_num_blocks_is_7_1_rcyyvEibaiwEmll:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_rcyyvEibaiwEmll
+	jb	.L_last_num_blocks_is_3_1_rcyyvEibaiwEmll
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_rcyyvEibaiwEmll
+	je	.L_last_num_blocks_is_6_rcyyvEibaiwEmll
+	jmp	.L_last_num_blocks_is_5_rcyyvEibaiwEmll
+
+.L_last_num_blocks_is_3_1_rcyyvEibaiwEmll:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_rcyyvEibaiwEmll
+	je	.L_last_num_blocks_is_2_rcyyvEibaiwEmll
+.L_last_num_blocks_is_1_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_otAspqCyymDxgtf
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_otAspqCyymDxgtf
+
+.L_16_blocks_overflow_otAspqCyymDxgtf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_otAspqCyymDxgtf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%xmm29,%xmm17,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_llyfgaCDEvfomlv
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_llyfgaCDEvfomlv
+.L_small_initial_partial_block_llyfgaCDEvfomlv:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_llyfgaCDEvfomlv
+.L_small_initial_compute_done_llyfgaCDEvfomlv:
+.L_after_reduction_llyfgaCDEvfomlv:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_2_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_nizyzxChzoGlnud
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_nizyzxChzoGlnud
+
+.L_16_blocks_overflow_nizyzxChzoGlnud:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_nizyzxChzoGlnud:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%ymm29,%ymm17,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ajGDfaevBodngsr
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ajGDfaevBodngsr
+.L_small_initial_partial_block_ajGDfaevBodngsr:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ajGDfaevBodngsr:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ajGDfaevBodngsr
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ajGDfaevBodngsr:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_3_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_iazwcdlEkonjFgc
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_iazwcdlEkonjFgc
+
+.L_16_blocks_overflow_iazwcdlEkonjFgc:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_iazwcdlEkonjFgc:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_hngcbbEdsAzliaf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_hngcbbEdsAzliaf
+.L_small_initial_partial_block_hngcbbEdsAzliaf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_hngcbbEdsAzliaf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_hngcbbEdsAzliaf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_hngcbbEdsAzliaf:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_4_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_zDgBcBtsbkpimiF
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_zDgBcBtsbkpimiF
+
+.L_16_blocks_overflow_zDgBcBtsbkpimiF:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_zDgBcBtsbkpimiF:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_nEfDyoglymEBceu
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_nEfDyoglymEBceu
+.L_small_initial_partial_block_nEfDyoglymEBceu:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_nEfDyoglymEBceu:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_nEfDyoglymEBceu
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_nEfDyoglymEBceu:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_5_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_ByhpdryAzupfrfB
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_ByhpdryAzupfrfB
+
+.L_16_blocks_overflow_ByhpdryAzupfrfB:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_ByhpdryAzupfrfB:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%xmm29,%xmm19,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_usFszlxBxwyBxwl
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_usFszlxBxwyBxwl
+.L_small_initial_partial_block_usFszlxBxwyBxwl:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_usFszlxBxwyBxwl:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_usFszlxBxwyBxwl
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_usFszlxBxwyBxwl:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_6_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_icgbEFDdheqyacb
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_icgbEFDdheqyacb
+
+.L_16_blocks_overflow_icgbEFDdheqyacb:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_icgbEFDdheqyacb:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%ymm29,%ymm19,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_xwlcavhwjmCfGni
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_xwlcavhwjmCfGni
+.L_small_initial_partial_block_xwlcavhwjmCfGni:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_xwlcavhwjmCfGni:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_xwlcavhwjmCfGni
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_xwlcavhwjmCfGni:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_7_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_mCAjvmAjlDCbDqf
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_mCAjvmAjlDCbDqf
+
+.L_16_blocks_overflow_mCAjvmAjlDCbDqf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_mCAjvmAjlDCbDqf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ckEAbtueFbwngrG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ckEAbtueFbwngrG
+.L_small_initial_partial_block_ckEAbtueFbwngrG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ckEAbtueFbwngrG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ckEAbtueFbwngrG
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ckEAbtueFbwngrG:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_8_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_BjnehDtzhrcysgc
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_BjnehDtzhrcysgc
+
+.L_16_blocks_overflow_BjnehDtzhrcysgc:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_BjnehDtzhrcysgc:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ccCaznujCzkyEAh
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ccCaznujCzkyEAh
+.L_small_initial_partial_block_ccCaznujCzkyEAh:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ccCaznujCzkyEAh:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ccCaznujCzkyEAh
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ccCaznujCzkyEAh:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_9_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_ExamtkpCBCAbdoq
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_ExamtkpCBCAbdoq
+
+.L_16_blocks_overflow_ExamtkpCBCAbdoq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_ExamtkpCBCAbdoq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%xmm29,%xmm20,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ysuAkhigvnxqqgr
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ysuAkhigvnxqqgr
+.L_small_initial_partial_block_ysuAkhigvnxqqgr:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ysuAkhigvnxqqgr:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ysuAkhigvnxqqgr
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ysuAkhigvnxqqgr:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_10_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_CvqFkbuihinihqd
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_CvqFkbuihinihqd
+
+.L_16_blocks_overflow_CvqFkbuihinihqd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_CvqFkbuihinihqd:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%ymm29,%ymm20,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_gyBsFvGzFwehEdu
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_gyBsFvGzFwehEdu
+.L_small_initial_partial_block_gyBsFvGzFwehEdu:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_gyBsFvGzFwehEdu:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_gyBsFvGzFwehEdu
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_gyBsFvGzFwehEdu:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_11_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_etCffFegeitBwja
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_etCffFegeitBwja
+
+.L_16_blocks_overflow_etCffFegeitBwja:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_etCffFegeitBwja:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_cnznBbanxasmyGb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_cnznBbanxasmyGb
+.L_small_initial_partial_block_cnznBbanxasmyGb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_cnznBbanxasmyGb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_cnznBbanxasmyGb
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_cnznBbanxasmyGb:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_12_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_wdgpgChkyAogBem
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_wdgpgChkyAogBem
+
+.L_16_blocks_overflow_wdgpgChkyAogBem:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_wdgpgChkyAogBem:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_nEjAqjAsxtlykjf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_nEjAqjAsxtlykjf
+.L_small_initial_partial_block_nEjAqjAsxtlykjf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_nEjAqjAsxtlykjf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_nEjAqjAsxtlykjf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_nEjAqjAsxtlykjf:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_13_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_vArjcxdaoogcwxq
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_vArjcxdaoogcwxq
+
+.L_16_blocks_overflow_vArjcxdaoogcwxq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_vArjcxdaoogcwxq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%xmm29,%xmm21,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_oDwgiBBxkytoAyr
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_oDwgiBBxkytoAyr
+.L_small_initial_partial_block_oDwgiBBxkytoAyr:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_oDwgiBBxkytoAyr:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_oDwgiBBxkytoAyr
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_oDwgiBBxkytoAyr:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_14_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_BrtsvilbizobqDE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_BrtsvilbizobqDE
+
+.L_16_blocks_overflow_BrtsvilbizobqDE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_BrtsvilbizobqDE:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%ymm29,%ymm21,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_xAjDjxrhaAkCDcw
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_xAjDjxrhaAkCDcw
+.L_small_initial_partial_block_xAjDjxrhaAkCDcw:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_xAjDjxrhaAkCDcw:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_xAjDjxrhaAkCDcw
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_xAjDjxrhaAkCDcw:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_15_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_mvqlfbCbieuBFkG
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_mvqlfbCbieuBFkG
+
+.L_16_blocks_overflow_mvqlfbCbieuBFkG:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_mvqlfbCbieuBFkG:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_yotewxBBjsegvGg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_yotewxBBjsegvGg
+.L_small_initial_partial_block_yotewxBBjsegvGg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_yotewxBBjsegvGg:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_yotewxBBjsegvGg
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_yotewxBBjsegvGg:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_16_rcyyvEibaiwEmll:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_qyEpAGmokwuxgnn
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_qyEpAGmokwuxgnn
+
+.L_16_blocks_overflow_qyEpAGmokwuxgnn:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_qyEpAGmokwuxgnn:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_FBCahGGoyifamdq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_FBCahGGoyifamdq:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_FBCahGGoyifamdq:
+	jmp	.L_last_blocks_done_rcyyvEibaiwEmll
+.L_last_num_blocks_is_0_rcyyvEibaiwEmll:
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_rcyyvEibaiwEmll:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_vbsGndgytpgfqCd
+.L_encrypt_16_blocks_vbsGndgytpgfqCd:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_yCrByeCkdhCdtlo
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_yCrByeCkdhCdtlo
+.L_16_blocks_overflow_yCrByeCkdhCdtlo:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_yCrByeCkdhCdtlo:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	vmovdqa64	1024(%rsp),%zmm13
+	vmovdqu64	256(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1088(%rsp),%zmm13
+	vmovdqu64	320(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1152(%rsp),%zmm13
+	vmovdqu64	384(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1216(%rsp),%zmm13
+	vmovdqu64	448(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	subq	$256,%r8
+	addq	$256,%rax
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_Cedzykoatrqkpvp
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_Cedzykoatrqkpvp
+	jb	.L_last_num_blocks_is_7_1_Cedzykoatrqkpvp
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_Cedzykoatrqkpvp
+	jb	.L_last_num_blocks_is_11_9_Cedzykoatrqkpvp
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_Cedzykoatrqkpvp
+	ja	.L_last_num_blocks_is_16_Cedzykoatrqkpvp
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_Cedzykoatrqkpvp
+	jmp	.L_last_num_blocks_is_13_Cedzykoatrqkpvp
+
+.L_last_num_blocks_is_11_9_Cedzykoatrqkpvp:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_Cedzykoatrqkpvp
+	ja	.L_last_num_blocks_is_11_Cedzykoatrqkpvp
+	jmp	.L_last_num_blocks_is_9_Cedzykoatrqkpvp
+
+.L_last_num_blocks_is_7_1_Cedzykoatrqkpvp:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_Cedzykoatrqkpvp
+	jb	.L_last_num_blocks_is_3_1_Cedzykoatrqkpvp
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_Cedzykoatrqkpvp
+	je	.L_last_num_blocks_is_6_Cedzykoatrqkpvp
+	jmp	.L_last_num_blocks_is_5_Cedzykoatrqkpvp
+
+.L_last_num_blocks_is_3_1_Cedzykoatrqkpvp:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_Cedzykoatrqkpvp
+	je	.L_last_num_blocks_is_2_Cedzykoatrqkpvp
+.L_last_num_blocks_is_1_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_zvoAvmjgmdwzuvn
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_zvoAvmjgmdwzuvn
+
+.L_16_blocks_overflow_zvoAvmjgmdwzuvn:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_zvoAvmjgmdwzuvn:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%xmm29,%xmm17,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vFBGccisjlbhFxA
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vFBGccisjlbhFxA
+.L_small_initial_partial_block_vFBGccisjlbhFxA:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_vFBGccisjlbhFxA
+.L_small_initial_compute_done_vFBGccisjlbhFxA:
+.L_after_reduction_vFBGccisjlbhFxA:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_2_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_suelhBEvchjqByq
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_suelhBEvchjqByq
+
+.L_16_blocks_overflow_suelhBEvchjqByq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_suelhBEvchjqByq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%ymm29,%ymm17,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_jfbkdxBsyaAlbsu
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_jfbkdxBsyaAlbsu
+.L_small_initial_partial_block_jfbkdxBsyaAlbsu:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_jfbkdxBsyaAlbsu:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_jfbkdxBsyaAlbsu
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_jfbkdxBsyaAlbsu:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_3_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_issshhwGvpkyzhg
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_issshhwGvpkyzhg
+
+.L_16_blocks_overflow_issshhwGvpkyzhg:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_issshhwGvpkyzhg:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GGArFcsuraDzokG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GGArFcsuraDzokG
+.L_small_initial_partial_block_GGArFcsuraDzokG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GGArFcsuraDzokG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GGArFcsuraDzokG
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GGArFcsuraDzokG:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_4_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_GtiiAxsCAChzxvw
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_GtiiAxsCAChzxvw
+
+.L_16_blocks_overflow_GtiiAxsCAChzxvw:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_GtiiAxsCAChzxvw:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_baitmoAunErvrtm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_baitmoAunErvrtm
+.L_small_initial_partial_block_baitmoAunErvrtm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_baitmoAunErvrtm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_baitmoAunErvrtm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_baitmoAunErvrtm:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_5_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_wmCbcbEehdDDvos
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_wmCbcbEehdDDvos
+
+.L_16_blocks_overflow_wmCbcbEehdDDvos:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_wmCbcbEehdDDvos:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%xmm29,%xmm19,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_FAxrzGgwGxcuwyg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_FAxrzGgwGxcuwyg
+.L_small_initial_partial_block_FAxrzGgwGxcuwyg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_FAxrzGgwGxcuwyg:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_FAxrzGgwGxcuwyg
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_FAxrzGgwGxcuwyg:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_6_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_zdmtgDAgbccnkih
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_zdmtgDAgbccnkih
+
+.L_16_blocks_overflow_zdmtgDAgbccnkih:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_zdmtgDAgbccnkih:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%ymm29,%ymm19,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_amCjipgpuxensDG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_amCjipgpuxensDG
+.L_small_initial_partial_block_amCjipgpuxensDG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_amCjipgpuxensDG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_amCjipgpuxensDG
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_amCjipgpuxensDG:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_7_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_szhEovukApomrwF
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_szhEovukApomrwF
+
+.L_16_blocks_overflow_szhEovukApomrwF:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_szhEovukApomrwF:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_bcayozDfphlGGas
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_bcayozDfphlGGas
+.L_small_initial_partial_block_bcayozDfphlGGas:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_bcayozDfphlGGas:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_bcayozDfphlGGas
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_bcayozDfphlGGas:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_8_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_DoozqGEqniEpcww
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_DoozqGEqniEpcww
+
+.L_16_blocks_overflow_DoozqGEqniEpcww:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_DoozqGEqniEpcww:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_kijsxpqCcaqaBBy
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_kijsxpqCcaqaBBy
+.L_small_initial_partial_block_kijsxpqCcaqaBBy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_kijsxpqCcaqaBBy:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_kijsxpqCcaqaBBy
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_kijsxpqCcaqaBBy:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_9_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_dAosatdaGEvBnfn
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_dAosatdaGEvBnfn
+
+.L_16_blocks_overflow_dAosatdaGEvBnfn:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_dAosatdaGEvBnfn:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%xmm29,%xmm20,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ktquuzrlClgjirG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ktquuzrlClgjirG
+.L_small_initial_partial_block_ktquuzrlClgjirG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ktquuzrlClgjirG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ktquuzrlClgjirG
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ktquuzrlClgjirG:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_10_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_yFxgxyvpyskjADd
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_yFxgxyvpyskjADd
+
+.L_16_blocks_overflow_yFxgxyvpyskjADd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_yFxgxyvpyskjADd:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%ymm29,%ymm20,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_fvekCDcCnCjvyuz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_fvekCDcCnCjvyuz
+.L_small_initial_partial_block_fvekCDcCnCjvyuz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_fvekCDcCnCjvyuz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_fvekCDcCnCjvyuz
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_fvekCDcCnCjvyuz:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_11_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_ehdFBobqgzyfzCb
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_ehdFBobqgzyfzCb
+
+.L_16_blocks_overflow_ehdFBobqgzyfzCb:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_ehdFBobqgzyfzCb:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_fCeFAcrgcmdrmFt
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_fCeFAcrgcmdrmFt
+.L_small_initial_partial_block_fCeFAcrgcmdrmFt:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_fCeFAcrgcmdrmFt:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_fCeFAcrgcmdrmFt
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_fCeFAcrgcmdrmFt:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_12_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_lCnrszzhAqwmtBh
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_lCnrszzhAqwmtBh
+
+.L_16_blocks_overflow_lCnrszzhAqwmtBh:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_lCnrszzhAqwmtBh:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_xhBjicgzjftnDvh
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_xhBjicgzjftnDvh
+.L_small_initial_partial_block_xhBjicgzjftnDvh:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_xhBjicgzjftnDvh:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_xhBjicgzjftnDvh
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_xhBjicgzjftnDvh:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_13_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_AcGpcGsrxCbmrzp
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_AcGpcGsrxCbmrzp
+
+.L_16_blocks_overflow_AcGpcGsrxCbmrzp:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_AcGpcGsrxCbmrzp:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%xmm29,%xmm21,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_mBsmvDlGmdeiEDh
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_mBsmvDlGmdeiEDh
+.L_small_initial_partial_block_mBsmvDlGmdeiEDh:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_mBsmvDlGmdeiEDh:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_mBsmvDlGmdeiEDh
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_mBsmvDlGmdeiEDh:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_14_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_ajgfovtkxrswvto
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_ajgfovtkxrswvto
+
+.L_16_blocks_overflow_ajgfovtkxrswvto:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_ajgfovtkxrswvto:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%ymm29,%ymm21,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_jamjbfiqpcmyhsg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_jamjbfiqpcmyhsg
+.L_small_initial_partial_block_jamjbfiqpcmyhsg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_jamjbfiqpcmyhsg:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_jamjbfiqpcmyhsg
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_jamjbfiqpcmyhsg:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_15_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_xoqyeqdrkCluywc
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_xoqyeqdrkCluywc
+
+.L_16_blocks_overflow_xoqyeqdrkCluywc:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_xoqyeqdrkCluywc:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_yjwcvsyxjDGfykF
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_yjwcvsyxjDGfykF
+.L_small_initial_partial_block_yjwcvsyxjDGfykF:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_yjwcvsyxjDGfykF:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_yjwcvsyxjDGfykF
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_yjwcvsyxjDGfykF:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_16_Cedzykoatrqkpvp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_vqxxpdzgftewxjg
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_vqxxpdzgftewxjg
+
+.L_16_blocks_overflow_vqxxpdzgftewxjg:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_vqxxpdzgftewxjg:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_djhgjvuAEhzDfaE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_djhgjvuAEhzDfaE:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_djhgjvuAEhzDfaE:
+	jmp	.L_last_blocks_done_Cedzykoatrqkpvp
+.L_last_num_blocks_is_0_Cedzykoatrqkpvp:
+	vmovdqa64	1280(%rsp),%zmm13
+	vmovdqu64	512(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1344(%rsp),%zmm13
+	vmovdqu64	576(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1408(%rsp),%zmm13
+	vmovdqu64	640(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1472(%rsp),%zmm13
+	vmovdqu64	704(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_Cedzykoatrqkpvp:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_vbsGndgytpgfqCd
+
+.L_message_below_32_blocks_vbsGndgytpgfqCd:
+
+
+	subq	$256,%r8
+	addq	$256,%rax
+	movl	%r8d,%r10d
+	leaq	96(%rsi),%r12
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_ildiycGoEzjbwEq
+	vmovdqu64	640(%rsp),%zmm3
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	576(%rsp),%zmm4
+	vmovdqu64	512(%rsp),%zmm5
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,256(%rsp)
+.L_skip_hkeys_precomputation_ildiycGoEzjbwEq:
+	movq	$1,%r14
+	andl	$~15,%r10d
+	movl	$512,%ebx
+	subl	%r10d,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_bogBdzlDGrFjDBh
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_bogBdzlDGrFjDBh
+	jb	.L_last_num_blocks_is_7_1_bogBdzlDGrFjDBh
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_bogBdzlDGrFjDBh
+	jb	.L_last_num_blocks_is_11_9_bogBdzlDGrFjDBh
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_bogBdzlDGrFjDBh
+	ja	.L_last_num_blocks_is_16_bogBdzlDGrFjDBh
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_bogBdzlDGrFjDBh
+	jmp	.L_last_num_blocks_is_13_bogBdzlDGrFjDBh
+
+.L_last_num_blocks_is_11_9_bogBdzlDGrFjDBh:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_bogBdzlDGrFjDBh
+	ja	.L_last_num_blocks_is_11_bogBdzlDGrFjDBh
+	jmp	.L_last_num_blocks_is_9_bogBdzlDGrFjDBh
+
+.L_last_num_blocks_is_7_1_bogBdzlDGrFjDBh:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_bogBdzlDGrFjDBh
+	jb	.L_last_num_blocks_is_3_1_bogBdzlDGrFjDBh
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_bogBdzlDGrFjDBh
+	je	.L_last_num_blocks_is_6_bogBdzlDGrFjDBh
+	jmp	.L_last_num_blocks_is_5_bogBdzlDGrFjDBh
+
+.L_last_num_blocks_is_3_1_bogBdzlDGrFjDBh:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_bogBdzlDGrFjDBh
+	je	.L_last_num_blocks_is_2_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_1_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_chpdBsttbykczxk
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_chpdBsttbykczxk
+
+.L_16_blocks_overflow_chpdBsttbykczxk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_chpdBsttbykczxk:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%xmm29,%xmm17,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ocxEyCqBycmFkxl
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ocxEyCqBycmFkxl
+.L_small_initial_partial_block_ocxEyCqBycmFkxl:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_ocxEyCqBycmFkxl
+.L_small_initial_compute_done_ocxEyCqBycmFkxl:
+.L_after_reduction_ocxEyCqBycmFkxl:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_2_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_FCgFgiaEGqjCcyh
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_FCgFgiaEGqjCcyh
+
+.L_16_blocks_overflow_FCgFgiaEGqjCcyh:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_FCgFgiaEGqjCcyh:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%ymm29,%ymm17,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_qoxCzmccdveFoAk
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_qoxCzmccdveFoAk
+.L_small_initial_partial_block_qoxCzmccdveFoAk:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_qoxCzmccdveFoAk:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_qoxCzmccdveFoAk
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_qoxCzmccdveFoAk:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_3_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_gizhDwfCksfweCD
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_gizhDwfCksfweCD
+
+.L_16_blocks_overflow_gizhDwfCksfweCD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_gizhDwfCksfweCD:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_alCmzqqfyEhlnFt
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_alCmzqqfyEhlnFt
+.L_small_initial_partial_block_alCmzqqfyEhlnFt:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_alCmzqqfyEhlnFt:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_alCmzqqfyEhlnFt
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_alCmzqqfyEhlnFt:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_4_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_cvqfassGjqfmbFc
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_cvqfassGjqfmbFc
+
+.L_16_blocks_overflow_cvqfassGjqfmbFc:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_cvqfassGjqfmbFc:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tshsBaxiugmxapq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tshsBaxiugmxapq
+.L_small_initial_partial_block_tshsBaxiugmxapq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tshsBaxiugmxapq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tshsBaxiugmxapq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tshsBaxiugmxapq:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_5_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_tumjmGqednyisqq
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_tumjmGqednyisqq
+
+.L_16_blocks_overflow_tumjmGqednyisqq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_tumjmGqednyisqq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%xmm29,%xmm19,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ErAinruDddolxzb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ErAinruDddolxzb
+.L_small_initial_partial_block_ErAinruDddolxzb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ErAinruDddolxzb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ErAinruDddolxzb
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ErAinruDddolxzb:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_6_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_tyDphneEDxtrgcb
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_tyDphneEDxtrgcb
+
+.L_16_blocks_overflow_tyDphneEDxtrgcb:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_tyDphneEDxtrgcb:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%ymm29,%ymm19,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_honzuGrsgyCGlys
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_honzuGrsgyCGlys
+.L_small_initial_partial_block_honzuGrsgyCGlys:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_honzuGrsgyCGlys:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_honzuGrsgyCGlys
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_honzuGrsgyCGlys:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_7_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_qmpebkDzDgevnuw
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_qmpebkDzDgevnuw
+
+.L_16_blocks_overflow_qmpebkDzDgevnuw:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_qmpebkDzDgevnuw:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_gFDanslDaekDjai
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_gFDanslDaekDjai
+.L_small_initial_partial_block_gFDanslDaekDjai:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_gFDanslDaekDjai:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_gFDanslDaekDjai
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_gFDanslDaekDjai:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_8_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_mxCwGvaudzkhEgm
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_mxCwGvaudzkhEgm
+
+.L_16_blocks_overflow_mxCwGvaudzkhEgm:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_mxCwGvaudzkhEgm:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dybGaraACuAFsCl
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dybGaraACuAFsCl
+.L_small_initial_partial_block_dybGaraACuAFsCl:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_dybGaraACuAFsCl:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_dybGaraACuAFsCl
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_dybGaraACuAFsCl:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_9_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_rDseoawfpmsonAt
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_rDseoawfpmsonAt
+
+.L_16_blocks_overflow_rDseoawfpmsonAt:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_rDseoawfpmsonAt:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%xmm29,%xmm20,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wgCGscetmzcBBlv
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wgCGscetmzcBBlv
+.L_small_initial_partial_block_wgCGscetmzcBBlv:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_wgCGscetmzcBBlv:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_wgCGscetmzcBBlv
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_wgCGscetmzcBBlv:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_10_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_DhjmbcDtanmucun
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_DhjmbcDtanmucun
+
+.L_16_blocks_overflow_DhjmbcDtanmucun:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_DhjmbcDtanmucun:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%ymm29,%ymm20,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_oiwdjAnqjADqeoD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_oiwdjAnqjADqeoD
+.L_small_initial_partial_block_oiwdjAnqjADqeoD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_oiwdjAnqjADqeoD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_oiwdjAnqjADqeoD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_oiwdjAnqjADqeoD:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_11_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_gEzFpyrgdgkxjBp
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_gEzFpyrgdgkxjBp
+
+.L_16_blocks_overflow_gEzFpyrgdgkxjBp:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_gEzFpyrgdgkxjBp:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_nFsrhBFEEhEFrxy
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_nFsrhBFEEhEFrxy
+.L_small_initial_partial_block_nFsrhBFEEhEFrxy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_nFsrhBFEEhEFrxy:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_nFsrhBFEEhEFrxy
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_nFsrhBFEEhEFrxy:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_12_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_cBcgnsCfaDzxpvd
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_cBcgnsCfaDzxpvd
+
+.L_16_blocks_overflow_cBcgnsCfaDzxpvd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_cBcgnsCfaDzxpvd:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CoxltfbtlBdCtnc
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CoxltfbtlBdCtnc
+.L_small_initial_partial_block_CoxltfbtlBdCtnc:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CoxltfbtlBdCtnc:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CoxltfbtlBdCtnc
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CoxltfbtlBdCtnc:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_13_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_CdroFrygyBxptcg
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_CdroFrygyBxptcg
+
+.L_16_blocks_overflow_CdroFrygyBxptcg:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_CdroFrygyBxptcg:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%xmm29,%xmm21,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_trpEaFclttFcdka
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_trpEaFclttFcdka
+.L_small_initial_partial_block_trpEaFclttFcdka:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_trpEaFclttFcdka:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_trpEaFclttFcdka
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_trpEaFclttFcdka:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_14_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_lwBbamlpsuGdvrE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_lwBbamlpsuGdvrE
+
+.L_16_blocks_overflow_lwBbamlpsuGdvrE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_lwBbamlpsuGdvrE:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%ymm29,%ymm21,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tgEcrijBvGlkxlv
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tgEcrijBvGlkxlv
+.L_small_initial_partial_block_tgEcrijBvGlkxlv:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tgEcrijBvGlkxlv:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tgEcrijBvGlkxlv
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tgEcrijBvGlkxlv:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_15_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_iDErxyzaxlpaboy
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_iDErxyzaxlpaboy
+
+.L_16_blocks_overflow_iDErxyzaxlpaboy:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_iDErxyzaxlpaboy:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ncpxovfhaoiyAuq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ncpxovfhaoiyAuq
+.L_small_initial_partial_block_ncpxovfhaoiyAuq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ncpxovfhaoiyAuq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ncpxovfhaoiyAuq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ncpxovfhaoiyAuq:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_16_bogBdzlDGrFjDBh:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_tkwinrasifjEFxz
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_tkwinrasifjEFxz
+
+.L_16_blocks_overflow_tkwinrasifjEFxz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_tkwinrasifjEFxz:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_ttqGzciAECttEDy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ttqGzciAECttEDy:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ttqGzciAECttEDy:
+	jmp	.L_last_blocks_done_bogBdzlDGrFjDBh
+.L_last_num_blocks_is_0_bogBdzlDGrFjDBh:
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_bogBdzlDGrFjDBh:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_vbsGndgytpgfqCd
+
+.L_message_below_equal_16_blocks_vbsGndgytpgfqCd:
+
+
+	movl	%r8d,%r12d
+	addl	$15,%r12d
+	shrl	$4,%r12d
+	cmpq	$8,%r12
+	je	.L_small_initial_num_blocks_is_8_ncGqwijFizzzvGl
+	jl	.L_small_initial_num_blocks_is_7_1_ncGqwijFizzzvGl
+
+
+	cmpq	$12,%r12
+	je	.L_small_initial_num_blocks_is_12_ncGqwijFizzzvGl
+	jl	.L_small_initial_num_blocks_is_11_9_ncGqwijFizzzvGl
+
+
+	cmpq	$16,%r12
+	je	.L_small_initial_num_blocks_is_16_ncGqwijFizzzvGl
+	cmpq	$15,%r12
+	je	.L_small_initial_num_blocks_is_15_ncGqwijFizzzvGl
+	cmpq	$14,%r12
+	je	.L_small_initial_num_blocks_is_14_ncGqwijFizzzvGl
+	jmp	.L_small_initial_num_blocks_is_13_ncGqwijFizzzvGl
+
+.L_small_initial_num_blocks_is_11_9_ncGqwijFizzzvGl:
+
+	cmpq	$11,%r12
+	je	.L_small_initial_num_blocks_is_11_ncGqwijFizzzvGl
+	cmpq	$10,%r12
+	je	.L_small_initial_num_blocks_is_10_ncGqwijFizzzvGl
+	jmp	.L_small_initial_num_blocks_is_9_ncGqwijFizzzvGl
+
+.L_small_initial_num_blocks_is_7_1_ncGqwijFizzzvGl:
+	cmpq	$4,%r12
+	je	.L_small_initial_num_blocks_is_4_ncGqwijFizzzvGl
+	jl	.L_small_initial_num_blocks_is_3_1_ncGqwijFizzzvGl
+
+	cmpq	$7,%r12
+	je	.L_small_initial_num_blocks_is_7_ncGqwijFizzzvGl
+	cmpq	$6,%r12
+	je	.L_small_initial_num_blocks_is_6_ncGqwijFizzzvGl
+	jmp	.L_small_initial_num_blocks_is_5_ncGqwijFizzzvGl
+
+.L_small_initial_num_blocks_is_3_1_ncGqwijFizzzvGl:
+
+	cmpq	$3,%r12
+	je	.L_small_initial_num_blocks_is_3_ncGqwijFizzzvGl
+	cmpq	$2,%r12
+	je	.L_small_initial_num_blocks_is_2_ncGqwijFizzzvGl
+
+
+
+
+
+.L_small_initial_num_blocks_is_1_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%xmm29
+	vpaddd	ONE(%rip),%xmm2,%xmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vpshufb	%xmm29,%xmm0,%xmm0
+	vmovdqu8	0(%rcx,%rax,1),%xmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%xmm15,%xmm0,%xmm0
+	vpxorq	%xmm6,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm6,%xmm6
+	vextracti32x4	$0,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wsqaztwzEpfwyrz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wsqaztwzEpfwyrz
+.L_small_initial_partial_block_wsqaztwzEpfwyrz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm13,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_wsqaztwzEpfwyrz
+.L_small_initial_compute_done_wsqaztwzEpfwyrz:
+.L_after_reduction_wsqaztwzEpfwyrz:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_2_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%ymm29
+	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
+	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vpshufb	%ymm29,%ymm0,%ymm0
+	vmovdqu8	0(%rcx,%rax,1),%ymm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%ymm15,%ymm0,%ymm0
+	vpxorq	%ymm6,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm6,%ymm6
+	vextracti32x4	$1,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GigktkuChyFavrd
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GigktkuChyFavrd
+.L_small_initial_partial_block_GigktkuChyFavrd:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GigktkuChyFavrd:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GigktkuChyFavrd
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_GigktkuChyFavrd:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_3_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vextracti32x4	$2,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_rGuawcmuegcpCgc
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_rGuawcmuegcpCgc
+.L_small_initial_partial_block_rGuawcmuegcpCgc:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_rGuawcmuegcpCgc:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_rGuawcmuegcpCgc
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_rGuawcmuegcpCgc:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_4_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vextracti32x4	$3,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dDiDfqjdqyefwxF
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dDiDfqjdqyefwxF
+.L_small_initial_partial_block_dDiDfqjdqyefwxF:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_dDiDfqjdqyefwxF:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_dDiDfqjdqyefwxF
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_dDiDfqjdqyefwxF:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_5_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%xmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%xmm15,%xmm3,%xmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%xmm7,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%xmm29,%xmm7,%xmm7
+	vextracti32x4	$0,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ibwrCEctfFzhjoG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ibwrCEctfFzhjoG
+.L_small_initial_partial_block_ibwrCEctfFzhjoG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ibwrCEctfFzhjoG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ibwrCEctfFzhjoG
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_ibwrCEctfFzhjoG:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_6_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%ymm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%ymm15,%ymm3,%ymm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%ymm7,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%ymm29,%ymm7,%ymm7
+	vextracti32x4	$1,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_xDifexifaGxGClF
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_xDifexifaGxGClF
+.L_small_initial_partial_block_xDifexifaGxGClF:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_xDifexifaGxGClF:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_xDifexifaGxGClF
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_xDifexifaGxGClF:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_7_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vextracti32x4	$2,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_usvmkdhfmAkrckm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_usvmkdhfmAkrckm
+.L_small_initial_partial_block_usvmkdhfmAkrckm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_usvmkdhfmAkrckm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_usvmkdhfmAkrckm
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_usvmkdhfmAkrckm:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_8_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vextracti32x4	$3,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_pudmodcouDrGaoi
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_pudmodcouDrGaoi
+.L_small_initial_partial_block_pudmodcouDrGaoi:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_pudmodcouDrGaoi:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_pudmodcouDrGaoi
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_pudmodcouDrGaoi:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_9_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%xmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%xmm15,%xmm4,%xmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%xmm10,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%xmm29,%xmm10,%xmm10
+	vextracti32x4	$0,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_hazElrsxbocFdDb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_hazElrsxbocFdDb
+.L_small_initial_partial_block_hazElrsxbocFdDb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_hazElrsxbocFdDb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_hazElrsxbocFdDb
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_hazElrsxbocFdDb:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_10_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%ymm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%ymm15,%ymm4,%ymm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%ymm10,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%ymm29,%ymm10,%ymm10
+	vextracti32x4	$1,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_mbfAklptzneyqdy
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_mbfAklptzneyqdy
+.L_small_initial_partial_block_mbfAklptzneyqdy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_mbfAklptzneyqdy:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_mbfAklptzneyqdy
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_mbfAklptzneyqdy:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_11_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vextracti32x4	$2,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_lAieFkoxpsvFAeq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_lAieFkoxpsvFAeq
+.L_small_initial_partial_block_lAieFkoxpsvFAeq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_lAieFkoxpsvFAeq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_lAieFkoxpsvFAeq
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_lAieFkoxpsvFAeq:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_12_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vextracti32x4	$3,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_FgpayByakqxzouC
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	64(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_FgpayByakqxzouC
+.L_small_initial_partial_block_FgpayByakqxzouC:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_FgpayByakqxzouC:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_FgpayByakqxzouC
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_FgpayByakqxzouC:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_13_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%xmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%xmm15,%xmm5,%xmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%xmm11,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%xmm29,%xmm11,%xmm11
+	vextracti32x4	$0,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dpFnumljtAhwbnk
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	48(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dpFnumljtAhwbnk
+.L_small_initial_partial_block_dpFnumljtAhwbnk:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	64(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_dpFnumljtAhwbnk:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_dpFnumljtAhwbnk
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_dpFnumljtAhwbnk:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_14_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%ymm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%ymm15,%ymm5,%ymm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%ymm11,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%ymm29,%ymm11,%ymm11
+	vextracti32x4	$1,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_jdxxBfbkFefsApv
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	32(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_jdxxBfbkFefsApv
+.L_small_initial_partial_block_jdxxBfbkFefsApv:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	48(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_jdxxBfbkFefsApv:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_jdxxBfbkFefsApv
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_jdxxBfbkFefsApv:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_15_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%zmm15,%zmm5,%zmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%zmm11,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vextracti32x4	$2,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dygwkberAdhpikp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	16(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dygwkberAdhpikp
+.L_small_initial_partial_block_dygwkberAdhpikp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	32(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_dygwkberAdhpikp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_dygwkberAdhpikp
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_dygwkberAdhpikp:
+	jmp	.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl
+.L_small_initial_num_blocks_is_16_ncGqwijFizzzvGl:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%zmm15,%zmm5,%zmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%zmm11,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vextracti32x4	$3,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_tFgDdGbqCCxGuzB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	16(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tFgDdGbqCCxGuzB:
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_tFgDdGbqCCxGuzB:
+.L_small_initial_blocks_encrypted_ncGqwijFizzzvGl:
+.L_ghash_done_vbsGndgytpgfqCd:
+	vmovdqu64	%xmm2,0(%rsi)
+.L_enc_dec_done_vbsGndgytpgfqCd:
+
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+	vmovdqu64	%xmm14,64(%rsi)
+.L_enc_dec_abort_vbsGndgytpgfqCd:
+	jmp	.Lexit_gcm_decrypt
+.align	32
+.Laes_gcm_decrypt_192_avx512:
+	orq	%r8,%r8
+	je	.L_enc_dec_abort_idpupdakyookwwt
+	xorq	%r14,%r14
+	vmovdqu64	64(%rsi),%xmm14
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+
+	movl	(%rdx),%eax
+	orq	%rax,%rax
+	je	.L_partial_block_done_sEcwBgAzrdjpjFa
+	movl	$16,%r10d
+	leaq	byte_len_to_mask_table(%rip),%r12
+	cmpq	%r10,%r8
+	cmovcq	%r8,%r10
+	kmovw	(%r12,%r10,2),%k1
+	vmovdqu8	(%rcx),%xmm0{%k1}{z}
+
+	vmovdqu64	16(%rsi),%xmm3
+
+	leaq	96(%rsi),%r10
+	vmovdqu64	240(%r10),%xmm4
+
+
+
+	leaq	SHIFT_MASK(%rip),%r12
+	addq	%rax,%r12
+	vmovdqu64	(%r12),%xmm5
+	vpshufb	%xmm5,%xmm3,%xmm3
+
+	vmovdqa64	%xmm0,%xmm6
+	vpxorq	%xmm0,%xmm3,%xmm3
+
+
+	leaq	(%r8,%rax,1),%r13
+	subq	$16,%r13
+	jge	.L_no_extra_mask_sEcwBgAzrdjpjFa
+	subq	%r13,%r12
+.L_no_extra_mask_sEcwBgAzrdjpjFa:
+
+
+
+	vmovdqu64	16(%r12),%xmm0
+	vpand	%xmm0,%xmm3,%xmm3
+	vpand	%xmm0,%xmm6,%xmm6
+	vpshufb	SHUF_MASK(%rip),%xmm6,%xmm6
+	vpshufb	%xmm5,%xmm6,%xmm6
+	vpxorq	%xmm6,%xmm14,%xmm14
+	cmpq	$0,%r13
+	jl	.L_partial_incomplete_sEcwBgAzrdjpjFa
+
+	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
+	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
+	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
+	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
+	vpxorq	%xmm11,%xmm14,%xmm14
+
+	vpsrldq	$8,%xmm14,%xmm11
+	vpslldq	$8,%xmm14,%xmm14
+	vpxorq	%xmm11,%xmm7,%xmm7
+	vpxorq	%xmm10,%xmm14,%xmm14
+
+
+
+	vmovdqu64	POLY2(%rip),%xmm11
+
+	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
+	vpslldq	$8,%xmm10,%xmm10
+	vpxorq	%xmm10,%xmm14,%xmm14
+
+
+
+	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
+	vpsrldq	$4,%xmm10,%xmm10
+	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+
+	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14
+
+	movl	$0,(%rdx)
+
+	movq	%rax,%r12
+	movq	$16,%rax
+	subq	%r12,%rax
+	jmp	.L_enc_dec_done_sEcwBgAzrdjpjFa
+
+.L_partial_incomplete_sEcwBgAzrdjpjFa:
+	addl	%r8d,(%rdx)
+	movq	%r8,%rax
+
+.L_enc_dec_done_sEcwBgAzrdjpjFa:
+
+
+	leaq	byte_len_to_mask_table(%rip),%r12
+	kmovw	(%r12,%rax,2),%k1
+	movq	%r9,%r12
+	vmovdqu8	%xmm3,(%r12){%k1}
+.L_partial_block_done_sEcwBgAzrdjpjFa:
+	vmovdqu64	0(%rsi),%xmm2
+	subq	%rax,%r8
+	je	.L_enc_dec_done_idpupdakyookwwt
+	cmpq	$256,%r8
+	jbe	.L_message_below_equal_16_blocks_idpupdakyookwwt
+
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
+	vmovdqa64	ddq_addbe_1234(%rip),%zmm28
+
+
+
+
+
+
+	vmovd	%xmm2,%r15d
+	andl	$255,%r15d
+
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpshufb	%zmm29,%zmm2,%zmm2
+
+
+
+	cmpb	$240,%r15b
+	jae	.L_next_16_overflow_grwkeEbEhcqzpFd
+	vpaddd	%zmm28,%zmm2,%zmm7
+	vpaddd	%zmm27,%zmm7,%zmm10
+	vpaddd	%zmm27,%zmm10,%zmm11
+	vpaddd	%zmm27,%zmm11,%zmm12
+	jmp	.L_next_16_ok_grwkeEbEhcqzpFd
+.L_next_16_overflow_grwkeEbEhcqzpFd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm12
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
+	vpaddd	%zmm12,%zmm7,%zmm10
+	vpaddd	%zmm12,%zmm10,%zmm11
+	vpaddd	%zmm12,%zmm11,%zmm12
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+.L_next_16_ok_grwkeEbEhcqzpFd:
+	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
+	addb	$16,%r15b
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm0
+	vmovdqu8	64(%rcx,%rax,1),%zmm3
+	vmovdqu8	128(%rcx,%rax,1),%zmm4
+	vmovdqu8	192(%rcx,%rax,1),%zmm5
+
+
+	vbroadcastf64x2	0(%rdi),%zmm6
+	vpxorq	%zmm6,%zmm7,%zmm7
+	vpxorq	%zmm6,%zmm10,%zmm10
+	vpxorq	%zmm6,%zmm11,%zmm11
+	vpxorq	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	16(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	32(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	48(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	64(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	80(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	96(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	112(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	128(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	144(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	160(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	176(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	192(%rdi),%zmm6
+	vaesenclast	%zmm6,%zmm7,%zmm7
+	vaesenclast	%zmm6,%zmm10,%zmm10
+	vaesenclast	%zmm6,%zmm11,%zmm11
+	vaesenclast	%zmm6,%zmm12,%zmm12
+
+
+	vpxorq	%zmm0,%zmm7,%zmm7
+	vpxorq	%zmm3,%zmm10,%zmm10
+	vpxorq	%zmm4,%zmm11,%zmm11
+	vpxorq	%zmm5,%zmm12,%zmm12
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm7,0(%r10,%rax,1)
+	vmovdqu8	%zmm10,64(%r10,%rax,1)
+	vmovdqu8	%zmm11,128(%r10,%rax,1)
+	vmovdqu8	%zmm12,192(%r10,%rax,1)
+
+	vpshufb	%zmm29,%zmm0,%zmm7
+	vpshufb	%zmm29,%zmm3,%zmm10
+	vpshufb	%zmm29,%zmm4,%zmm11
+	vpshufb	%zmm29,%zmm5,%zmm12
+	vmovdqa64	%zmm7,768(%rsp)
+	vmovdqa64	%zmm10,832(%rsp)
+	vmovdqa64	%zmm11,896(%rsp)
+	vmovdqa64	%zmm12,960(%rsp)
+	leaq	96(%rsi),%r12
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_BuedDssrdeFzckC
+
+	vmovdqu64	192(%r12),%zmm0
+	vmovdqu64	%zmm0,704(%rsp)
+
+	vmovdqu64	128(%r12),%zmm3
+	vmovdqu64	%zmm3,640(%rsp)
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	64(%r12),%zmm4
+	vmovdqu64	%zmm4,576(%rsp)
+
+	vmovdqu64	0(%r12),%zmm5
+	vmovdqu64	%zmm5,512(%rsp)
+.L_skip_hkeys_precomputation_BuedDssrdeFzckC:
+	cmpq	$512,%r8
+	jb	.L_message_below_32_blocks_idpupdakyookwwt
+
+
+
+	cmpb	$240,%r15b
+	jae	.L_next_16_overflow_zFhxjiFGeunCpFx
+	vpaddd	%zmm28,%zmm2,%zmm7
+	vpaddd	%zmm27,%zmm7,%zmm10
+	vpaddd	%zmm27,%zmm10,%zmm11
+	vpaddd	%zmm27,%zmm11,%zmm12
+	jmp	.L_next_16_ok_zFhxjiFGeunCpFx
+.L_next_16_overflow_zFhxjiFGeunCpFx:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm12
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
+	vpaddd	%zmm12,%zmm7,%zmm10
+	vpaddd	%zmm12,%zmm10,%zmm11
+	vpaddd	%zmm12,%zmm11,%zmm12
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+.L_next_16_ok_zFhxjiFGeunCpFx:
+	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
+	addb	$16,%r15b
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm0
+	vmovdqu8	320(%rcx,%rax,1),%zmm3
+	vmovdqu8	384(%rcx,%rax,1),%zmm4
+	vmovdqu8	448(%rcx,%rax,1),%zmm5
+
+
+	vbroadcastf64x2	0(%rdi),%zmm6
+	vpxorq	%zmm6,%zmm7,%zmm7
+	vpxorq	%zmm6,%zmm10,%zmm10
+	vpxorq	%zmm6,%zmm11,%zmm11
+	vpxorq	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	16(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	32(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	48(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	64(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	80(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	96(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	112(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	128(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	144(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	160(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	176(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	192(%rdi),%zmm6
+	vaesenclast	%zmm6,%zmm7,%zmm7
+	vaesenclast	%zmm6,%zmm10,%zmm10
+	vaesenclast	%zmm6,%zmm11,%zmm11
+	vaesenclast	%zmm6,%zmm12,%zmm12
+
+
+	vpxorq	%zmm0,%zmm7,%zmm7
+	vpxorq	%zmm3,%zmm10,%zmm10
+	vpxorq	%zmm4,%zmm11,%zmm11
+	vpxorq	%zmm5,%zmm12,%zmm12
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm7,256(%r10,%rax,1)
+	vmovdqu8	%zmm10,320(%r10,%rax,1)
+	vmovdqu8	%zmm11,384(%r10,%rax,1)
+	vmovdqu8	%zmm12,448(%r10,%rax,1)
+
+	vpshufb	%zmm29,%zmm0,%zmm7
+	vpshufb	%zmm29,%zmm3,%zmm10
+	vpshufb	%zmm29,%zmm4,%zmm11
+	vpshufb	%zmm29,%zmm5,%zmm12
+	vmovdqa64	%zmm7,1024(%rsp)
+	vmovdqa64	%zmm10,1088(%rsp)
+	vmovdqa64	%zmm11,1152(%rsp)
+	vmovdqa64	%zmm12,1216(%rsp)
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_indAFgkywvzkfuh
+	vmovdqu64	640(%rsp),%zmm3
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	576(%rsp),%zmm4
+	vmovdqu64	512(%rsp),%zmm5
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,256(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,192(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,128(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,64(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,0(%rsp)
+.L_skip_hkeys_precomputation_indAFgkywvzkfuh:
+	movq	$1,%r14
+	addq	$512,%rax
+	subq	$512,%r8
+
+	cmpq	$768,%r8
+	jb	.L_no_more_big_nblocks_idpupdakyookwwt
+.L_encrypt_big_nblocks_idpupdakyookwwt:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_vpuFobmAooDDAkE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_vpuFobmAooDDAkE
+.L_16_blocks_overflow_vpuFobmAooDDAkE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_vpuFobmAooDDAkE:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_zfEhgAdtieGtdpa
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_zfEhgAdtieGtdpa
+.L_16_blocks_overflow_zfEhgAdtieGtdpa:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_zfEhgAdtieGtdpa:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	256(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	320(%rsp),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	384(%rsp),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	448(%rsp),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm17
+	vmovdqu8	320(%rcx,%rax,1),%zmm19
+	vmovdqu8	384(%rcx,%rax,1),%zmm20
+	vmovdqu8	448(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,256(%r10,%rax,1)
+	vmovdqu8	%zmm3,320(%r10,%rax,1)
+	vmovdqu8	%zmm4,384(%r10,%rax,1)
+	vmovdqu8	%zmm5,448(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,768(%rsp)
+	vmovdqa64	%zmm3,832(%rsp)
+	vmovdqa64	%zmm4,896(%rsp)
+	vmovdqa64	%zmm5,960(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_ouyBsnwwdlsCdEa
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_ouyBsnwwdlsCdEa
+.L_16_blocks_overflow_ouyBsnwwdlsCdEa:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_ouyBsnwwdlsCdEa:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	512(%rcx,%rax,1),%zmm17
+	vmovdqu8	576(%rcx,%rax,1),%zmm19
+	vmovdqu8	640(%rcx,%rax,1),%zmm20
+	vmovdqu8	704(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+
+
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
+	vpxorq	%zmm24,%zmm6,%zmm6
+	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
+	vpxorq	%zmm25,%zmm7,%zmm7
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vextracti64x4	$1,%zmm6,%ymm12
+	vpxorq	%ymm12,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm12
+	vpxorq	%xmm12,%xmm6,%xmm6
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,512(%r10,%rax,1)
+	vmovdqu8	%zmm3,576(%r10,%rax,1)
+	vmovdqu8	%zmm4,640(%r10,%rax,1)
+	vmovdqu8	%zmm5,704(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,1024(%rsp)
+	vmovdqa64	%zmm3,1088(%rsp)
+	vmovdqa64	%zmm4,1152(%rsp)
+	vmovdqa64	%zmm5,1216(%rsp)
+	vmovdqa64	%zmm6,%zmm14
+
+	addq	$768,%rax
+	subq	$768,%r8
+	cmpq	$768,%r8
+	jae	.L_encrypt_big_nblocks_idpupdakyookwwt
+
+.L_no_more_big_nblocks_idpupdakyookwwt:
+
+	cmpq	$512,%r8
+	jae	.L_encrypt_32_blocks_idpupdakyookwwt
+
+	cmpq	$256,%r8
+	jae	.L_encrypt_16_blocks_idpupdakyookwwt
+.L_encrypt_0_blocks_ghash_32_idpupdakyookwwt:
+	movl	%r8d,%r10d
+	andl	$~15,%r10d
+	movl	$256,%ebx
+	subl	%r10d,%ebx
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	addl	$256,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_vlClyryvtabmszu
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_vlClyryvtabmszu
+	jb	.L_last_num_blocks_is_7_1_vlClyryvtabmszu
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_vlClyryvtabmszu
+	jb	.L_last_num_blocks_is_11_9_vlClyryvtabmszu
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_vlClyryvtabmszu
+	ja	.L_last_num_blocks_is_16_vlClyryvtabmszu
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_vlClyryvtabmszu
+	jmp	.L_last_num_blocks_is_13_vlClyryvtabmszu
+
+.L_last_num_blocks_is_11_9_vlClyryvtabmszu:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_vlClyryvtabmszu
+	ja	.L_last_num_blocks_is_11_vlClyryvtabmszu
+	jmp	.L_last_num_blocks_is_9_vlClyryvtabmszu
+
+.L_last_num_blocks_is_7_1_vlClyryvtabmszu:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_vlClyryvtabmszu
+	jb	.L_last_num_blocks_is_3_1_vlClyryvtabmszu
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_vlClyryvtabmszu
+	je	.L_last_num_blocks_is_6_vlClyryvtabmszu
+	jmp	.L_last_num_blocks_is_5_vlClyryvtabmszu
+
+.L_last_num_blocks_is_3_1_vlClyryvtabmszu:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_vlClyryvtabmszu
+	je	.L_last_num_blocks_is_2_vlClyryvtabmszu
+.L_last_num_blocks_is_1_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_augxdybnzzqBlFq
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_augxdybnzzqBlFq
+
+.L_16_blocks_overflow_augxdybnzzqBlFq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_augxdybnzzqBlFq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%xmm29,%xmm17,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BDzBGxAqwjiwDor
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BDzBGxAqwjiwDor
+.L_small_initial_partial_block_BDzBGxAqwjiwDor:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_BDzBGxAqwjiwDor
+.L_small_initial_compute_done_BDzBGxAqwjiwDor:
+.L_after_reduction_BDzBGxAqwjiwDor:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_2_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_eqcubnyqxFwzmvq
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_eqcubnyqxFwzmvq
+
+.L_16_blocks_overflow_eqcubnyqxFwzmvq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_eqcubnyqxFwzmvq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%ymm29,%ymm17,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GazizskujbucGFs
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GazizskujbucGFs
+.L_small_initial_partial_block_GazizskujbucGFs:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GazizskujbucGFs:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GazizskujbucGFs
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GazizskujbucGFs:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_3_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_AejftBoDtDikaoh
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_AejftBoDtDikaoh
+
+.L_16_blocks_overflow_AejftBoDtDikaoh:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_AejftBoDtDikaoh:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ejuCyxDfmvvqECp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ejuCyxDfmvvqECp
+.L_small_initial_partial_block_ejuCyxDfmvvqECp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ejuCyxDfmvvqECp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ejuCyxDfmvvqECp
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ejuCyxDfmvvqECp:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_4_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_FubBAwoBDAgErqd
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_FubBAwoBDAgErqd
+
+.L_16_blocks_overflow_FubBAwoBDAgErqd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_FubBAwoBDAgErqd:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CkdyhphyAsekmDG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CkdyhphyAsekmDG
+.L_small_initial_partial_block_CkdyhphyAsekmDG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CkdyhphyAsekmDG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CkdyhphyAsekmDG
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CkdyhphyAsekmDG:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_5_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_DvxjEosxcGagjoz
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_DvxjEosxcGagjoz
+
+.L_16_blocks_overflow_DvxjEosxcGagjoz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_DvxjEosxcGagjoz:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%xmm29,%xmm19,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_Bnbpwhqzxyvrlid
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_Bnbpwhqzxyvrlid
+.L_small_initial_partial_block_Bnbpwhqzxyvrlid:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_Bnbpwhqzxyvrlid:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_Bnbpwhqzxyvrlid
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_Bnbpwhqzxyvrlid:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_6_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_uegvlqgvGzjpAAC
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_uegvlqgvGzjpAAC
+
+.L_16_blocks_overflow_uegvlqgvGzjpAAC:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_uegvlqgvGzjpAAC:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%ymm29,%ymm19,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_leAwwddcldCfcwr
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_leAwwddcldCfcwr
+.L_small_initial_partial_block_leAwwddcldCfcwr:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_leAwwddcldCfcwr:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_leAwwddcldCfcwr
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_leAwwddcldCfcwr:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_7_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_uptvGioxworiyvn
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_uptvGioxworiyvn
+
+.L_16_blocks_overflow_uptvGioxworiyvn:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_uptvGioxworiyvn:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_qFyfFcjFhuoaruc
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_qFyfFcjFhuoaruc
+.L_small_initial_partial_block_qFyfFcjFhuoaruc:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_qFyfFcjFhuoaruc:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_qFyfFcjFhuoaruc
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_qFyfFcjFhuoaruc:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_8_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_pfelsqndDGawcAg
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_pfelsqndDGawcAg
+
+.L_16_blocks_overflow_pfelsqndDGawcAg:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_pfelsqndDGawcAg:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_sincuzdhrhzryvr
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_sincuzdhrhzryvr
+.L_small_initial_partial_block_sincuzdhrhzryvr:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_sincuzdhrhzryvr:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_sincuzdhrhzryvr
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_sincuzdhrhzryvr:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_9_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_mefaiqlingCtCbq
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_mefaiqlingCtCbq
+
+.L_16_blocks_overflow_mefaiqlingCtCbq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_mefaiqlingCtCbq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%xmm29,%xmm20,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_oxqqyjpGrpfhdlE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_oxqqyjpGrpfhdlE
+.L_small_initial_partial_block_oxqqyjpGrpfhdlE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_oxqqyjpGrpfhdlE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_oxqqyjpGrpfhdlE
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_oxqqyjpGrpfhdlE:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_10_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_lksygjFDCvFiziy
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_lksygjFDCvFiziy
+
+.L_16_blocks_overflow_lksygjFDCvFiziy:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_lksygjFDCvFiziy:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%ymm29,%ymm20,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_udGxaDdBvkpwCpc
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_udGxaDdBvkpwCpc
+.L_small_initial_partial_block_udGxaDdBvkpwCpc:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_udGxaDdBvkpwCpc:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_udGxaDdBvkpwCpc
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_udGxaDdBvkpwCpc:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_11_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_humhBrlryojffna
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_humhBrlryojffna
+
+.L_16_blocks_overflow_humhBrlryojffna:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_humhBrlryojffna:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_pfjkfvGBrtkAtnA
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_pfjkfvGBrtkAtnA
+.L_small_initial_partial_block_pfjkfvGBrtkAtnA:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_pfjkfvGBrtkAtnA:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_pfjkfvGBrtkAtnA
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_pfjkfvGBrtkAtnA:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_12_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_jgepmxvwirjhDrF
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_jgepmxvwirjhDrF
+
+.L_16_blocks_overflow_jgepmxvwirjhDrF:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_jgepmxvwirjhDrF:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_toAqqmiAxcEanCG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_toAqqmiAxcEanCG
+.L_small_initial_partial_block_toAqqmiAxcEanCG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_toAqqmiAxcEanCG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_toAqqmiAxcEanCG
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_toAqqmiAxcEanCG:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_13_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_whCEektghCaEcib
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_whCEektghCaEcib
+
+.L_16_blocks_overflow_whCEektghCaEcib:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_whCEektghCaEcib:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%xmm29,%xmm21,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_eynhijAFiussiyG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_eynhijAFiussiyG
+.L_small_initial_partial_block_eynhijAFiussiyG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_eynhijAFiussiyG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_eynhijAFiussiyG
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_eynhijAFiussiyG:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_14_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_szasaibEDGFtxwF
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_szasaibEDGFtxwF
+
+.L_16_blocks_overflow_szasaibEDGFtxwF:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_szasaibEDGFtxwF:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%ymm29,%ymm21,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BGnnyejbdBlgqAl
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BGnnyejbdBlgqAl
+.L_small_initial_partial_block_BGnnyejbdBlgqAl:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BGnnyejbdBlgqAl:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BGnnyejbdBlgqAl
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_BGnnyejbdBlgqAl:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_15_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_GCApgtaqkGvqmai
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_GCApgtaqkGvqmai
+
+.L_16_blocks_overflow_GCApgtaqkGvqmai:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_GCApgtaqkGvqmai:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_srcsAlsEiulGolE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_srcsAlsEiulGolE
+.L_small_initial_partial_block_srcsAlsEiulGolE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_srcsAlsEiulGolE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_srcsAlsEiulGolE
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_srcsAlsEiulGolE:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_16_vlClyryvtabmszu:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_jgjnebgmllipkza
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_jgjnebgmllipkza
+
+.L_16_blocks_overflow_jgjnebgmllipkza:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_jgjnebgmllipkza:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_fmgnqDiaDzpaGcv:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_fmgnqDiaDzpaGcv:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_fmgnqDiaDzpaGcv:
+	jmp	.L_last_blocks_done_vlClyryvtabmszu
+.L_last_num_blocks_is_0_vlClyryvtabmszu:
+	vmovdqa64	1024(%rsp),%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1088(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1152(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1216(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_vlClyryvtabmszu:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_idpupdakyookwwt
+.L_encrypt_32_blocks_idpupdakyookwwt:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_qzBCjqbmxCvGmpa
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_qzBCjqbmxCvGmpa
+.L_16_blocks_overflow_qzBCjqbmxCvGmpa:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_qzBCjqbmxCvGmpa:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_oktnjvqaCfulnAE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_oktnjvqaCfulnAE
+.L_16_blocks_overflow_oktnjvqaCfulnAE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_oktnjvqaCfulnAE:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	256(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	320(%rsp),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	384(%rsp),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	448(%rsp),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm17
+	vmovdqu8	320(%rcx,%rax,1),%zmm19
+	vmovdqu8	384(%rcx,%rax,1),%zmm20
+	vmovdqu8	448(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,256(%r10,%rax,1)
+	vmovdqu8	%zmm3,320(%r10,%rax,1)
+	vmovdqu8	%zmm4,384(%r10,%rax,1)
+	vmovdqu8	%zmm5,448(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,768(%rsp)
+	vmovdqa64	%zmm3,832(%rsp)
+	vmovdqa64	%zmm4,896(%rsp)
+	vmovdqa64	%zmm5,960(%rsp)
+	vmovdqa64	1280(%rsp),%zmm13
+	vmovdqu64	512(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1344(%rsp),%zmm13
+	vmovdqu64	576(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1408(%rsp),%zmm13
+	vmovdqu64	640(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1472(%rsp),%zmm13
+	vmovdqu64	704(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+	subq	$512,%r8
+	addq	$512,%rax
+	movl	%r8d,%r10d
+	andl	$~15,%r10d
+	movl	$512,%ebx
+	subl	%r10d,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_ywwtvbBdjBfpjwg
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_ywwtvbBdjBfpjwg
+	jb	.L_last_num_blocks_is_7_1_ywwtvbBdjBfpjwg
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_ywwtvbBdjBfpjwg
+	jb	.L_last_num_blocks_is_11_9_ywwtvbBdjBfpjwg
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_ywwtvbBdjBfpjwg
+	ja	.L_last_num_blocks_is_16_ywwtvbBdjBfpjwg
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_ywwtvbBdjBfpjwg
+	jmp	.L_last_num_blocks_is_13_ywwtvbBdjBfpjwg
+
+.L_last_num_blocks_is_11_9_ywwtvbBdjBfpjwg:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_ywwtvbBdjBfpjwg
+	ja	.L_last_num_blocks_is_11_ywwtvbBdjBfpjwg
+	jmp	.L_last_num_blocks_is_9_ywwtvbBdjBfpjwg
+
+.L_last_num_blocks_is_7_1_ywwtvbBdjBfpjwg:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_ywwtvbBdjBfpjwg
+	jb	.L_last_num_blocks_is_3_1_ywwtvbBdjBfpjwg
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_ywwtvbBdjBfpjwg
+	je	.L_last_num_blocks_is_6_ywwtvbBdjBfpjwg
+	jmp	.L_last_num_blocks_is_5_ywwtvbBdjBfpjwg
+
+.L_last_num_blocks_is_3_1_ywwtvbBdjBfpjwg:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_ywwtvbBdjBfpjwg
+	je	.L_last_num_blocks_is_2_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_1_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_lrvxvtmqCmharxz
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_lrvxvtmqCmharxz
+
+.L_16_blocks_overflow_lrvxvtmqCmharxz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_lrvxvtmqCmharxz:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%xmm29,%xmm17,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dmfubdglAvpEgon
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dmfubdglAvpEgon
+.L_small_initial_partial_block_dmfubdglAvpEgon:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_dmfubdglAvpEgon
+.L_small_initial_compute_done_dmfubdglAvpEgon:
+.L_after_reduction_dmfubdglAvpEgon:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_2_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_mfjvwyBfvAlhFAq
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_mfjvwyBfvAlhFAq
+
+.L_16_blocks_overflow_mfjvwyBfvAlhFAq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_mfjvwyBfvAlhFAq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%ymm29,%ymm17,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_DwCCnmjxkzxbxfm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_DwCCnmjxkzxbxfm
+.L_small_initial_partial_block_DwCCnmjxkzxbxfm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_DwCCnmjxkzxbxfm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_DwCCnmjxkzxbxfm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_DwCCnmjxkzxbxfm:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_3_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_GouDizmhFBrpefu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_GouDizmhFBrpefu
+
+.L_16_blocks_overflow_GouDizmhFBrpefu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_GouDizmhFBrpefu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_mlfwFqiohjvhBtu
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_mlfwFqiohjvhBtu
+.L_small_initial_partial_block_mlfwFqiohjvhBtu:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_mlfwFqiohjvhBtu:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_mlfwFqiohjvhBtu
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_mlfwFqiohjvhBtu:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_4_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_jAkGvvgxkvpcgup
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_jAkGvvgxkvpcgup
+
+.L_16_blocks_overflow_jAkGvvgxkvpcgup:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_jAkGvvgxkvpcgup:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_fohmdAohpntgnmE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_fohmdAohpntgnmE
+.L_small_initial_partial_block_fohmdAohpntgnmE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_fohmdAohpntgnmE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_fohmdAohpntgnmE
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_fohmdAohpntgnmE:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_5_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_rkgtkrCkBGEkqEw
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_rkgtkrCkBGEkqEw
+
+.L_16_blocks_overflow_rkgtkrCkBGEkqEw:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_rkgtkrCkBGEkqEw:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%xmm29,%xmm19,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GedzjlrtzhzBimx
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GedzjlrtzhzBimx
+.L_small_initial_partial_block_GedzjlrtzhzBimx:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GedzjlrtzhzBimx:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GedzjlrtzhzBimx
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GedzjlrtzhzBimx:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_6_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_iAoyrpkhggujgeb
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_iAoyrpkhggujgeb
+
+.L_16_blocks_overflow_iAoyrpkhggujgeb:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_iAoyrpkhggujgeb:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%ymm29,%ymm19,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_lhAawwwGAffrlBE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_lhAawwwGAffrlBE
+.L_small_initial_partial_block_lhAawwwGAffrlBE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_lhAawwwGAffrlBE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_lhAawwwGAffrlBE
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_lhAawwwGAffrlBE:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_7_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_EEjjmFGgkrdqljj
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_EEjjmFGgkrdqljj
+
+.L_16_blocks_overflow_EEjjmFGgkrdqljj:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_EEjjmFGgkrdqljj:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_zAkfDuunwikGuyy
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_zAkfDuunwikGuyy
+.L_small_initial_partial_block_zAkfDuunwikGuyy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zAkfDuunwikGuyy:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_zAkfDuunwikGuyy
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_zAkfDuunwikGuyy:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_8_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_cAkyeGuEhFpFoCw
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_cAkyeGuEhFpFoCw
+
+.L_16_blocks_overflow_cAkyeGuEhFpFoCw:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_cAkyeGuEhFpFoCw:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wAvntAmwtrbaldr
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wAvntAmwtrbaldr
+.L_small_initial_partial_block_wAvntAmwtrbaldr:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_wAvntAmwtrbaldr:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_wAvntAmwtrbaldr
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_wAvntAmwtrbaldr:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_9_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_xpkcooCxoepCFxx
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_xpkcooCxoepCFxx
+
+.L_16_blocks_overflow_xpkcooCxoepCFxx:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_xpkcooCxoepCFxx:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%xmm29,%xmm20,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_jxxciczEwxiljkG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_jxxciczEwxiljkG
+.L_small_initial_partial_block_jxxciczEwxiljkG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_jxxciczEwxiljkG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_jxxciczEwxiljkG
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_jxxciczEwxiljkG:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_10_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_wxwhpfnpapmwwcE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_wxwhpfnpapmwwcE
+
+.L_16_blocks_overflow_wxwhpfnpapmwwcE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_wxwhpfnpapmwwcE:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%ymm29,%ymm20,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_rughxEjslwCqGCn
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_rughxEjslwCqGCn
+.L_small_initial_partial_block_rughxEjslwCqGCn:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_rughxEjslwCqGCn:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_rughxEjslwCqGCn
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_rughxEjslwCqGCn:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_11_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_dnkEinnEyladlhw
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_dnkEinnEyladlhw
+
+.L_16_blocks_overflow_dnkEinnEyladlhw:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_dnkEinnEyladlhw:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_nedxDAisiabtszx
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_nedxDAisiabtszx
+.L_small_initial_partial_block_nedxDAisiabtszx:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_nedxDAisiabtszx:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_nedxDAisiabtszx
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_nedxDAisiabtszx:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_12_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_dwFjobqlCssllsu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_dwFjobqlCssllsu
+
+.L_16_blocks_overflow_dwFjobqlCssllsu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_dwFjobqlCssllsu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_smgvGggekDsrfaG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_smgvGggekDsrfaG
+.L_small_initial_partial_block_smgvGggekDsrfaG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_smgvGggekDsrfaG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_smgvGggekDsrfaG
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_smgvGggekDsrfaG:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_13_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_vuxzxuswtgoniDi
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_vuxzxuswtgoniDi
+
+.L_16_blocks_overflow_vuxzxuswtgoniDi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_vuxzxuswtgoniDi:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%xmm29,%xmm21,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_FkzlCDdflAcaobb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_FkzlCDdflAcaobb
+.L_small_initial_partial_block_FkzlCDdflAcaobb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_FkzlCDdflAcaobb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_FkzlCDdflAcaobb
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_FkzlCDdflAcaobb:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_14_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_cwlDeBjuyjDfrve
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_cwlDeBjuyjDfrve
+
+.L_16_blocks_overflow_cwlDeBjuyjDfrve:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_cwlDeBjuyjDfrve:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%ymm29,%ymm21,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AaeGgoBGpEkercd
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AaeGgoBGpEkercd
+.L_small_initial_partial_block_AaeGgoBGpEkercd:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AaeGgoBGpEkercd:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AaeGgoBGpEkercd
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_AaeGgoBGpEkercd:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_15_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_xvzouGhfrEuACjl
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_xvzouGhfrEuACjl
+
+.L_16_blocks_overflow_xvzouGhfrEuACjl:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_xvzouGhfrEuACjl:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_xjzEBsasCtushba
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_xjzEBsasCtushba
+.L_small_initial_partial_block_xjzEBsasCtushba:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_xjzEBsasCtushba:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_xjzEBsasCtushba
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_xjzEBsasCtushba:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_16_ywwtvbBdjBfpjwg:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_kGvhquqoaxfGFDu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_kGvhquqoaxfGFDu
+
+.L_16_blocks_overflow_kGvhquqoaxfGFDu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_kGvhquqoaxfGFDu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_bfrxexukqvCBqxe:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_bfrxexukqvCBqxe:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_bfrxexukqvCBqxe:
+	jmp	.L_last_blocks_done_ywwtvbBdjBfpjwg
+.L_last_num_blocks_is_0_ywwtvbBdjBfpjwg:
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_ywwtvbBdjBfpjwg:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_idpupdakyookwwt
+.L_encrypt_16_blocks_idpupdakyookwwt:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_qppFkgDawakFCzq
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_qppFkgDawakFCzq
+.L_16_blocks_overflow_qppFkgDawakFCzq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_qppFkgDawakFCzq:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	vmovdqa64	1024(%rsp),%zmm13
+	vmovdqu64	256(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1088(%rsp),%zmm13
+	vmovdqu64	320(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1152(%rsp),%zmm13
+	vmovdqu64	384(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1216(%rsp),%zmm13
+	vmovdqu64	448(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	subq	$256,%r8
+	addq	$256,%rax
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_sBgbysbwpsEAgwD
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_sBgbysbwpsEAgwD
+	jb	.L_last_num_blocks_is_7_1_sBgbysbwpsEAgwD
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_sBgbysbwpsEAgwD
+	jb	.L_last_num_blocks_is_11_9_sBgbysbwpsEAgwD
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_sBgbysbwpsEAgwD
+	ja	.L_last_num_blocks_is_16_sBgbysbwpsEAgwD
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_sBgbysbwpsEAgwD
+	jmp	.L_last_num_blocks_is_13_sBgbysbwpsEAgwD
+
+.L_last_num_blocks_is_11_9_sBgbysbwpsEAgwD:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_sBgbysbwpsEAgwD
+	ja	.L_last_num_blocks_is_11_sBgbysbwpsEAgwD
+	jmp	.L_last_num_blocks_is_9_sBgbysbwpsEAgwD
+
+.L_last_num_blocks_is_7_1_sBgbysbwpsEAgwD:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_sBgbysbwpsEAgwD
+	jb	.L_last_num_blocks_is_3_1_sBgbysbwpsEAgwD
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_sBgbysbwpsEAgwD
+	je	.L_last_num_blocks_is_6_sBgbysbwpsEAgwD
+	jmp	.L_last_num_blocks_is_5_sBgbysbwpsEAgwD
+
+.L_last_num_blocks_is_3_1_sBgbysbwpsEAgwD:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_sBgbysbwpsEAgwD
+	je	.L_last_num_blocks_is_2_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_1_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_vFbnqtabcGjjedf
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_vFbnqtabcGjjedf
+
+.L_16_blocks_overflow_vFbnqtabcGjjedf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_vFbnqtabcGjjedf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%xmm29,%xmm17,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wupwlmvtaqCbfrr
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wupwlmvtaqCbfrr
+.L_small_initial_partial_block_wupwlmvtaqCbfrr:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_wupwlmvtaqCbfrr
+.L_small_initial_compute_done_wupwlmvtaqCbfrr:
+.L_after_reduction_wupwlmvtaqCbfrr:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_2_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_scpCgDrBhjiFmvh
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_scpCgDrBhjiFmvh
+
+.L_16_blocks_overflow_scpCgDrBhjiFmvh:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_scpCgDrBhjiFmvh:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%ymm29,%ymm17,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_jvzEraEyBECivGc
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_jvzEraEyBECivGc
+.L_small_initial_partial_block_jvzEraEyBECivGc:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_jvzEraEyBECivGc:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_jvzEraEyBECivGc
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_jvzEraEyBECivGc:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_3_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_gAnjqAubbjECrqs
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_gAnjqAubbjECrqs
+
+.L_16_blocks_overflow_gAnjqAubbjECrqs:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_gAnjqAubbjECrqs:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vlbiCGmqaidAaxq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vlbiCGmqaidAaxq
+.L_small_initial_partial_block_vlbiCGmqaidAaxq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vlbiCGmqaidAaxq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vlbiCGmqaidAaxq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_vlbiCGmqaidAaxq:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_4_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_flwdDqfljwpFEai
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_flwdDqfljwpFEai
+
+.L_16_blocks_overflow_flwdDqfljwpFEai:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_flwdDqfljwpFEai:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_rueraltEEekxvub
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_rueraltEEekxvub
+.L_small_initial_partial_block_rueraltEEekxvub:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_rueraltEEekxvub:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_rueraltEEekxvub
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_rueraltEEekxvub:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_5_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_oGogavcywypmFzD
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_oGogavcywypmFzD
+
+.L_16_blocks_overflow_oGogavcywypmFzD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_oGogavcywypmFzD:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%xmm29,%xmm19,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_bsebegEeAExsEbo
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_bsebegEeAExsEbo
+.L_small_initial_partial_block_bsebegEeAExsEbo:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_bsebegEeAExsEbo:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_bsebegEeAExsEbo
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_bsebegEeAExsEbo:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_6_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_iydnjvpphBljtys
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_iydnjvpphBljtys
+
+.L_16_blocks_overflow_iydnjvpphBljtys:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_iydnjvpphBljtys:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%ymm29,%ymm19,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vytmAxGjuebECCw
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vytmAxGjuebECCw
+.L_small_initial_partial_block_vytmAxGjuebECCw:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vytmAxGjuebECCw:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vytmAxGjuebECCw
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_vytmAxGjuebECCw:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_7_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_lmoofeAtokhzBbE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_lmoofeAtokhzBbE
+
+.L_16_blocks_overflow_lmoofeAtokhzBbE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_lmoofeAtokhzBbE:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_doFpjwwofenrvFj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_doFpjwwofenrvFj
+.L_small_initial_partial_block_doFpjwwofenrvFj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_doFpjwwofenrvFj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_doFpjwwofenrvFj
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_doFpjwwofenrvFj:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_8_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_fEqhkyetElBhGlw
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_fEqhkyetElBhGlw
+
+.L_16_blocks_overflow_fEqhkyetElBhGlw:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_fEqhkyetElBhGlw:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tmjBzoqfCyaotyq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tmjBzoqfCyaotyq
+.L_small_initial_partial_block_tmjBzoqfCyaotyq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tmjBzoqfCyaotyq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tmjBzoqfCyaotyq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tmjBzoqfCyaotyq:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_9_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_CovCjjmvkeyfppn
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_CovCjjmvkeyfppn
+
+.L_16_blocks_overflow_CovCjjmvkeyfppn:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_CovCjjmvkeyfppn:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%xmm29,%xmm20,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tlnCEvvcfilgvcg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tlnCEvvcfilgvcg
+.L_small_initial_partial_block_tlnCEvvcfilgvcg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tlnCEvvcfilgvcg:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tlnCEvvcfilgvcg
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tlnCEvvcfilgvcg:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_10_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_FusGotahlGsdgFq
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_FusGotahlGsdgFq
+
+.L_16_blocks_overflow_FusGotahlGsdgFq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_FusGotahlGsdgFq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%ymm29,%ymm20,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CplDdyEaDFvnckp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CplDdyEaDFvnckp
+.L_small_initial_partial_block_CplDdyEaDFvnckp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CplDdyEaDFvnckp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CplDdyEaDFvnckp
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CplDdyEaDFvnckp:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_11_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_wEFnxnEmBknfcdg
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_wEFnxnEmBknfcdg
+
+.L_16_blocks_overflow_wEFnxnEmBknfcdg:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_wEFnxnEmBknfcdg:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_lkyDvslbnuutFCz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_lkyDvslbnuutFCz
+.L_small_initial_partial_block_lkyDvslbnuutFCz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_lkyDvslbnuutFCz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_lkyDvslbnuutFCz
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_lkyDvslbnuutFCz:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_12_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_aCkjsECuAFkBwrm
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_aCkjsECuAFkBwrm
+
+.L_16_blocks_overflow_aCkjsECuAFkBwrm:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_aCkjsECuAFkBwrm:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_msyjgzkDtxuudjz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_msyjgzkDtxuudjz
+.L_small_initial_partial_block_msyjgzkDtxuudjz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_msyjgzkDtxuudjz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_msyjgzkDtxuudjz
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_msyjgzkDtxuudjz:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_13_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_rwmwcFzidwvBCcu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_rwmwcFzidwvBCcu
+
+.L_16_blocks_overflow_rwmwcFzidwvBCcu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_rwmwcFzidwvBCcu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%xmm29,%xmm21,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vigvsBCChBurqwE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vigvsBCChBurqwE
+.L_small_initial_partial_block_vigvsBCChBurqwE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vigvsBCChBurqwE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vigvsBCChBurqwE
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_vigvsBCChBurqwE:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_14_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_zAvjewolkqiqkvl
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_zAvjewolkqiqkvl
+
+.L_16_blocks_overflow_zAvjewolkqiqkvl:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_zAvjewolkqiqkvl:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%ymm29,%ymm21,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_oFoExEDxgFafAck
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_oFoExEDxgFafAck
+.L_small_initial_partial_block_oFoExEDxgFafAck:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_oFoExEDxgFafAck:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_oFoExEDxgFafAck
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_oFoExEDxgFafAck:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_15_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_mwFghwnDulyultA
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_mwFghwnDulyultA
+
+.L_16_blocks_overflow_mwFghwnDulyultA:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_mwFghwnDulyultA:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vvapjkubBibxkfd
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vvapjkubBibxkfd
+.L_small_initial_partial_block_vvapjkubBibxkfd:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vvapjkubBibxkfd:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vvapjkubBibxkfd
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_vvapjkubBibxkfd:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_16_sBgbysbwpsEAgwD:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_qzDipGmimEAmnxy
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_qzDipGmimEAmnxy
+
+.L_16_blocks_overflow_qzDipGmimEAmnxy:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_qzDipGmimEAmnxy:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_zxdinlBggFvaDwj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zxdinlBggFvaDwj:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_zxdinlBggFvaDwj:
+	jmp	.L_last_blocks_done_sBgbysbwpsEAgwD
+.L_last_num_blocks_is_0_sBgbysbwpsEAgwD:
+	vmovdqa64	1280(%rsp),%zmm13
+	vmovdqu64	512(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1344(%rsp),%zmm13
+	vmovdqu64	576(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1408(%rsp),%zmm13
+	vmovdqu64	640(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1472(%rsp),%zmm13
+	vmovdqu64	704(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_sBgbysbwpsEAgwD:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_idpupdakyookwwt
+
+.L_message_below_32_blocks_idpupdakyookwwt:
+
+
+	subq	$256,%r8
+	addq	$256,%rax
+	movl	%r8d,%r10d
+	leaq	96(%rsi),%r12
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_oGfgFplzrBBfloo
+	vmovdqu64	640(%rsp),%zmm3
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	576(%rsp),%zmm4
+	vmovdqu64	512(%rsp),%zmm5
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,256(%rsp)
+.L_skip_hkeys_precomputation_oGfgFplzrBBfloo:
+	movq	$1,%r14
+	andl	$~15,%r10d
+	movl	$512,%ebx
+	subl	%r10d,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_rtDpFbEmzDgpqhp
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_rtDpFbEmzDgpqhp
+	jb	.L_last_num_blocks_is_7_1_rtDpFbEmzDgpqhp
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_rtDpFbEmzDgpqhp
+	jb	.L_last_num_blocks_is_11_9_rtDpFbEmzDgpqhp
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_rtDpFbEmzDgpqhp
+	ja	.L_last_num_blocks_is_16_rtDpFbEmzDgpqhp
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_rtDpFbEmzDgpqhp
+	jmp	.L_last_num_blocks_is_13_rtDpFbEmzDgpqhp
+
+.L_last_num_blocks_is_11_9_rtDpFbEmzDgpqhp:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_rtDpFbEmzDgpqhp
+	ja	.L_last_num_blocks_is_11_rtDpFbEmzDgpqhp
+	jmp	.L_last_num_blocks_is_9_rtDpFbEmzDgpqhp
+
+.L_last_num_blocks_is_7_1_rtDpFbEmzDgpqhp:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_rtDpFbEmzDgpqhp
+	jb	.L_last_num_blocks_is_3_1_rtDpFbEmzDgpqhp
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_rtDpFbEmzDgpqhp
+	je	.L_last_num_blocks_is_6_rtDpFbEmzDgpqhp
+	jmp	.L_last_num_blocks_is_5_rtDpFbEmzDgpqhp
+
+.L_last_num_blocks_is_3_1_rtDpFbEmzDgpqhp:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_rtDpFbEmzDgpqhp
+	je	.L_last_num_blocks_is_2_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_1_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_xEFCfkFAlplGCaE
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_xEFCfkFAlplGCaE
+
+.L_16_blocks_overflow_xEFCfkFAlplGCaE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_xEFCfkFAlplGCaE:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%xmm29,%xmm17,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wFgawhiAcpmppAB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wFgawhiAcpmppAB
+.L_small_initial_partial_block_wFgawhiAcpmppAB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_wFgawhiAcpmppAB
+.L_small_initial_compute_done_wFgawhiAcpmppAB:
+.L_after_reduction_wFgawhiAcpmppAB:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_2_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_awyvrtDaxdsczdt
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_awyvrtDaxdsczdt
+
+.L_16_blocks_overflow_awyvrtDaxdsczdt:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_awyvrtDaxdsczdt:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%ymm29,%ymm17,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_zjzgflsGDcssdtD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_zjzgflsGDcssdtD
+.L_small_initial_partial_block_zjzgflsGDcssdtD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zjzgflsGDcssdtD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_zjzgflsGDcssdtD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_zjzgflsGDcssdtD:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_3_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_yqaizoapdxcmvBu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_yqaizoapdxcmvBu
+
+.L_16_blocks_overflow_yqaizoapdxcmvBu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_yqaizoapdxcmvBu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_qvDoFCkijahEggC
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_qvDoFCkijahEggC
+.L_small_initial_partial_block_qvDoFCkijahEggC:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_qvDoFCkijahEggC:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_qvDoFCkijahEggC
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_qvDoFCkijahEggC:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_4_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_diyqAwgopfjycyk
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_diyqAwgopfjycyk
+
+.L_16_blocks_overflow_diyqAwgopfjycyk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_diyqAwgopfjycyk:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GhgdwhBktcciEvw
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GhgdwhBktcciEvw
+.L_small_initial_partial_block_GhgdwhBktcciEvw:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GhgdwhBktcciEvw:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GhgdwhBktcciEvw
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GhgdwhBktcciEvw:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_5_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_ceFyorcstgzpukl
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_ceFyorcstgzpukl
+
+.L_16_blocks_overflow_ceFyorcstgzpukl:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_ceFyorcstgzpukl:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%xmm29,%xmm19,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_npjzDAkdAcozgAr
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_npjzDAkdAcozgAr
+.L_small_initial_partial_block_npjzDAkdAcozgAr:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_npjzDAkdAcozgAr:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_npjzDAkdAcozgAr
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_npjzDAkdAcozgAr:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_6_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_wkDeCAmGftsebsG
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_wkDeCAmGftsebsG
+
+.L_16_blocks_overflow_wkDeCAmGftsebsG:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_wkDeCAmGftsebsG:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%ymm29,%ymm19,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_qyAndunnnDhgFib
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_qyAndunnnDhgFib
+.L_small_initial_partial_block_qyAndunnnDhgFib:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_qyAndunnnDhgFib:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_qyAndunnnDhgFib
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_qyAndunnnDhgFib:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_7_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_utnggztbcBxCdwD
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_utnggztbcBxCdwD
+
+.L_16_blocks_overflow_utnggztbcBxCdwD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_utnggztbcBxCdwD:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_kuxjwcanuxGekCB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_kuxjwcanuxGekCB
+.L_small_initial_partial_block_kuxjwcanuxGekCB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_kuxjwcanuxGekCB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_kuxjwcanuxGekCB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_kuxjwcanuxGekCB:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_8_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_hBoifyBjFsabovg
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_hBoifyBjFsabovg
+
+.L_16_blocks_overflow_hBoifyBjFsabovg:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_hBoifyBjFsabovg:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BikdrplacdwuDsA
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BikdrplacdwuDsA
+.L_small_initial_partial_block_BikdrplacdwuDsA:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BikdrplacdwuDsA:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BikdrplacdwuDsA
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_BikdrplacdwuDsA:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_9_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_EDqyvpbtFbDArBe
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_EDqyvpbtFbDArBe
+
+.L_16_blocks_overflow_EDqyvpbtFbDArBe:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_EDqyvpbtFbDArBe:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%xmm29,%xmm20,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vCmansruaqysqsp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vCmansruaqysqsp
+.L_small_initial_partial_block_vCmansruaqysqsp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vCmansruaqysqsp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vCmansruaqysqsp
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_vCmansruaqysqsp:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_10_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_vbtuBwixavFFEmd
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_vbtuBwixavFFEmd
+
+.L_16_blocks_overflow_vbtuBwixavFFEmd:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_vbtuBwixavFFEmd:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%ymm29,%ymm20,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_nAzgfknpyCiqqpa
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_nAzgfknpyCiqqpa
+.L_small_initial_partial_block_nAzgfknpyCiqqpa:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_nAzgfknpyCiqqpa:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_nAzgfknpyCiqqpa
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_nAzgfknpyCiqqpa:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_11_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_nGtkebdllkzxCEq
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_nGtkebdllkzxCEq
+
+.L_16_blocks_overflow_nGtkebdllkzxCEq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_nGtkebdllkzxCEq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_iBpAgdoviwlgkrw
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_iBpAgdoviwlgkrw
+.L_small_initial_partial_block_iBpAgdoviwlgkrw:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_iBpAgdoviwlgkrw:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_iBpAgdoviwlgkrw
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_iBpAgdoviwlgkrw:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_12_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_umbbxfuDcbejFiy
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_umbbxfuDcbejFiy
+
+.L_16_blocks_overflow_umbbxfuDcbejFiy:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_umbbxfuDcbejFiy:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_amwhpqfffwfCkxE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_amwhpqfffwfCkxE
+.L_small_initial_partial_block_amwhpqfffwfCkxE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_amwhpqfffwfCkxE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_amwhpqfffwfCkxE
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_amwhpqfffwfCkxE:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_13_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_ssqadCngEEvEvua
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_ssqadCngEEvEvua
+
+.L_16_blocks_overflow_ssqadCngEEvEvua:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_ssqadCngEEvEvua:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%xmm29,%xmm21,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_uhupBdmxAAhuiob
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_uhupBdmxAAhuiob
+.L_small_initial_partial_block_uhupBdmxAAhuiob:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_uhupBdmxAAhuiob:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_uhupBdmxAAhuiob
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_uhupBdmxAAhuiob:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_14_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_oyyngstyhDoooyf
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_oyyngstyhDoooyf
+
+.L_16_blocks_overflow_oyyngstyhDoooyf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_oyyngstyhDoooyf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%ymm29,%ymm21,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_msrpoeGcisEwzum
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_msrpoeGcisEwzum
+.L_small_initial_partial_block_msrpoeGcisEwzum:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_msrpoeGcisEwzum:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_msrpoeGcisEwzum
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_msrpoeGcisEwzum:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_15_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_wqghivyyxCwuhea
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_wqghivyyxCwuhea
+
+.L_16_blocks_overflow_wqghivyyxCwuhea:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_wqghivyyxCwuhea:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_eadkDfinnyrkEBl
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_eadkDfinnyrkEBl
+.L_small_initial_partial_block_eadkDfinnyrkEBl:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_eadkDfinnyrkEBl:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_eadkDfinnyrkEBl
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_eadkDfinnyrkEBl:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_16_rtDpFbEmzDgpqhp:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_rknjAkElxeoChdb
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_rknjAkElxeoChdb
+
+.L_16_blocks_overflow_rknjAkElxeoChdb:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_rknjAkElxeoChdb:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_hqomCqGwtrzBxtg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_hqomCqGwtrzBxtg:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_hqomCqGwtrzBxtg:
+	jmp	.L_last_blocks_done_rtDpFbEmzDgpqhp
+.L_last_num_blocks_is_0_rtDpFbEmzDgpqhp:
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_rtDpFbEmzDgpqhp:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_idpupdakyookwwt
+
+.L_message_below_equal_16_blocks_idpupdakyookwwt:
+
+
+	movl	%r8d,%r12d
+	addl	$15,%r12d
+	shrl	$4,%r12d
+	cmpq	$8,%r12
+	je	.L_small_initial_num_blocks_is_8_rjxEBywrzojzlpz
+	jl	.L_small_initial_num_blocks_is_7_1_rjxEBywrzojzlpz
+
+
+	cmpq	$12,%r12
+	je	.L_small_initial_num_blocks_is_12_rjxEBywrzojzlpz
+	jl	.L_small_initial_num_blocks_is_11_9_rjxEBywrzojzlpz
+
+
+	cmpq	$16,%r12
+	je	.L_small_initial_num_blocks_is_16_rjxEBywrzojzlpz
+	cmpq	$15,%r12
+	je	.L_small_initial_num_blocks_is_15_rjxEBywrzojzlpz
+	cmpq	$14,%r12
+	je	.L_small_initial_num_blocks_is_14_rjxEBywrzojzlpz
+	jmp	.L_small_initial_num_blocks_is_13_rjxEBywrzojzlpz
+
+.L_small_initial_num_blocks_is_11_9_rjxEBywrzojzlpz:
+
+	cmpq	$11,%r12
+	je	.L_small_initial_num_blocks_is_11_rjxEBywrzojzlpz
+	cmpq	$10,%r12
+	je	.L_small_initial_num_blocks_is_10_rjxEBywrzojzlpz
+	jmp	.L_small_initial_num_blocks_is_9_rjxEBywrzojzlpz
+
+.L_small_initial_num_blocks_is_7_1_rjxEBywrzojzlpz:
+	cmpq	$4,%r12
+	je	.L_small_initial_num_blocks_is_4_rjxEBywrzojzlpz
+	jl	.L_small_initial_num_blocks_is_3_1_rjxEBywrzojzlpz
+
+	cmpq	$7,%r12
+	je	.L_small_initial_num_blocks_is_7_rjxEBywrzojzlpz
+	cmpq	$6,%r12
+	je	.L_small_initial_num_blocks_is_6_rjxEBywrzojzlpz
+	jmp	.L_small_initial_num_blocks_is_5_rjxEBywrzojzlpz
+
+.L_small_initial_num_blocks_is_3_1_rjxEBywrzojzlpz:
+
+	cmpq	$3,%r12
+	je	.L_small_initial_num_blocks_is_3_rjxEBywrzojzlpz
+	cmpq	$2,%r12
+	je	.L_small_initial_num_blocks_is_2_rjxEBywrzojzlpz
+
+
+
+
+
+.L_small_initial_num_blocks_is_1_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%xmm29
+	vpaddd	ONE(%rip),%xmm2,%xmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vpshufb	%xmm29,%xmm0,%xmm0
+	vmovdqu8	0(%rcx,%rax,1),%xmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%xmm15,%xmm0,%xmm0
+	vpxorq	%xmm6,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm6,%xmm6
+	vextracti32x4	$0,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wmvtEmByFpndqDd
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wmvtEmByFpndqDd
+.L_small_initial_partial_block_wmvtEmByFpndqDd:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm13,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_wmvtEmByFpndqDd
+.L_small_initial_compute_done_wmvtEmByFpndqDd:
+.L_after_reduction_wmvtEmByFpndqDd:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_2_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%ymm29
+	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
+	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vpshufb	%ymm29,%ymm0,%ymm0
+	vmovdqu8	0(%rcx,%rax,1),%ymm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%ymm15,%ymm0,%ymm0
+	vpxorq	%ymm6,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm6,%ymm6
+	vextracti32x4	$1,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_obDymcdjrwxmrzh
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_obDymcdjrwxmrzh
+.L_small_initial_partial_block_obDymcdjrwxmrzh:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_obDymcdjrwxmrzh:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_obDymcdjrwxmrzh
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_obDymcdjrwxmrzh:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_3_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vextracti32x4	$2,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wsjyujBGqywymdn
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wsjyujBGqywymdn
+.L_small_initial_partial_block_wsjyujBGqywymdn:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_wsjyujBGqywymdn:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_wsjyujBGqywymdn
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_wsjyujBGqywymdn:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_4_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vextracti32x4	$3,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AGFfueziBhsgFCb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AGFfueziBhsgFCb
+.L_small_initial_partial_block_AGFfueziBhsgFCb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AGFfueziBhsgFCb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AGFfueziBhsgFCb
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_AGFfueziBhsgFCb:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_5_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%xmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%xmm15,%xmm3,%xmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%xmm7,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%xmm29,%xmm7,%xmm7
+	vextracti32x4	$0,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_uEupdBnxqokfCfj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_uEupdBnxqokfCfj
+.L_small_initial_partial_block_uEupdBnxqokfCfj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_uEupdBnxqokfCfj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_uEupdBnxqokfCfj
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_uEupdBnxqokfCfj:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_6_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%ymm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%ymm15,%ymm3,%ymm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%ymm7,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%ymm29,%ymm7,%ymm7
+	vextracti32x4	$1,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_cfesukddCblyxbp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_cfesukddCblyxbp
+.L_small_initial_partial_block_cfesukddCblyxbp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_cfesukddCblyxbp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_cfesukddCblyxbp
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_cfesukddCblyxbp:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_7_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vextracti32x4	$2,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_rgvBjvADnCnpFiv
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_rgvBjvADnCnpFiv
+.L_small_initial_partial_block_rgvBjvADnCnpFiv:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_rgvBjvADnCnpFiv:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_rgvBjvADnCnpFiv
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_rgvBjvADnCnpFiv:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_8_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vextracti32x4	$3,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_EdmpfogzCGbmejp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_EdmpfogzCGbmejp
+.L_small_initial_partial_block_EdmpfogzCGbmejp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_EdmpfogzCGbmejp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_EdmpfogzCGbmejp
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_EdmpfogzCGbmejp:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_9_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%xmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%xmm15,%xmm4,%xmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%xmm10,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%xmm29,%xmm10,%xmm10
+	vextracti32x4	$0,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_fnBhhFjADgjlqDb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_fnBhhFjADgjlqDb
+.L_small_initial_partial_block_fnBhhFjADgjlqDb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_fnBhhFjADgjlqDb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_fnBhhFjADgjlqDb
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_fnBhhFjADgjlqDb:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_10_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%ymm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%ymm15,%ymm4,%ymm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%ymm10,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%ymm29,%ymm10,%ymm10
+	vextracti32x4	$1,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_bjBauxtbqBhvlkx
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_bjBauxtbqBhvlkx
+.L_small_initial_partial_block_bjBauxtbqBhvlkx:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_bjBauxtbqBhvlkx:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_bjBauxtbqBhvlkx
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_bjBauxtbqBhvlkx:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_11_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vextracti32x4	$2,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ftrnffzivrfiqdg
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ftrnffzivrfiqdg
+.L_small_initial_partial_block_ftrnffzivrfiqdg:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ftrnffzivrfiqdg:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ftrnffzivrfiqdg
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_ftrnffzivrfiqdg:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_12_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vextracti32x4	$3,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_gwBnuickemriobp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	64(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_gwBnuickemriobp
+.L_small_initial_partial_block_gwBnuickemriobp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_gwBnuickemriobp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_gwBnuickemriobp
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_gwBnuickemriobp:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_13_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%xmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%xmm15,%xmm5,%xmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%xmm11,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%xmm29,%xmm11,%xmm11
+	vextracti32x4	$0,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dApuzBzBCAhnptb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	48(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dApuzBzBCAhnptb
+.L_small_initial_partial_block_dApuzBzBCAhnptb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	64(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_dApuzBzBCAhnptb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_dApuzBzBCAhnptb
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_dApuzBzBCAhnptb:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_14_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%ymm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%ymm15,%ymm5,%ymm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%ymm11,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%ymm29,%ymm11,%ymm11
+	vextracti32x4	$1,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_cwkcpswGgECCvzG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	32(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_cwkcpswGgECCvzG
+.L_small_initial_partial_block_cwkcpswGgECCvzG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	48(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_cwkcpswGgECCvzG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_cwkcpswGgECCvzG
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_cwkcpswGgECCvzG:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_15_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%zmm15,%zmm5,%zmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%zmm11,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vextracti32x4	$2,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_Gskglbrbjkgckhy
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	16(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_Gskglbrbjkgckhy
+.L_small_initial_partial_block_Gskglbrbjkgckhy:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	32(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_Gskglbrbjkgckhy:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_Gskglbrbjkgckhy
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_Gskglbrbjkgckhy:
+	jmp	.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz
+.L_small_initial_num_blocks_is_16_rjxEBywrzojzlpz:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%zmm15,%zmm5,%zmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%zmm11,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vextracti32x4	$3,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_gpAaljbCtxlAhmD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	16(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_gpAaljbCtxlAhmD:
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_gpAaljbCtxlAhmD:
+.L_small_initial_blocks_encrypted_rjxEBywrzojzlpz:
+.L_ghash_done_idpupdakyookwwt:
+	vmovdqu64	%xmm2,0(%rsi)
+.L_enc_dec_done_idpupdakyookwwt:
+
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+	vmovdqu64	%xmm14,64(%rsi)
+.L_enc_dec_abort_idpupdakyookwwt:
+	jmp	.Lexit_gcm_decrypt
+.align	32
+.Laes_gcm_decrypt_256_avx512:
+	orq	%r8,%r8
+	je	.L_enc_dec_abort_jitrpnrpcoxxhbG
+	xorq	%r14,%r14
+	vmovdqu64	64(%rsi),%xmm14
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+
+	movl	(%rdx),%eax
+	orq	%rax,%rax
+	je	.L_partial_block_done_AGfDiqDahkkCvoz
+	movl	$16,%r10d
+	leaq	byte_len_to_mask_table(%rip),%r12
+	cmpq	%r10,%r8
+	cmovcq	%r8,%r10
+	kmovw	(%r12,%r10,2),%k1
+	vmovdqu8	(%rcx),%xmm0{%k1}{z}
+
+	vmovdqu64	16(%rsi),%xmm3
+
+	leaq	96(%rsi),%r10
+	vmovdqu64	240(%r10),%xmm4
+
+
+
+	leaq	SHIFT_MASK(%rip),%r12
+	addq	%rax,%r12
+	vmovdqu64	(%r12),%xmm5
+	vpshufb	%xmm5,%xmm3,%xmm3
+
+	vmovdqa64	%xmm0,%xmm6
+	vpxorq	%xmm0,%xmm3,%xmm3
+
+
+	leaq	(%r8,%rax,1),%r13
+	subq	$16,%r13
+	jge	.L_no_extra_mask_AGfDiqDahkkCvoz
+	subq	%r13,%r12
+.L_no_extra_mask_AGfDiqDahkkCvoz:
+
+
+
+	vmovdqu64	16(%r12),%xmm0
+	vpand	%xmm0,%xmm3,%xmm3
+	vpand	%xmm0,%xmm6,%xmm6
+	vpshufb	SHUF_MASK(%rip),%xmm6,%xmm6
+	vpshufb	%xmm5,%xmm6,%xmm6
+	vpxorq	%xmm6,%xmm14,%xmm14
+	cmpq	$0,%r13
+	jl	.L_partial_incomplete_AGfDiqDahkkCvoz
+
+	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
+	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
+	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
+	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
+	vpxorq	%xmm11,%xmm14,%xmm14
+
+	vpsrldq	$8,%xmm14,%xmm11
+	vpslldq	$8,%xmm14,%xmm14
+	vpxorq	%xmm11,%xmm7,%xmm7
+	vpxorq	%xmm10,%xmm14,%xmm14
+
+
+
+	vmovdqu64	POLY2(%rip),%xmm11
+
+	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
+	vpslldq	$8,%xmm10,%xmm10
+	vpxorq	%xmm10,%xmm14,%xmm14
+
+
+
+	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
+	vpsrldq	$4,%xmm10,%xmm10
+	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+
+	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14
+
+	movl	$0,(%rdx)
+
+	movq	%rax,%r12
+	movq	$16,%rax
+	subq	%r12,%rax
+	jmp	.L_enc_dec_done_AGfDiqDahkkCvoz
+
+.L_partial_incomplete_AGfDiqDahkkCvoz:
+	addl	%r8d,(%rdx)
+	movq	%r8,%rax
+
+.L_enc_dec_done_AGfDiqDahkkCvoz:
+
+
+	leaq	byte_len_to_mask_table(%rip),%r12
+	kmovw	(%r12,%rax,2),%k1
+	movq	%r9,%r12
+	vmovdqu8	%xmm3,(%r12){%k1}
+.L_partial_block_done_AGfDiqDahkkCvoz:
+	vmovdqu64	0(%rsi),%xmm2
+	subq	%rax,%r8
+	je	.L_enc_dec_done_jitrpnrpcoxxhbG
+	cmpq	$256,%r8
+	jbe	.L_message_below_equal_16_blocks_jitrpnrpcoxxhbG
+
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
+	vmovdqa64	ddq_addbe_1234(%rip),%zmm28
+
+
+
+
+
+
+	vmovd	%xmm2,%r15d
+	andl	$255,%r15d
+
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpshufb	%zmm29,%zmm2,%zmm2
+
+
+
+	cmpb	$240,%r15b
+	jae	.L_next_16_overflow_prkuhlAmenzwfha
+	vpaddd	%zmm28,%zmm2,%zmm7
+	vpaddd	%zmm27,%zmm7,%zmm10
+	vpaddd	%zmm27,%zmm10,%zmm11
+	vpaddd	%zmm27,%zmm11,%zmm12
+	jmp	.L_next_16_ok_prkuhlAmenzwfha
+.L_next_16_overflow_prkuhlAmenzwfha:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm12
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
+	vpaddd	%zmm12,%zmm7,%zmm10
+	vpaddd	%zmm12,%zmm10,%zmm11
+	vpaddd	%zmm12,%zmm11,%zmm12
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+.L_next_16_ok_prkuhlAmenzwfha:
+	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
+	addb	$16,%r15b
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm0
+	vmovdqu8	64(%rcx,%rax,1),%zmm3
+	vmovdqu8	128(%rcx,%rax,1),%zmm4
+	vmovdqu8	192(%rcx,%rax,1),%zmm5
+
+
+	vbroadcastf64x2	0(%rdi),%zmm6
+	vpxorq	%zmm6,%zmm7,%zmm7
+	vpxorq	%zmm6,%zmm10,%zmm10
+	vpxorq	%zmm6,%zmm11,%zmm11
+	vpxorq	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	16(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	32(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	48(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	64(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	80(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	96(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	112(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	128(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	144(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	160(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	176(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	192(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	208(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	224(%rdi),%zmm6
+	vaesenclast	%zmm6,%zmm7,%zmm7
+	vaesenclast	%zmm6,%zmm10,%zmm10
+	vaesenclast	%zmm6,%zmm11,%zmm11
+	vaesenclast	%zmm6,%zmm12,%zmm12
+
+
+	vpxorq	%zmm0,%zmm7,%zmm7
+	vpxorq	%zmm3,%zmm10,%zmm10
+	vpxorq	%zmm4,%zmm11,%zmm11
+	vpxorq	%zmm5,%zmm12,%zmm12
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm7,0(%r10,%rax,1)
+	vmovdqu8	%zmm10,64(%r10,%rax,1)
+	vmovdqu8	%zmm11,128(%r10,%rax,1)
+	vmovdqu8	%zmm12,192(%r10,%rax,1)
+
+	vpshufb	%zmm29,%zmm0,%zmm7
+	vpshufb	%zmm29,%zmm3,%zmm10
+	vpshufb	%zmm29,%zmm4,%zmm11
+	vpshufb	%zmm29,%zmm5,%zmm12
+	vmovdqa64	%zmm7,768(%rsp)
+	vmovdqa64	%zmm10,832(%rsp)
+	vmovdqa64	%zmm11,896(%rsp)
+	vmovdqa64	%zmm12,960(%rsp)
+	leaq	96(%rsi),%r12
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_zxfzqwcyBhBhFcs
+
+	vmovdqu64	192(%r12),%zmm0
+	vmovdqu64	%zmm0,704(%rsp)
+
+	vmovdqu64	128(%r12),%zmm3
+	vmovdqu64	%zmm3,640(%rsp)
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	64(%r12),%zmm4
+	vmovdqu64	%zmm4,576(%rsp)
+
+	vmovdqu64	0(%r12),%zmm5
+	vmovdqu64	%zmm5,512(%rsp)
+.L_skip_hkeys_precomputation_zxfzqwcyBhBhFcs:
+	cmpq	$512,%r8
+	jb	.L_message_below_32_blocks_jitrpnrpcoxxhbG
+
+
+
+	cmpb	$240,%r15b
+	jae	.L_next_16_overflow_iDzCewyDlaDyavk
+	vpaddd	%zmm28,%zmm2,%zmm7
+	vpaddd	%zmm27,%zmm7,%zmm10
+	vpaddd	%zmm27,%zmm10,%zmm11
+	vpaddd	%zmm27,%zmm11,%zmm12
+	jmp	.L_next_16_ok_iDzCewyDlaDyavk
+.L_next_16_overflow_iDzCewyDlaDyavk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm12
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
+	vpaddd	%zmm12,%zmm7,%zmm10
+	vpaddd	%zmm12,%zmm10,%zmm11
+	vpaddd	%zmm12,%zmm11,%zmm12
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vpshufb	%zmm29,%zmm12,%zmm12
+.L_next_16_ok_iDzCewyDlaDyavk:
+	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
+	addb	$16,%r15b
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm0
+	vmovdqu8	320(%rcx,%rax,1),%zmm3
+	vmovdqu8	384(%rcx,%rax,1),%zmm4
+	vmovdqu8	448(%rcx,%rax,1),%zmm5
+
+
+	vbroadcastf64x2	0(%rdi),%zmm6
+	vpxorq	%zmm6,%zmm7,%zmm7
+	vpxorq	%zmm6,%zmm10,%zmm10
+	vpxorq	%zmm6,%zmm11,%zmm11
+	vpxorq	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	16(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	32(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	48(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	64(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	80(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	96(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	112(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	128(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	144(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	160(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	176(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	192(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	208(%rdi),%zmm6
+	vaesenc	%zmm6,%zmm7,%zmm7
+	vaesenc	%zmm6,%zmm10,%zmm10
+	vaesenc	%zmm6,%zmm11,%zmm11
+	vaesenc	%zmm6,%zmm12,%zmm12
+	vbroadcastf64x2	224(%rdi),%zmm6
+	vaesenclast	%zmm6,%zmm7,%zmm7
+	vaesenclast	%zmm6,%zmm10,%zmm10
+	vaesenclast	%zmm6,%zmm11,%zmm11
+	vaesenclast	%zmm6,%zmm12,%zmm12
+
+
+	vpxorq	%zmm0,%zmm7,%zmm7
+	vpxorq	%zmm3,%zmm10,%zmm10
+	vpxorq	%zmm4,%zmm11,%zmm11
+	vpxorq	%zmm5,%zmm12,%zmm12
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm7,256(%r10,%rax,1)
+	vmovdqu8	%zmm10,320(%r10,%rax,1)
+	vmovdqu8	%zmm11,384(%r10,%rax,1)
+	vmovdqu8	%zmm12,448(%r10,%rax,1)
+
+	vpshufb	%zmm29,%zmm0,%zmm7
+	vpshufb	%zmm29,%zmm3,%zmm10
+	vpshufb	%zmm29,%zmm4,%zmm11
+	vpshufb	%zmm29,%zmm5,%zmm12
+	vmovdqa64	%zmm7,1024(%rsp)
+	vmovdqa64	%zmm10,1088(%rsp)
+	vmovdqa64	%zmm11,1152(%rsp)
+	vmovdqa64	%zmm12,1216(%rsp)
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_wxeifxCFreBhncp
+	vmovdqu64	640(%rsp),%zmm3
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	576(%rsp),%zmm4
+	vmovdqu64	512(%rsp),%zmm5
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,256(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,192(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,128(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,64(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,0(%rsp)
+.L_skip_hkeys_precomputation_wxeifxCFreBhncp:
+	movq	$1,%r14
+	addq	$512,%rax
+	subq	$512,%r8
+
+	cmpq	$768,%r8
+	jb	.L_no_more_big_nblocks_jitrpnrpcoxxhbG
+.L_encrypt_big_nblocks_jitrpnrpcoxxhbG:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_mepDqBfDrujrpoE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_mepDqBfDrujrpoE
+.L_16_blocks_overflow_mepDqBfDrujrpoE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_mepDqBfDrujrpoE:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_odmgwkilkqAGtnD
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_odmgwkilkqAGtnD
+.L_16_blocks_overflow_odmgwkilkqAGtnD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_odmgwkilkqAGtnD:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	256(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	320(%rsp),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	384(%rsp),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	448(%rsp),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm17
+	vmovdqu8	320(%rcx,%rax,1),%zmm19
+	vmovdqu8	384(%rcx,%rax,1),%zmm20
+	vmovdqu8	448(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,256(%r10,%rax,1)
+	vmovdqu8	%zmm3,320(%r10,%rax,1)
+	vmovdqu8	%zmm4,384(%r10,%rax,1)
+	vmovdqu8	%zmm5,448(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,768(%rsp)
+	vmovdqa64	%zmm3,832(%rsp)
+	vmovdqa64	%zmm4,896(%rsp)
+	vmovdqa64	%zmm5,960(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_rCkEGljicruApxi
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_rCkEGljicruApxi
+.L_16_blocks_overflow_rCkEGljicruApxi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_rCkEGljicruApxi:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	512(%rcx,%rax,1),%zmm17
+	vmovdqu8	576(%rcx,%rax,1),%zmm19
+	vmovdqu8	640(%rcx,%rax,1),%zmm20
+	vmovdqu8	704(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+
+
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
+	vpxorq	%zmm24,%zmm6,%zmm6
+	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
+	vpxorq	%zmm25,%zmm7,%zmm7
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vextracti64x4	$1,%zmm6,%ymm12
+	vpxorq	%ymm12,%ymm6,%ymm6
+	vextracti32x4	$1,%ymm6,%xmm12
+	vpxorq	%xmm12,%xmm6,%xmm6
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,512(%r10,%rax,1)
+	vmovdqu8	%zmm3,576(%r10,%rax,1)
+	vmovdqu8	%zmm4,640(%r10,%rax,1)
+	vmovdqu8	%zmm5,704(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,1024(%rsp)
+	vmovdqa64	%zmm3,1088(%rsp)
+	vmovdqa64	%zmm4,1152(%rsp)
+	vmovdqa64	%zmm5,1216(%rsp)
+	vmovdqa64	%zmm6,%zmm14
+
+	addq	$768,%rax
+	subq	$768,%r8
+	cmpq	$768,%r8
+	jae	.L_encrypt_big_nblocks_jitrpnrpcoxxhbG
+
+.L_no_more_big_nblocks_jitrpnrpcoxxhbG:
+
+	cmpq	$512,%r8
+	jae	.L_encrypt_32_blocks_jitrpnrpcoxxhbG
+
+	cmpq	$256,%r8
+	jae	.L_encrypt_16_blocks_jitrpnrpcoxxhbG
+.L_encrypt_0_blocks_ghash_32_jitrpnrpcoxxhbG:
+	movl	%r8d,%r10d
+	andl	$~15,%r10d
+	movl	$256,%ebx
+	subl	%r10d,%ebx
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	addl	$256,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_oijqcfjlgGpesCs
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_oijqcfjlgGpesCs
+	jb	.L_last_num_blocks_is_7_1_oijqcfjlgGpesCs
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_oijqcfjlgGpesCs
+	jb	.L_last_num_blocks_is_11_9_oijqcfjlgGpesCs
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_oijqcfjlgGpesCs
+	ja	.L_last_num_blocks_is_16_oijqcfjlgGpesCs
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_oijqcfjlgGpesCs
+	jmp	.L_last_num_blocks_is_13_oijqcfjlgGpesCs
+
+.L_last_num_blocks_is_11_9_oijqcfjlgGpesCs:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_oijqcfjlgGpesCs
+	ja	.L_last_num_blocks_is_11_oijqcfjlgGpesCs
+	jmp	.L_last_num_blocks_is_9_oijqcfjlgGpesCs
+
+.L_last_num_blocks_is_7_1_oijqcfjlgGpesCs:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_oijqcfjlgGpesCs
+	jb	.L_last_num_blocks_is_3_1_oijqcfjlgGpesCs
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_oijqcfjlgGpesCs
+	je	.L_last_num_blocks_is_6_oijqcfjlgGpesCs
+	jmp	.L_last_num_blocks_is_5_oijqcfjlgGpesCs
+
+.L_last_num_blocks_is_3_1_oijqcfjlgGpesCs:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_oijqcfjlgGpesCs
+	je	.L_last_num_blocks_is_2_oijqcfjlgGpesCs
+.L_last_num_blocks_is_1_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_jdfcAduucBEGyvq
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_jdfcAduucBEGyvq
+
+.L_16_blocks_overflow_jdfcAduucBEGyvq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_jdfcAduucBEGyvq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%xmm29,%xmm17,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dqyjlqxyEzvbgzr
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dqyjlqxyEzvbgzr
+.L_small_initial_partial_block_dqyjlqxyEzvbgzr:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_dqyjlqxyEzvbgzr
+.L_small_initial_compute_done_dqyjlqxyEzvbgzr:
+.L_after_reduction_dqyjlqxyEzvbgzr:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_2_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_FkovcBAApwgsjiq
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_FkovcBAApwgsjiq
+
+.L_16_blocks_overflow_FkovcBAApwgsjiq:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_FkovcBAApwgsjiq:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%ymm29,%ymm17,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_htdpiFvbeorGpjE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_htdpiFvbeorGpjE
+.L_small_initial_partial_block_htdpiFvbeorGpjE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_htdpiFvbeorGpjE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_htdpiFvbeorGpjE
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_htdpiFvbeorGpjE:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_3_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_lCltukcvineisgj
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_lCltukcvineisgj
+
+.L_16_blocks_overflow_lCltukcvineisgj:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_lCltukcvineisgj:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ajviujfDwkokwhj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ajviujfDwkokwhj
+.L_small_initial_partial_block_ajviujfDwkokwhj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ajviujfDwkokwhj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ajviujfDwkokwhj
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ajviujfDwkokwhj:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_4_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_qsqtraBhigjGqek
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_qsqtraBhigjGqek
+
+.L_16_blocks_overflow_qsqtraBhigjGqek:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_qsqtraBhigjGqek:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_FvspxAhwmCfwjBl
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_FvspxAhwmCfwjBl
+.L_small_initial_partial_block_FvspxAhwmCfwjBl:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_FvspxAhwmCfwjBl:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_FvspxAhwmCfwjBl
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_FvspxAhwmCfwjBl:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_5_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_ovjBActkqhyzxEg
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_ovjBActkqhyzxEg
+
+.L_16_blocks_overflow_ovjBActkqhyzxEg:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_ovjBActkqhyzxEg:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%xmm29,%xmm19,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vgdwsloqpnuvvqc
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vgdwsloqpnuvvqc
+.L_small_initial_partial_block_vgdwsloqpnuvvqc:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vgdwsloqpnuvvqc:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vgdwsloqpnuvvqc
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_vgdwsloqpnuvvqc:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_6_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_AyalgmFkykuGyAj
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_AyalgmFkykuGyAj
+
+.L_16_blocks_overflow_AyalgmFkykuGyAj:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_AyalgmFkykuGyAj:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%ymm29,%ymm19,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_lBxtkeChepxwiCb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_lBxtkeChepxwiCb
+.L_small_initial_partial_block_lBxtkeChepxwiCb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_lBxtkeChepxwiCb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_lBxtkeChepxwiCb
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_lBxtkeChepxwiCb:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_7_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_eaedtmyisozkfuj
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_eaedtmyisozkfuj
+
+.L_16_blocks_overflow_eaedtmyisozkfuj:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_eaedtmyisozkfuj:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_FAevzllgslqukbd
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_FAevzllgslqukbd
+.L_small_initial_partial_block_FAevzllgslqukbd:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_FAevzllgslqukbd:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_FAevzllgslqukbd
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_FAevzllgslqukbd:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_8_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_bhEzmkucEdzzkrh
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_bhEzmkucEdzzkrh
+
+.L_16_blocks_overflow_bhEzmkucEdzzkrh:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_bhEzmkucEdzzkrh:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CxCFtBdExgrbEEc
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CxCFtBdExgrbEEc
+.L_small_initial_partial_block_CxCFtBdExgrbEEc:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CxCFtBdExgrbEEc:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CxCFtBdExgrbEEc
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CxCFtBdExgrbEEc:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_9_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_sApmuomGzbEzros
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_sApmuomGzbEzros
+
+.L_16_blocks_overflow_sApmuomGzbEzros:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_sApmuomGzbEzros:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%xmm29,%xmm20,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vnwcwilBvzBsksi
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vnwcwilBvzBsksi
+.L_small_initial_partial_block_vnwcwilBvzBsksi:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vnwcwilBvzBsksi:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vnwcwilBvzBsksi
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_vnwcwilBvzBsksi:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_10_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_FnfctnoAvFyDEzv
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_FnfctnoAvFyDEzv
+
+.L_16_blocks_overflow_FnfctnoAvFyDEzv:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_FnfctnoAvFyDEzv:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%ymm29,%ymm20,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ornmpniwqztDEpa
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ornmpniwqztDEpa
+.L_small_initial_partial_block_ornmpniwqztDEpa:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ornmpniwqztDEpa:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ornmpniwqztDEpa
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ornmpniwqztDEpa:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_11_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_gnzmszAkwFvijdG
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_gnzmszAkwFvijdG
+
+.L_16_blocks_overflow_gnzmszAkwFvijdG:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_gnzmszAkwFvijdG:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AqDwudDzBjwDylp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AqDwudDzBjwDylp
+.L_small_initial_partial_block_AqDwudDzBjwDylp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AqDwudDzBjwDylp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AqDwudDzBjwDylp
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_AqDwudDzBjwDylp:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_12_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_vFdtxteAimfneDB
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_vFdtxteAimfneDB
+
+.L_16_blocks_overflow_vFdtxteAimfneDB:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_vFdtxteAimfneDB:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_uvqAmmDsGcEphtt
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_uvqAmmDsGcEphtt
+.L_small_initial_partial_block_uvqAmmDsGcEphtt:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_uvqAmmDsGcEphtt:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_uvqAmmDsGcEphtt
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_uvqAmmDsGcEphtt:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_13_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_inEinphdjrwsage
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_inEinphdjrwsage
+
+.L_16_blocks_overflow_inEinphdjrwsage:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_inEinphdjrwsage:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%xmm29,%xmm21,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_tspbcrGqzhiCfow
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_tspbcrGqzhiCfow
+.L_small_initial_partial_block_tspbcrGqzhiCfow:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_tspbcrGqzhiCfow:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_tspbcrGqzhiCfow
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_tspbcrGqzhiCfow:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_14_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_GiybGbauFbjlkmg
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_GiybGbauFbjlkmg
+
+.L_16_blocks_overflow_GiybGbauFbjlkmg:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_GiybGbauFbjlkmg:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%ymm29,%ymm21,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CctvfBcgrEatvEt
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CctvfBcgrEatvEt
+.L_small_initial_partial_block_CctvfBcgrEatvEt:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CctvfBcgrEatvEt:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CctvfBcgrEatvEt
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CctvfBcgrEatvEt:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_15_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_gFdbhmCswnahFzG
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_gFdbhmCswnahFzG
+
+.L_16_blocks_overflow_gFdbhmCswnahFzG:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_gFdbhmCswnahFzG:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_jxesvngBnqzgDaB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_jxesvngBnqzgDaB
+.L_small_initial_partial_block_jxesvngBnqzgDaB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_jxesvngBnqzgDaB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_jxesvngBnqzgDaB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_jxesvngBnqzgDaB:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_16_oijqcfjlgGpesCs:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_nbubbDvptiAwCuf
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_nbubbDvptiAwCuf
+
+.L_16_blocks_overflow_nbubbDvptiAwCuf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_nbubbDvptiAwCuf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_pgoiwiqihmEykFq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_pgoiwiqihmEykFq:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_pgoiwiqihmEykFq:
+	jmp	.L_last_blocks_done_oijqcfjlgGpesCs
+.L_last_num_blocks_is_0_oijqcfjlgGpesCs:
+	vmovdqa64	1024(%rsp),%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1088(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1152(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1216(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_oijqcfjlgGpesCs:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_jitrpnrpcoxxhbG
+.L_encrypt_32_blocks_jitrpnrpcoxxhbG:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_kjpwkufohdtooea
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_kjpwkufohdtooea
+.L_16_blocks_overflow_kjpwkufohdtooea:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_kjpwkufohdtooea:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_vfCmExqucwmmAcE
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_vfCmExqucwmmAcE
+.L_16_blocks_overflow_vfCmExqucwmmAcE:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_vfCmExqucwmmAcE:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1024(%rsp),%zmm8
+	vmovdqu64	256(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	320(%rsp),%zmm18
+	vmovdqa64	1088(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	384(%rsp),%zmm1
+	vmovdqa64	1152(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	448(%rsp),%zmm18
+	vmovdqa64	1216(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	256(%rcx,%rax,1),%zmm17
+	vmovdqu8	320(%rcx,%rax,1),%zmm19
+	vmovdqu8	384(%rcx,%rax,1),%zmm20
+	vmovdqu8	448(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
+	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
+	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,256(%r10,%rax,1)
+	vmovdqu8	%zmm3,320(%r10,%rax,1)
+	vmovdqu8	%zmm4,384(%r10,%rax,1)
+	vmovdqu8	%zmm5,448(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,768(%rsp)
+	vmovdqa64	%zmm3,832(%rsp)
+	vmovdqa64	%zmm4,896(%rsp)
+	vmovdqa64	%zmm5,960(%rsp)
+	vmovdqa64	1280(%rsp),%zmm13
+	vmovdqu64	512(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1344(%rsp),%zmm13
+	vmovdqu64	576(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1408(%rsp),%zmm13
+	vmovdqu64	640(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1472(%rsp),%zmm13
+	vmovdqu64	704(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+	subq	$512,%r8
+	addq	$512,%rax
+	movl	%r8d,%r10d
+	andl	$~15,%r10d
+	movl	$512,%ebx
+	subl	%r10d,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_BtroCzlvvCangzc
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_BtroCzlvvCangzc
+	jb	.L_last_num_blocks_is_7_1_BtroCzlvvCangzc
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_BtroCzlvvCangzc
+	jb	.L_last_num_blocks_is_11_9_BtroCzlvvCangzc
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_BtroCzlvvCangzc
+	ja	.L_last_num_blocks_is_16_BtroCzlvvCangzc
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_BtroCzlvvCangzc
+	jmp	.L_last_num_blocks_is_13_BtroCzlvvCangzc
+
+.L_last_num_blocks_is_11_9_BtroCzlvvCangzc:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_BtroCzlvvCangzc
+	ja	.L_last_num_blocks_is_11_BtroCzlvvCangzc
+	jmp	.L_last_num_blocks_is_9_BtroCzlvvCangzc
+
+.L_last_num_blocks_is_7_1_BtroCzlvvCangzc:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_BtroCzlvvCangzc
+	jb	.L_last_num_blocks_is_3_1_BtroCzlvvCangzc
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_BtroCzlvvCangzc
+	je	.L_last_num_blocks_is_6_BtroCzlvvCangzc
+	jmp	.L_last_num_blocks_is_5_BtroCzlvvCangzc
+
+.L_last_num_blocks_is_3_1_BtroCzlvvCangzc:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_BtroCzlvvCangzc
+	je	.L_last_num_blocks_is_2_BtroCzlvvCangzc
+.L_last_num_blocks_is_1_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_ummvEGrFimbdmGi
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_ummvEGrFimbdmGi
+
+.L_16_blocks_overflow_ummvEGrFimbdmGi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_ummvEGrFimbdmGi:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%xmm29,%xmm17,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_toAbzcgtyhexFzb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_toAbzcgtyhexFzb
+.L_small_initial_partial_block_toAbzcgtyhexFzb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_toAbzcgtyhexFzb
+.L_small_initial_compute_done_toAbzcgtyhexFzb:
+.L_after_reduction_toAbzcgtyhexFzb:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_2_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_Dvtvywfdussaxzo
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_Dvtvywfdussaxzo
+
+.L_16_blocks_overflow_Dvtvywfdussaxzo:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_Dvtvywfdussaxzo:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%ymm29,%ymm17,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_gAzybEepmvguibA
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_gAzybEepmvguibA
+.L_small_initial_partial_block_gAzybEepmvguibA:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_gAzybEepmvguibA:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_gAzybEepmvguibA
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_gAzybEepmvguibA:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_3_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_DplmhbjmbwaFxjC
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_DplmhbjmbwaFxjC
+
+.L_16_blocks_overflow_DplmhbjmbwaFxjC:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_DplmhbjmbwaFxjC:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_oozofFqiqBkGabz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_oozofFqiqBkGabz
+.L_small_initial_partial_block_oozofFqiqBkGabz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_oozofFqiqBkGabz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_oozofFqiqBkGabz
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_oozofFqiqBkGabz:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_4_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_zsbzCshiirvhuiD
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_zsbzCshiirvhuiD
+
+.L_16_blocks_overflow_zsbzCshiirvhuiD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_zsbzCshiirvhuiD:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_myGrpBlgukvnfxx
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_myGrpBlgukvnfxx
+.L_small_initial_partial_block_myGrpBlgukvnfxx:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_myGrpBlgukvnfxx:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_myGrpBlgukvnfxx
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_myGrpBlgukvnfxx:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_5_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_aooxmvbphEgxpfh
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_aooxmvbphEgxpfh
+
+.L_16_blocks_overflow_aooxmvbphEgxpfh:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_aooxmvbphEgxpfh:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%xmm29,%xmm19,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wwarcagxFvdwdgz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wwarcagxFvdwdgz
+.L_small_initial_partial_block_wwarcagxFvdwdgz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_wwarcagxFvdwdgz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_wwarcagxFvdwdgz
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_wwarcagxFvdwdgz:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_6_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_wvjiFcpidkdArrC
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_wvjiFcpidkdArrC
+
+.L_16_blocks_overflow_wvjiFcpidkdArrC:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_wvjiFcpidkdArrC:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%ymm29,%ymm19,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CbuxwutcwgjqtAm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CbuxwutcwgjqtAm
+.L_small_initial_partial_block_CbuxwutcwgjqtAm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CbuxwutcwgjqtAm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CbuxwutcwgjqtAm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CbuxwutcwgjqtAm:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_7_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_alpkoxEkasfBGae
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_alpkoxEkasfBGae
+
+.L_16_blocks_overflow_alpkoxEkasfBGae:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_alpkoxEkasfBGae:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ribmgGocfiefzuo
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ribmgGocfiefzuo
+.L_small_initial_partial_block_ribmgGocfiefzuo:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ribmgGocfiefzuo:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ribmgGocfiefzuo
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ribmgGocfiefzuo:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_8_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_faixnAGmDewkCiD
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_faixnAGmDewkCiD
+
+.L_16_blocks_overflow_faixnAGmDewkCiD:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_faixnAGmDewkCiD:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_yvAdGGugCAoGjFD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_yvAdGGugCAoGjFD
+.L_small_initial_partial_block_yvAdGGugCAoGjFD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_yvAdGGugCAoGjFD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_yvAdGGugCAoGjFD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_yvAdGGugCAoGjFD:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_9_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_ypkwgyGwdlglcpf
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_ypkwgyGwdlglcpf
+
+.L_16_blocks_overflow_ypkwgyGwdlglcpf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_ypkwgyGwdlglcpf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%xmm29,%xmm20,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_cbByuGcvcBxfpGp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_cbByuGcvcBxfpGp
+.L_small_initial_partial_block_cbByuGcvcBxfpGp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_cbByuGcvcBxfpGp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_cbByuGcvcBxfpGp
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_cbByuGcvcBxfpGp:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_10_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_ykaBjtrafhafvdb
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_ykaBjtrafhafvdb
+
+.L_16_blocks_overflow_ykaBjtrafhafvdb:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_ykaBjtrafhafvdb:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%ymm29,%ymm20,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CpEjiiwAhtdftaz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CpEjiiwAhtdftaz
+.L_small_initial_partial_block_CpEjiiwAhtdftaz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CpEjiiwAhtdftaz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CpEjiiwAhtdftaz
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CpEjiiwAhtdftaz:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_11_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_tElhGxutEdicFrF
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_tElhGxutEdicFrF
+
+.L_16_blocks_overflow_tElhGxutEdicFrF:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_tElhGxutEdicFrF:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_oBtferedfrtvDDt
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_oBtferedfrtvDDt
+.L_small_initial_partial_block_oBtferedfrtvDDt:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_oBtferedfrtvDDt:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_oBtferedfrtvDDt
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_oBtferedfrtvDDt:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_12_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_lxGsACgijrzGsit
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_lxGsACgijrzGsit
+
+.L_16_blocks_overflow_lxGsACgijrzGsit:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_lxGsACgijrzGsit:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_sFbwAoBxyjAeayB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_sFbwAoBxyjAeayB
+.L_small_initial_partial_block_sFbwAoBxyjAeayB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_sFbwAoBxyjAeayB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_sFbwAoBxyjAeayB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_sFbwAoBxyjAeayB:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_13_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_aophiqiqhoFyrxw
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_aophiqiqhoFyrxw
+
+.L_16_blocks_overflow_aophiqiqhoFyrxw:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_aophiqiqhoFyrxw:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%xmm29,%xmm21,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_mjzoCoyolBbtowf
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_mjzoCoyolBbtowf
+.L_small_initial_partial_block_mjzoCoyolBbtowf:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_mjzoCoyolBbtowf:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_mjzoCoyolBbtowf
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_mjzoCoyolBbtowf:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_14_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_GCegsgskwjbBcju
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_GCegsgskwjbBcju
+
+.L_16_blocks_overflow_GCegsgskwjbBcju:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_GCegsgskwjbBcju:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%ymm29,%ymm21,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vsozsEnmpsiekhu
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vsozsEnmpsiekhu
+.L_small_initial_partial_block_vsozsEnmpsiekhu:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vsozsEnmpsiekhu:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vsozsEnmpsiekhu
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_vsozsEnmpsiekhu:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_15_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_GGAsEfycEGFzdtn
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_GGAsEfycEGFzdtn
+
+.L_16_blocks_overflow_GGAsEfycEGFzdtn:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_GGAsEfycEGFzdtn:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ubCbCnsavganBzi
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ubCbCnsavganBzi
+.L_small_initial_partial_block_ubCbCnsavganBzi:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ubCbCnsavganBzi:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ubCbCnsavganBzi
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ubCbCnsavganBzi:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_16_BtroCzlvvCangzc:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_jBgEfqhdDDlmwhs
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_jBgEfqhdDDlmwhs
+
+.L_16_blocks_overflow_jBgEfqhdDDlmwhs:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_jBgEfqhdDDlmwhs:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_joFbEoFkmywdCud:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_joFbEoFkmywdCud:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_joFbEoFkmywdCud:
+	jmp	.L_last_blocks_done_BtroCzlvvCangzc
+.L_last_num_blocks_is_0_BtroCzlvvCangzc:
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_BtroCzlvvCangzc:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_jitrpnrpcoxxhbG
+.L_encrypt_16_blocks_jitrpnrpcoxxhbG:
+	cmpb	$240,%r15b
+	jae	.L_16_blocks_overflow_jajBcuquhiqDhFk
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_jajBcuquhiqDhFk
+.L_16_blocks_overflow_jajBcuquhiqDhFk:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_jajBcuquhiqDhFk:
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp),%zmm1
+
+
+
+
+	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
+	addb	$16,%r15b
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+
+
+
+
+
+
+
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+
+
+
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21
+
+
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vpxorq	%zmm12,%zmm6,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+
+
+
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+
+
+
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1)
+	vpshufb	%zmm29,%zmm17,%zmm0
+	vpshufb	%zmm29,%zmm19,%zmm3
+	vpshufb	%zmm29,%zmm20,%zmm4
+	vpshufb	%zmm29,%zmm21,%zmm5
+	vmovdqa64	%zmm0,1280(%rsp)
+	vmovdqa64	%zmm3,1344(%rsp)
+	vmovdqa64	%zmm4,1408(%rsp)
+	vmovdqa64	%zmm5,1472(%rsp)
+	vmovdqa64	1024(%rsp),%zmm13
+	vmovdqu64	256(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1088(%rsp),%zmm13
+	vmovdqu64	320(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1152(%rsp),%zmm13
+	vmovdqu64	384(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1216(%rsp),%zmm13
+	vmovdqu64	448(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	subq	$256,%r8
+	addq	$256,%rax
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_wnFhGaFmuujjFeb
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_wnFhGaFmuujjFeb
+	jb	.L_last_num_blocks_is_7_1_wnFhGaFmuujjFeb
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_wnFhGaFmuujjFeb
+	jb	.L_last_num_blocks_is_11_9_wnFhGaFmuujjFeb
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_wnFhGaFmuujjFeb
+	ja	.L_last_num_blocks_is_16_wnFhGaFmuujjFeb
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_wnFhGaFmuujjFeb
+	jmp	.L_last_num_blocks_is_13_wnFhGaFmuujjFeb
+
+.L_last_num_blocks_is_11_9_wnFhGaFmuujjFeb:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_wnFhGaFmuujjFeb
+	ja	.L_last_num_blocks_is_11_wnFhGaFmuujjFeb
+	jmp	.L_last_num_blocks_is_9_wnFhGaFmuujjFeb
+
+.L_last_num_blocks_is_7_1_wnFhGaFmuujjFeb:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_wnFhGaFmuujjFeb
+	jb	.L_last_num_blocks_is_3_1_wnFhGaFmuujjFeb
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_wnFhGaFmuujjFeb
+	je	.L_last_num_blocks_is_6_wnFhGaFmuujjFeb
+	jmp	.L_last_num_blocks_is_5_wnFhGaFmuujjFeb
+
+.L_last_num_blocks_is_3_1_wnFhGaFmuujjFeb:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_wnFhGaFmuujjFeb
+	je	.L_last_num_blocks_is_2_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_1_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_rhABkgeoyAejvmn
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_rhABkgeoyAejvmn
+
+.L_16_blocks_overflow_rhABkgeoyAejvmn:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_rhABkgeoyAejvmn:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%xmm29,%xmm17,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ngvvDCmfgCuCeEk
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ngvvDCmfgCuCeEk
+.L_small_initial_partial_block_ngvvDCmfgCuCeEk:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_ngvvDCmfgCuCeEk
+.L_small_initial_compute_done_ngvvDCmfgCuCeEk:
+.L_after_reduction_ngvvDCmfgCuCeEk:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_2_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_tswoGxinodirdEy
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_tswoGxinodirdEy
+
+.L_16_blocks_overflow_tswoGxinodirdEy:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_tswoGxinodirdEy:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%ymm29,%ymm17,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_gpADwgAvspvpbpm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_gpADwgAvspvpbpm
+.L_small_initial_partial_block_gpADwgAvspvpbpm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_gpADwgAvspvpbpm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_gpADwgAvspvpbpm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_gpADwgAvspvpbpm:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_3_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_huwlEtrwyflqwuy
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_huwlEtrwyflqwuy
+
+.L_16_blocks_overflow_huwlEtrwyflqwuy:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_huwlEtrwyflqwuy:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BtmqzfthtGmoitd
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BtmqzfthtGmoitd
+.L_small_initial_partial_block_BtmqzfthtGmoitd:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BtmqzfthtGmoitd:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BtmqzfthtGmoitd
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_BtmqzfthtGmoitd:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_4_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_jnqbhvwEzzfyFqz
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_jnqbhvwEzzfyFqz
+
+.L_16_blocks_overflow_jnqbhvwEzzfyFqz:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_jnqbhvwEzzfyFqz:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_brbesiFtmBuwEdk
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_brbesiFtmBuwEdk
+.L_small_initial_partial_block_brbesiFtmBuwEdk:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_brbesiFtmBuwEdk:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_brbesiFtmBuwEdk
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_brbesiFtmBuwEdk:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_5_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_jriwtdwBytpEncj
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_jriwtdwBytpEncj
+
+.L_16_blocks_overflow_jriwtdwBytpEncj:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_jriwtdwBytpEncj:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%xmm29,%xmm19,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AbddspowuvcqnGm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AbddspowuvcqnGm
+.L_small_initial_partial_block_AbddspowuvcqnGm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AbddspowuvcqnGm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AbddspowuvcqnGm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_AbddspowuvcqnGm:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_6_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_EiBdgpxigBasCsr
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_EiBdgpxigBasCsr
+
+.L_16_blocks_overflow_EiBdgpxigBasCsr:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_EiBdgpxigBasCsr:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%ymm29,%ymm19,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_CmdukntxEbqobtc
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_CmdukntxEbqobtc
+.L_small_initial_partial_block_CmdukntxEbqobtc:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_CmdukntxEbqobtc:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_CmdukntxEbqobtc
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_CmdukntxEbqobtc:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_7_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_kByyDvwfzoyycFa
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_kByyDvwfzoyycFa
+
+.L_16_blocks_overflow_kByyDvwfzoyycFa:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_kByyDvwfzoyycFa:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GjGbEyyuFCxdEDm
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GjGbEyyuFCxdEDm
+.L_small_initial_partial_block_GjGbEyyuFCxdEDm:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GjGbEyyuFCxdEDm:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GjGbEyyuFCxdEDm
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GjGbEyyuFCxdEDm:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_8_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_wauhDcqjFoyAzfs
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_wauhDcqjFoyAzfs
+
+.L_16_blocks_overflow_wauhDcqjFoyAzfs:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_wauhDcqjFoyAzfs:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_DipuincnjafzcfD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_DipuincnjafzcfD
+.L_small_initial_partial_block_DipuincnjafzcfD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_DipuincnjafzcfD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_DipuincnjafzcfD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_DipuincnjafzcfD:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_9_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_uvhjdqdcshGAsxr
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_uvhjdqdcshGAsxr
+
+.L_16_blocks_overflow_uvhjdqdcshGAsxr:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_uvhjdqdcshGAsxr:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%xmm29,%xmm20,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_zgbDiEwquBhuqaj
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_zgbDiEwquBhuqaj
+.L_small_initial_partial_block_zgbDiEwquBhuqaj:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zgbDiEwquBhuqaj:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_zgbDiEwquBhuqaj
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_zgbDiEwquBhuqaj:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_10_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_bAkvwrGjyGhCwED
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_bAkvwrGjyGhCwED
+
+.L_16_blocks_overflow_bAkvwrGjyGhCwED:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_bAkvwrGjyGhCwED:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%ymm29,%ymm20,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_EasEhffgufnGrwD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_EasEhffgufnGrwD
+.L_small_initial_partial_block_EasEhffgufnGrwD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_EasEhffgufnGrwD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_EasEhffgufnGrwD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_EasEhffgufnGrwD:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_11_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_FoFFxAcjsmycDbb
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_FoFFxAcjsmycDbb
+
+.L_16_blocks_overflow_FoFFxAcjsmycDbb:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_FoFFxAcjsmycDbb:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vtwxkwFoFgselBD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vtwxkwFoFgselBD
+.L_small_initial_partial_block_vtwxkwFoFgselBD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vtwxkwFoFgselBD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vtwxkwFoFgselBD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_vtwxkwFoFgselBD:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_12_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_dFmrjqdnhDfuaiC
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_dFmrjqdnhDfuaiC
+
+.L_16_blocks_overflow_dFmrjqdnhDfuaiC:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_dFmrjqdnhDfuaiC:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_auuEpxBFerBjbyG
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_auuEpxBFerBjbyG
+.L_small_initial_partial_block_auuEpxBFerBjbyG:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_auuEpxBFerBjbyG:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_auuEpxBFerBjbyG
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_auuEpxBFerBjbyG:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_13_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_DevpzfimljeBmfj
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_DevpzfimljeBmfj
+
+.L_16_blocks_overflow_DevpzfimljeBmfj:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_DevpzfimljeBmfj:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%xmm29,%xmm21,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ouoEotoDghrpblb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ouoEotoDghrpblb
+.L_small_initial_partial_block_ouoEotoDghrpblb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ouoEotoDghrpblb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ouoEotoDghrpblb
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ouoEotoDghrpblb:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_14_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_qGjmEiDtuEwyfDm
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_qGjmEiDtuEwyfDm
+
+.L_16_blocks_overflow_qGjmEiDtuEwyfDm:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_qGjmEiDtuEwyfDm:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%ymm29,%ymm21,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_yyredBsrolGlmdb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_yyredBsrolGlmdb
+.L_small_initial_partial_block_yyredBsrolGlmdb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_yyredBsrolGlmdb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_yyredBsrolGlmdb
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_yyredBsrolGlmdb:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_15_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_lmiFgCmrdjbbBrr
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_lmiFgCmrdjbbBrr
+
+.L_16_blocks_overflow_lmiFgCmrdjbbBrr:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_lmiFgCmrdjbbBrr:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_lnCCulrrFiyBpmC
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_lnCCulrrFiyBpmC
+.L_small_initial_partial_block_lnCCulrrFiyBpmC:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_lnCCulrrFiyBpmC:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_lnCCulrrFiyBpmC
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_lnCCulrrFiyBpmC:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_16_wnFhGaFmuujjFeb:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_mvdzqEziFnEvxfB
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_mvdzqEziFnEvxfB
+
+.L_16_blocks_overflow_mvdzqEziFnEvxfB:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_mvdzqEziFnEvxfB:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vmovdqa64	1280(%rsp),%zmm8
+	vmovdqu64	512(%rsp),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	576(%rsp),%zmm18
+	vmovdqa64	1344(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	640(%rsp),%zmm1
+	vmovdqa64	1408(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	704(%rsp),%zmm18
+	vmovdqa64	1472(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
+	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
+	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vpsrldq	$8,%zmm10,%zmm15
+	vpslldq	$8,%zmm10,%zmm10
+
+	vmovdqa64	POLY2(%rip),%xmm16
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vpxorq	%zmm15,%zmm14,%zmm14
+	vpxorq	%zmm10,%zmm7,%zmm7
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vextracti64x4	$1,%zmm14,%ymm12
+	vpxorq	%ymm12,%ymm14,%ymm14
+	vextracti32x4	$1,%ymm14,%xmm12
+	vpxorq	%xmm12,%xmm14,%xmm14
+	vextracti64x4	$1,%zmm7,%ymm13
+	vpxorq	%ymm13,%ymm7,%ymm7
+	vextracti32x4	$1,%ymm7,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm7
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
+	vpslldq	$8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm7,%xmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
+	vpsrldq	$4,%xmm12,%xmm12
+	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
+	vpslldq	$4,%xmm15,%xmm15
+
+	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_nfdjicByoFgkstw:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vpxorq	%zmm14,%zmm17,%zmm17
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm31,%zmm5,%zmm5
+	vpxorq	%zmm8,%zmm0,%zmm0
+	vpxorq	%zmm22,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_nfdjicByoFgkstw:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_nfdjicByoFgkstw:
+	jmp	.L_last_blocks_done_wnFhGaFmuujjFeb
+.L_last_num_blocks_is_0_wnFhGaFmuujjFeb:
+	vmovdqa64	1280(%rsp),%zmm13
+	vmovdqu64	512(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1344(%rsp),%zmm13
+	vmovdqu64	576(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	1408(%rsp),%zmm13
+	vmovdqu64	640(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	1472(%rsp),%zmm13
+	vmovdqu64	704(%rsp),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_wnFhGaFmuujjFeb:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_jitrpnrpcoxxhbG
+
+.L_message_below_32_blocks_jitrpnrpcoxxhbG:
+
+
+	subq	$256,%r8
+	addq	$256,%rax
+	movl	%r8d,%r10d
+	leaq	96(%rsi),%r12
+	testq	%r14,%r14
+	jnz	.L_skip_hkeys_precomputation_yznutCEoqxAgjqo
+	vmovdqu64	640(%rsp),%zmm3
+
+
+	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3
+
+	vmovdqu64	576(%rsp),%zmm4
+	vmovdqu64	512(%rsp),%zmm5
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,448(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,384(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm4,%zmm4
+
+	vpsrldq	$8,%zmm4,%zmm10
+	vpslldq	$8,%zmm4,%zmm4
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm4,%zmm4
+
+
+
+	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
+	vpslldq	$4,%zmm4,%zmm4
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4
+
+	vmovdqu64	%zmm4,320(%rsp)
+
+	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
+	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
+	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
+	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm5,%zmm5
+
+	vpsrldq	$8,%zmm5,%zmm10
+	vpslldq	$8,%zmm5,%zmm5
+	vpxorq	%zmm10,%zmm6,%zmm6
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vmovdqu64	POLY2(%rip),%zmm10
+
+	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
+	vpslldq	$8,%zmm7,%zmm7
+	vpxorq	%zmm7,%zmm5,%zmm5
+
+
+
+	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
+	vpsrldq	$4,%zmm7,%zmm7
+	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
+	vpslldq	$4,%zmm5,%zmm5
+
+	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5
+
+	vmovdqu64	%zmm5,256(%rsp)
+.L_skip_hkeys_precomputation_yznutCEoqxAgjqo:
+	movq	$1,%r14
+	andl	$~15,%r10d
+	movl	$512,%ebx
+	subl	%r10d,%ebx
+	movl	%r8d,%r10d
+	addl	$15,%r10d
+	shrl	$4,%r10d
+	je	.L_last_num_blocks_is_0_ElskehiCzqhzidf
+
+	cmpl	$8,%r10d
+	je	.L_last_num_blocks_is_8_ElskehiCzqhzidf
+	jb	.L_last_num_blocks_is_7_1_ElskehiCzqhzidf
+
+
+	cmpl	$12,%r10d
+	je	.L_last_num_blocks_is_12_ElskehiCzqhzidf
+	jb	.L_last_num_blocks_is_11_9_ElskehiCzqhzidf
+
+
+	cmpl	$15,%r10d
+	je	.L_last_num_blocks_is_15_ElskehiCzqhzidf
+	ja	.L_last_num_blocks_is_16_ElskehiCzqhzidf
+	cmpl	$14,%r10d
+	je	.L_last_num_blocks_is_14_ElskehiCzqhzidf
+	jmp	.L_last_num_blocks_is_13_ElskehiCzqhzidf
+
+.L_last_num_blocks_is_11_9_ElskehiCzqhzidf:
+
+	cmpl	$10,%r10d
+	je	.L_last_num_blocks_is_10_ElskehiCzqhzidf
+	ja	.L_last_num_blocks_is_11_ElskehiCzqhzidf
+	jmp	.L_last_num_blocks_is_9_ElskehiCzqhzidf
+
+.L_last_num_blocks_is_7_1_ElskehiCzqhzidf:
+	cmpl	$4,%r10d
+	je	.L_last_num_blocks_is_4_ElskehiCzqhzidf
+	jb	.L_last_num_blocks_is_3_1_ElskehiCzqhzidf
+
+	cmpl	$6,%r10d
+	ja	.L_last_num_blocks_is_7_ElskehiCzqhzidf
+	je	.L_last_num_blocks_is_6_ElskehiCzqhzidf
+	jmp	.L_last_num_blocks_is_5_ElskehiCzqhzidf
+
+.L_last_num_blocks_is_3_1_ElskehiCzqhzidf:
+
+	cmpl	$2,%r10d
+	ja	.L_last_num_blocks_is_3_ElskehiCzqhzidf
+	je	.L_last_num_blocks_is_2_ElskehiCzqhzidf
+.L_last_num_blocks_is_1_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$255,%r15d
+	jae	.L_16_blocks_overflow_GrvEhrlAphGlvvg
+	vpaddd	%xmm28,%xmm2,%xmm0
+	jmp	.L_16_blocks_ok_GrvEhrlAphGlvvg
+
+.L_16_blocks_overflow_GrvEhrlAphGlvvg:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%xmm29,%xmm0,%xmm0
+.L_16_blocks_ok_GrvEhrlAphGlvvg:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%xmm30,%xmm0,%xmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%xmm31,%xmm0,%xmm0
+	vaesenclast	%xmm30,%xmm0,%xmm0
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%xmm29,%xmm17,%xmm17
+	vextracti32x4	$0,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_yetwpmsqegmkckb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_yetwpmsqegmkckb
+.L_small_initial_partial_block_yetwpmsqegmkckb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm0
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
+	vpslldq	$8,%xmm3,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm3
+
+
+	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
+	vpsrldq	$4,%xmm4,%xmm4
+	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14
+
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm7,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_yetwpmsqegmkckb
+.L_small_initial_compute_done_yetwpmsqegmkckb:
+.L_after_reduction_yetwpmsqegmkckb:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_2_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$254,%r15d
+	jae	.L_16_blocks_overflow_ApBCDyjzvGomesf
+	vpaddd	%ymm28,%ymm2,%ymm0
+	jmp	.L_16_blocks_ok_ApBCDyjzvGomesf
+
+.L_16_blocks_overflow_ApBCDyjzvGomesf:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%ymm29,%ymm0,%ymm0
+.L_16_blocks_ok_ApBCDyjzvGomesf:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%ymm30,%ymm0,%ymm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%ymm31,%ymm0,%ymm0
+	vaesenclast	%ymm30,%ymm0,%ymm0
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%ymm29,%ymm17,%ymm17
+	vextracti32x4	$1,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GavzefblbwwDDBb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GavzefblbwwDDBb
+.L_small_initial_partial_block_GavzefblbwwDDBb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GavzefblbwwDDBb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GavzefblbwwDDBb
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_GavzefblbwwDDBb:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_3_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$253,%r15d
+	jae	.L_16_blocks_overflow_usyAipEvpGwfEha
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_usyAipEvpGwfEha
+
+.L_16_blocks_overflow_usyAipEvpGwfEha:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_usyAipEvpGwfEha:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$2,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BGGdlvBiBuGnzfk
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BGGdlvBiBuGnzfk
+.L_small_initial_partial_block_BGGdlvBiBuGnzfk:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BGGdlvBiBuGnzfk:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BGGdlvBiBuGnzfk
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_BGGdlvBiBuGnzfk:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_4_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$252,%r15d
+	jae	.L_16_blocks_overflow_aanpAyhEgvmvoxx
+	vpaddd	%zmm28,%zmm2,%zmm0
+	jmp	.L_16_blocks_ok_aanpAyhEgvmvoxx
+
+.L_16_blocks_overflow_aanpAyhEgvmvoxx:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpshufb	%zmm29,%zmm0,%zmm0
+.L_16_blocks_ok_aanpAyhEgvmvoxx:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm17,%zmm17{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vextracti32x4	$3,%zmm17,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_vyfBBaGeegxpGFE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_vyfBBaGeegxpGFE
+.L_small_initial_partial_block_vyfBBaGeegxpGFE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpxorq	%zmm26,%zmm4,%zmm4
+	vpxorq	%zmm24,%zmm0,%zmm0
+	vpxorq	%zmm25,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_vyfBBaGeegxpGFE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_vyfBBaGeegxpGFE
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_vyfBBaGeegxpGFE:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_5_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$251,%r15d
+	jae	.L_16_blocks_overflow_GhEtuyjfFvGBsEs
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%xmm27,%xmm0,%xmm3
+	jmp	.L_16_blocks_ok_GhEtuyjfFvGBsEs
+
+.L_16_blocks_overflow_GhEtuyjfFvGBsEs:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+.L_16_blocks_ok_GhEtuyjfFvGBsEs:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%xmm30,%xmm3,%xmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%xmm31,%xmm3,%xmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%xmm30,%xmm3,%xmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%xmm29,%xmm19,%xmm19
+	vextracti32x4	$0,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_qehgbiEicBpcowb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_qehgbiEicBpcowb
+.L_small_initial_partial_block_qehgbiEicBpcowb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_qehgbiEicBpcowb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_qehgbiEicBpcowb
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_qehgbiEicBpcowb:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_6_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$250,%r15d
+	jae	.L_16_blocks_overflow_iBuzjtntEquxkyg
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%ymm27,%ymm0,%ymm3
+	jmp	.L_16_blocks_ok_iBuzjtntEquxkyg
+
+.L_16_blocks_overflow_iBuzjtntEquxkyg:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+.L_16_blocks_ok_iBuzjtntEquxkyg:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%ymm30,%ymm3,%ymm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%ymm31,%ymm3,%ymm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%ymm30,%ymm3,%ymm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%ymm29,%ymm19,%ymm19
+	vextracti32x4	$1,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_DhwvrFkproncnyF
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_DhwvrFkproncnyF
+.L_small_initial_partial_block_DhwvrFkproncnyF:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_DhwvrFkproncnyF:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_DhwvrFkproncnyF
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_DhwvrFkproncnyF:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_7_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$249,%r15d
+	jae	.L_16_blocks_overflow_abvkdfDAAdeddwo
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_abvkdfDAAdeddwo
+
+.L_16_blocks_overflow_abvkdfDAAdeddwo:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_abvkdfDAAdeddwo:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$2,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BGGpfiszevmfnDh
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BGGpfiszevmfnDh
+.L_small_initial_partial_block_BGGpfiszevmfnDh:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BGGpfiszevmfnDh:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BGGpfiszevmfnDh
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_BGGpfiszevmfnDh:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_8_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$64,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$248,%r15d
+	jae	.L_16_blocks_overflow_bmDcmBbjteawrab
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	jmp	.L_16_blocks_ok_bmDcmBbjteawrab
+
+.L_16_blocks_overflow_bmDcmBbjteawrab:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+.L_16_blocks_ok_bmDcmBbjteawrab:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm19,%zmm19{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vextracti32x4	$3,%zmm19,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_ijbcvbbrhpjreFD
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_ijbcvbbrhpjreFD
+.L_small_initial_partial_block_ijbcvbbrhpjreFD:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_ijbcvbbrhpjreFD:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_ijbcvbbrhpjreFD
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_ijbcvbbrhpjreFD:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_9_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$247,%r15d
+	jae	.L_16_blocks_overflow_ogvysBkmCFydlbB
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%xmm27,%xmm3,%xmm4
+	jmp	.L_16_blocks_ok_ogvysBkmCFydlbB
+
+.L_16_blocks_overflow_ogvysBkmCFydlbB:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+.L_16_blocks_ok_ogvysBkmCFydlbB:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%xmm30,%xmm4,%xmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%xmm31,%xmm4,%xmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%xmm30,%xmm4,%xmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%xmm20,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%xmm29,%xmm20,%xmm20
+	vextracti32x4	$0,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_nElyctmorwChisB
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_nElyctmorwChisB
+.L_small_initial_partial_block_nElyctmorwChisB:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_nElyctmorwChisB:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_nElyctmorwChisB
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_nElyctmorwChisB:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_10_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$246,%r15d
+	jae	.L_16_blocks_overflow_mdvtvoEGCkAgBAu
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%ymm27,%ymm3,%ymm4
+	jmp	.L_16_blocks_ok_mdvtvoEGCkAgBAu
+
+.L_16_blocks_overflow_mdvtvoEGCkAgBAu:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+.L_16_blocks_ok_mdvtvoEGCkAgBAu:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%ymm30,%ymm4,%ymm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%ymm31,%ymm4,%ymm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%ymm30,%ymm4,%ymm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%ymm20,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%ymm29,%ymm20,%ymm20
+	vextracti32x4	$1,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_yyAlkvDzAlffCkv
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_yyAlkvDzAlffCkv
+.L_small_initial_partial_block_yyAlkvDzAlffCkv:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_yyAlkvDzAlffCkv:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_yyAlkvDzAlffCkv
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_yyAlkvDzAlffCkv:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_11_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$245,%r15d
+	jae	.L_16_blocks_overflow_DbBntposxzGkmck
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_DbBntposxzGkmck
+
+.L_16_blocks_overflow_DbBntposxzGkmck:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_DbBntposxzGkmck:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$2,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_Fepodrdwccowklq
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_Fepodrdwccowklq
+.L_small_initial_partial_block_Fepodrdwccowklq:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_Fepodrdwccowklq:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_Fepodrdwccowklq
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_Fepodrdwccowklq:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_12_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$128,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$244,%r15d
+	jae	.L_16_blocks_overflow_qAqbwlEfnAaiigh
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	jmp	.L_16_blocks_ok_qAqbwlEfnAaiigh
+
+.L_16_blocks_overflow_qAqbwlEfnAaiigh:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+.L_16_blocks_ok_qAqbwlEfnAaiigh:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm20,%zmm20{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vextracti32x4	$3,%zmm20,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wuzubBAFnGjCiyC
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wuzubBAFnGjCiyC
+.L_small_initial_partial_block_wuzubBAFnGjCiyC:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vpxorq	%zmm8,%zmm0,%zmm8
+	vpxorq	%zmm22,%zmm3,%zmm22
+	vpxorq	%zmm30,%zmm4,%zmm30
+	vpxorq	%zmm31,%zmm5,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_wuzubBAFnGjCiyC:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_wuzubBAFnGjCiyC
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_wuzubBAFnGjCiyC:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_13_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$243,%r15d
+	jae	.L_16_blocks_overflow_wtlezDiEFkGbqGh
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%xmm27,%xmm4,%xmm5
+	jmp	.L_16_blocks_ok_wtlezDiEFkGbqGh
+
+.L_16_blocks_overflow_wtlezDiEFkGbqGh:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+.L_16_blocks_ok_wtlezDiEFkGbqGh:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%xmm30,%xmm5,%xmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%xmm31,%xmm5,%xmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%xmm30,%xmm5,%xmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%xmm21,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%xmm29,%xmm21,%xmm21
+	vextracti32x4	$0,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dxuhvdEzsauzrzC
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dxuhvdEzsauzrzC
+.L_small_initial_partial_block_dxuhvdEzsauzrzC:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	64(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	128(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	192(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+
+	vpxorq	%zmm26,%zmm30,%zmm30
+	vpxorq	%zmm24,%zmm8,%zmm8
+	vpxorq	%zmm25,%zmm22,%zmm22
+
+	vpxorq	%zmm31,%zmm30,%zmm30
+	vpsrldq	$8,%zmm30,%zmm4
+	vpslldq	$8,%zmm30,%zmm5
+	vpxorq	%zmm4,%zmm8,%zmm0
+	vpxorq	%zmm5,%zmm22,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_dxuhvdEzsauzrzC:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_dxuhvdEzsauzrzC
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_dxuhvdEzsauzrzC:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_14_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$242,%r15d
+	jae	.L_16_blocks_overflow_BpBowehoxtcCqab
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%ymm27,%ymm4,%ymm5
+	jmp	.L_16_blocks_ok_BpBowehoxtcCqab
+
+.L_16_blocks_overflow_BpBowehoxtcCqab:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+.L_16_blocks_ok_BpBowehoxtcCqab:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%ymm30,%ymm5,%ymm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%ymm31,%ymm5,%ymm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%ymm30,%ymm5,%ymm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%ymm21,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%ymm29,%ymm21,%ymm21
+	vextracti32x4	$1,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_lwjCvdwdnyAectx
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_lwjCvdwdnyAectx
+.L_small_initial_partial_block_lwjCvdwdnyAectx:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	48(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	112(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	176(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	240(%r10),%xmm1
+	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
+	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
+	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
+	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_lwjCvdwdnyAectx:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_lwjCvdwdnyAectx
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_lwjCvdwdnyAectx:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_15_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$241,%r15d
+	jae	.L_16_blocks_overflow_EyjzpDDCDaettqi
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_EyjzpDDCDaettqi
+
+.L_16_blocks_overflow_EyjzpDDCDaettqi:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_EyjzpDDCDaettqi:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$2,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_kwhksdEliCqnqvx
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_kwhksdEliCqnqvx
+.L_small_initial_partial_block_kwhksdEliCqnqvx:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	32(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	96(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	160(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	224(%r10),%ymm1
+	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
+	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
+	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
+	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_kwhksdEliCqnqvx:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_kwhksdEliCqnqvx
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_kwhksdEliCqnqvx:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_16_ElskehiCzqhzidf:
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r11
+	subq	$192,%r11
+	kmovq	(%r10,%r11,8),%k1
+	cmpl	$240,%r15d
+	jae	.L_16_blocks_overflow_lAvDEgcgGiAlspB
+	vpaddd	%zmm28,%zmm2,%zmm0
+	vpaddd	%zmm27,%zmm0,%zmm3
+	vpaddd	%zmm27,%zmm3,%zmm4
+	vpaddd	%zmm27,%zmm4,%zmm5
+	jmp	.L_16_blocks_ok_lAvDEgcgGiAlspB
+
+.L_16_blocks_overflow_lAvDEgcgGiAlspB:
+	vpshufb	%zmm29,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vmovdqa64	ddq_add_4444(%rip),%zmm5
+	vpaddd	%zmm5,%zmm0,%zmm3
+	vpaddd	%zmm5,%zmm3,%zmm4
+	vpaddd	%zmm5,%zmm4,%zmm5
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+.L_16_blocks_ok_lAvDEgcgGiAlspB:
+
+
+
+
+	vbroadcastf64x2	0(%rdi),%zmm30
+	vpxorq	768(%rsp),%zmm14,%zmm8
+	vmovdqu64	0(%rsp,%rbx,1),%zmm1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+
+
+	vbroadcastf64x2	16(%rdi),%zmm31
+	vmovdqu64	64(%rsp,%rbx,1),%zmm18
+	vmovdqa64	832(%rsp),%zmm22
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm30,%zmm3,%zmm3
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpxorq	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
+	vmovdqu64	128(%rsp,%rbx,1),%zmm1
+	vmovdqa64	896(%rsp),%zmm8
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm18
+	vmovdqa64	960(%rsp),%zmm22
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm30
+
+
+	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
+	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
+	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm31
+
+
+	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
+	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
+	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
+	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm30
+	vmovdqu8	0(%rcx,%rax,1),%zmm17
+	vmovdqu8	64(%rcx,%rax,1),%zmm19
+	vmovdqu8	128(%rcx,%rax,1),%zmm20
+	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm31
+
+
+	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
+	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
+	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
+	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm30
+	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
+	vpxorq	%zmm12,%zmm14,%zmm24
+	vpxorq	%zmm13,%zmm7,%zmm25
+	vpxorq	%zmm15,%zmm10,%zmm26
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm31
+	vaesenc	%zmm30,%zmm0,%zmm0
+	vaesenc	%zmm30,%zmm3,%zmm3
+	vaesenc	%zmm30,%zmm4,%zmm4
+	vaesenc	%zmm30,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm30
+	vaesenc	%zmm31,%zmm0,%zmm0
+	vaesenc	%zmm31,%zmm3,%zmm3
+	vaesenc	%zmm31,%zmm4,%zmm4
+	vaesenc	%zmm31,%zmm5,%zmm5
+	vaesenclast	%zmm30,%zmm0,%zmm0
+	vaesenclast	%zmm30,%zmm3,%zmm3
+	vaesenclast	%zmm30,%zmm4,%zmm4
+	vaesenclast	%zmm30,%zmm5,%zmm5
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vpxorq	%zmm20,%zmm4,%zmm4
+	vpxorq	%zmm21,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm11
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm21,%zmm21{%k1}{z}
+	vpshufb	%zmm29,%zmm17,%zmm17
+	vpshufb	%zmm29,%zmm19,%zmm19
+	vpshufb	%zmm29,%zmm20,%zmm20
+	vpshufb	%zmm29,%zmm21,%zmm21
+	vextracti32x4	$3,%zmm21,%xmm7
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_FvllnecehBftnGh:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm11,16(%rsi)
+	vmovdqu64	16(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
+	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
+	vmovdqu64	80(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
+	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
+	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
+	vmovdqu64	144(%r10),%zmm1
+	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
+	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
+	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
+	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
+	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
+	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
+	vmovdqu64	208(%r10),%ymm1
+	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
+	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
+	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
+	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
+	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3
+
+	vpxorq	%zmm30,%zmm4,%zmm4
+	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
+	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
+	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm30
+	vpslldq	$8,%zmm4,%zmm31
+	vpxorq	%zmm30,%zmm0,%zmm0
+	vpxorq	%zmm31,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm30
+	vpxorq	%ymm30,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm30
+	vpxorq	%xmm30,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm31
+	vpxorq	%ymm31,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm31
+	vpxorq	%xmm31,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm1
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_FvllnecehBftnGh:
+	vpxorq	%xmm7,%xmm14,%xmm14
+.L_after_reduction_FvllnecehBftnGh:
+	jmp	.L_last_blocks_done_ElskehiCzqhzidf
+.L_last_num_blocks_is_0_ElskehiCzqhzidf:
+	vmovdqa64	768(%rsp),%zmm13
+	vpxorq	%zmm14,%zmm13,%zmm13
+	vmovdqu64	0(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	832(%rsp),%zmm13
+	vmovdqu64	64(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+	vpxorq	%zmm10,%zmm4,%zmm26
+	vpxorq	%zmm6,%zmm0,%zmm24
+	vpxorq	%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+	vmovdqa64	896(%rsp),%zmm13
+	vmovdqu64	128(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
+	vmovdqa64	960(%rsp),%zmm13
+	vmovdqu64	192(%rsp,%rbx,1),%zmm12
+	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
+	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
+	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
+	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
+
+	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
+	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
+	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
+	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
+
+	vpsrldq	$8,%zmm26,%zmm0
+	vpslldq	$8,%zmm26,%zmm3
+	vpxorq	%zmm0,%zmm24,%zmm24
+	vpxorq	%zmm3,%zmm25,%zmm25
+	vextracti64x4	$1,%zmm24,%ymm0
+	vpxorq	%ymm0,%ymm24,%ymm24
+	vextracti32x4	$1,%ymm24,%xmm0
+	vpxorq	%xmm0,%xmm24,%xmm24
+	vextracti64x4	$1,%zmm25,%ymm3
+	vpxorq	%ymm3,%ymm25,%ymm25
+	vextracti32x4	$1,%ymm25,%xmm3
+	vpxorq	%xmm3,%xmm25,%xmm25
+	vmovdqa64	POLY2(%rip),%xmm4
+
+
+	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
+	vpslldq	$8,%xmm0,%xmm0
+	vpxorq	%xmm0,%xmm25,%xmm0
+
+
+	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
+	vpsrldq	$4,%xmm3,%xmm3
+	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14
+
+.L_last_blocks_done_ElskehiCzqhzidf:
+	vpshufb	%xmm29,%xmm2,%xmm2
+	jmp	.L_ghash_done_jitrpnrpcoxxhbG
+
+.L_message_below_equal_16_blocks_jitrpnrpcoxxhbG:
+
+
+	movl	%r8d,%r12d
+	addl	$15,%r12d
+	shrl	$4,%r12d
+	cmpq	$8,%r12
+	je	.L_small_initial_num_blocks_is_8_klqllxxEikEAFFD
+	jl	.L_small_initial_num_blocks_is_7_1_klqllxxEikEAFFD
+
+
+	cmpq	$12,%r12
+	je	.L_small_initial_num_blocks_is_12_klqllxxEikEAFFD
+	jl	.L_small_initial_num_blocks_is_11_9_klqllxxEikEAFFD
+
+
+	cmpq	$16,%r12
+	je	.L_small_initial_num_blocks_is_16_klqllxxEikEAFFD
+	cmpq	$15,%r12
+	je	.L_small_initial_num_blocks_is_15_klqllxxEikEAFFD
+	cmpq	$14,%r12
+	je	.L_small_initial_num_blocks_is_14_klqllxxEikEAFFD
+	jmp	.L_small_initial_num_blocks_is_13_klqllxxEikEAFFD
+
+.L_small_initial_num_blocks_is_11_9_klqllxxEikEAFFD:
+
+	cmpq	$11,%r12
+	je	.L_small_initial_num_blocks_is_11_klqllxxEikEAFFD
+	cmpq	$10,%r12
+	je	.L_small_initial_num_blocks_is_10_klqllxxEikEAFFD
+	jmp	.L_small_initial_num_blocks_is_9_klqllxxEikEAFFD
+
+.L_small_initial_num_blocks_is_7_1_klqllxxEikEAFFD:
+	cmpq	$4,%r12
+	je	.L_small_initial_num_blocks_is_4_klqllxxEikEAFFD
+	jl	.L_small_initial_num_blocks_is_3_1_klqllxxEikEAFFD
+
+	cmpq	$7,%r12
+	je	.L_small_initial_num_blocks_is_7_klqllxxEikEAFFD
+	cmpq	$6,%r12
+	je	.L_small_initial_num_blocks_is_6_klqllxxEikEAFFD
+	jmp	.L_small_initial_num_blocks_is_5_klqllxxEikEAFFD
+
+.L_small_initial_num_blocks_is_3_1_klqllxxEikEAFFD:
+
+	cmpq	$3,%r12
+	je	.L_small_initial_num_blocks_is_3_klqllxxEikEAFFD
+	cmpq	$2,%r12
+	je	.L_small_initial_num_blocks_is_2_klqllxxEikEAFFD
+
+
+
+
+
+.L_small_initial_num_blocks_is_1_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%xmm29
+	vpaddd	ONE(%rip),%xmm2,%xmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm0,%xmm2
+	vpshufb	%xmm29,%xmm0,%xmm0
+	vmovdqu8	0(%rcx,%rax,1),%xmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%xmm15,%xmm0,%xmm0
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%xmm15,%xmm0,%xmm0
+	vpxorq	%xmm6,%xmm0,%xmm0
+	vextracti32x4	$0,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%xmm29,%xmm6,%xmm6
+	vextracti32x4	$0,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_oddkauhsDBxhGEu
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_oddkauhsDBxhGEu
+.L_small_initial_partial_block_oddkauhsDBxhGEu:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+
+
+
+
+
+
+
+
+
+
+
+	vpxorq	%xmm13,%xmm14,%xmm14
+
+	jmp	.L_after_reduction_oddkauhsDBxhGEu
+.L_small_initial_compute_done_oddkauhsDBxhGEu:
+.L_after_reduction_oddkauhsDBxhGEu:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_2_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%ymm29
+	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
+	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm0,%xmm2
+	vpshufb	%ymm29,%ymm0,%ymm0
+	vmovdqu8	0(%rcx,%rax,1),%ymm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%ymm15,%ymm0,%ymm0
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%ymm15,%ymm0,%ymm0
+	vpxorq	%ymm6,%ymm0,%ymm0
+	vextracti32x4	$1,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%ymm29,%ymm6,%ymm6
+	vextracti32x4	$1,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (2 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_kjojqurabeEcxan
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_kjojqurabeEcxan
+.L_small_initial_partial_block_kjojqurabeEcxan:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_kjojqurabeEcxan:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_kjojqurabeEcxan
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_kjojqurabeEcxan:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_3_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm0,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vextracti32x4	$2,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vextracti32x4	$2,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (3 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_aGzlpmeocaDgebC
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_aGzlpmeocaDgebC
+.L_small_initial_partial_block_aGzlpmeocaDgebC:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_aGzlpmeocaDgebC:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_aGzlpmeocaDgebC
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_aGzlpmeocaDgebC:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_4_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm0,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vextracti32x4	$3,%zmm0,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm0,%zmm0{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vextracti32x4	$3,%zmm6,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (4 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_kzvcxoweasdwqok
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_kzvcxoweasdwqok
+.L_small_initial_partial_block_kzvcxoweasdwqok:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_kzvcxoweasdwqok:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_kzvcxoweasdwqok
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_kzvcxoweasdwqok:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_5_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%xmm29,%xmm3,%xmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%xmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%xmm15,%xmm3,%xmm3
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%xmm15,%xmm3,%xmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%xmm7,%xmm3,%xmm3
+	vextracti32x4	$0,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%xmm29,%xmm7,%xmm7
+	vextracti32x4	$0,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (5 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_pgmuaGzwpGbflhd
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_pgmuaGzwpGbflhd
+.L_small_initial_partial_block_pgmuaGzwpGbflhd:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_pgmuaGzwpGbflhd:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_pgmuaGzwpGbflhd
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_pgmuaGzwpGbflhd:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_6_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%ymm29,%ymm3,%ymm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%ymm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%ymm15,%ymm3,%ymm3
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%ymm15,%ymm3,%ymm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%ymm7,%ymm3,%ymm3
+	vextracti32x4	$1,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%ymm29,%ymm7,%ymm7
+	vextracti32x4	$1,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (6 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_dvydktyCoDwuGDp
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_dvydktyCoDwuGDp
+.L_small_initial_partial_block_dvydktyCoDwuGDp:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_dvydktyCoDwuGDp:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_dvydktyCoDwuGDp
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_dvydktyCoDwuGDp:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_7_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vextracti32x4	$2,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vextracti32x4	$2,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (7 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_AzmFisepEjvcsFd
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_AzmFisepEjvcsFd
+.L_small_initial_partial_block_AzmFisepEjvcsFd:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_AzmFisepEjvcsFd:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_AzmFisepEjvcsFd
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_AzmFisepEjvcsFd:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_8_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$64,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm3,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vextracti32x4	$3,%zmm3,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm3,%zmm3{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vextracti32x4	$3,%zmm7,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (8 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_GzEplmDeeBstBGz
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_GzEplmDeeBstBGz
+.L_small_initial_partial_block_GzEplmDeeBstBGz:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_GzEplmDeeBstBGz:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_GzEplmDeeBstBGz
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_GzEplmDeeBstBGz:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_9_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%xmm29,%xmm4,%xmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%xmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%xmm15,%xmm4,%xmm4
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%xmm15,%xmm4,%xmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%xmm10,%xmm4,%xmm4
+	vextracti32x4	$0,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%xmm29,%xmm10,%xmm10
+	vextracti32x4	$0,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (9 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_BexfEqdrtDDAAfE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_BexfEqdrtDDAAfE
+.L_small_initial_partial_block_BexfEqdrtDDAAfE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_BexfEqdrtDDAAfE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_BexfEqdrtDDAAfE
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_BexfEqdrtDDAAfE:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_10_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%ymm29,%ymm4,%ymm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%ymm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%ymm15,%ymm4,%ymm4
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%ymm15,%ymm4,%ymm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%ymm10,%ymm4,%ymm4
+	vextracti32x4	$1,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%ymm29,%ymm10,%ymm10
+	vextracti32x4	$1,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (10 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_Gewdupjzohoefas
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_Gewdupjzohoefas
+.L_small_initial_partial_block_Gewdupjzohoefas:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_Gewdupjzohoefas:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_Gewdupjzohoefas
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_Gewdupjzohoefas:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_11_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vextracti32x4	$2,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vextracti32x4	$2,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (11 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_fmzEejbhaipmgDE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_fmzEejbhaipmgDE
+.L_small_initial_partial_block_fmzEejbhaipmgDE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_fmzEejbhaipmgDE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_fmzEejbhaipmgDE
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_fmzEejbhaipmgDE:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_12_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$128,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm4,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vextracti32x4	$3,%zmm4,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm4,%zmm4{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vextracti32x4	$3,%zmm10,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (12 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_wdBBucsgpcjrkkw
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	64(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_wdBBucsgpcjrkkw
+.L_small_initial_partial_block_wdBBucsgpcjrkkw:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vpxorq	%zmm15,%zmm0,%zmm15
+	vpxorq	%zmm16,%zmm3,%zmm16
+	vpxorq	%zmm17,%zmm4,%zmm17
+	vpxorq	%zmm19,%zmm5,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_wdBBucsgpcjrkkw:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_wdBBucsgpcjrkkw
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_wdBBucsgpcjrkkw:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_13_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$0,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%xmm29,%xmm5,%xmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%xmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%xmm15,%xmm5,%xmm5
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%xmm15,%xmm5,%xmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%xmm11,%xmm5,%xmm5
+	vextracti32x4	$0,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%xmm29,%xmm11,%xmm11
+	vextracti32x4	$0,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (13 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_lszmapgyFmpsiof
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	48(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_lszmapgyFmpsiof
+.L_small_initial_partial_block_lszmapgyFmpsiof:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	64(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	128(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	192(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+
+	vpxorq	%zmm19,%zmm17,%zmm17
+	vpsrldq	$8,%zmm17,%zmm4
+	vpslldq	$8,%zmm17,%zmm5
+	vpxorq	%zmm4,%zmm15,%zmm0
+	vpxorq	%zmm5,%zmm16,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_lszmapgyFmpsiof:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_lszmapgyFmpsiof
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_lszmapgyFmpsiof:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_14_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$1,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%ymm29,%ymm5,%ymm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%ymm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%ymm15,%ymm5,%ymm5
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%ymm15,%ymm5,%ymm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%ymm11,%ymm5,%ymm5
+	vextracti32x4	$1,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%ymm29,%ymm11,%ymm11
+	vextracti32x4	$1,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (14 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_DCyjzvlowbyEvhb
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	32(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_DCyjzvlowbyEvhb
+.L_small_initial_partial_block_DCyjzvlowbyEvhb:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	48(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	112(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	176(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	240(%r10),%xmm20
+	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
+	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
+	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
+	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_DCyjzvlowbyEvhb:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_DCyjzvlowbyEvhb
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_DCyjzvlowbyEvhb:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_15_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$2,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%zmm15,%zmm5,%zmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%zmm11,%zmm5,%zmm5
+	vextracti32x4	$2,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vextracti32x4	$2,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (15 - 1),%r8
+
+
+	cmpq	$16,%r8
+	jl	.L_small_initial_partial_block_zljAoxDvFngeniE
+
+
+
+
+
+	subq	$16,%r8
+	movl	$0,(%rdx)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	16(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+	jmp	.L_small_initial_compute_done_zljAoxDvFngeniE
+.L_small_initial_partial_block_zljAoxDvFngeniE:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	32(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	96(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	160(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	224(%r10),%ymm20
+	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
+	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
+	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
+	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_zljAoxDvFngeniE:
+
+	orq	%r8,%r8
+	je	.L_after_reduction_zljAoxDvFngeniE
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_zljAoxDvFngeniE:
+	jmp	.L_small_initial_blocks_encrypted_klqllxxEikEAFFD
+.L_small_initial_num_blocks_is_16_klqllxxEikEAFFD:
+	vmovdqa64	SHUF_MASK(%rip),%zmm29
+	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
+	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
+	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
+	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
+	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
+	leaq	byte64_len_to_mask_table(%rip),%r10
+	movq	%r8,%r15
+	subq	$192,%r15
+	kmovq	(%r10,%r15,8),%k1
+	vextracti32x4	$3,%zmm5,%xmm2
+	vpshufb	%zmm29,%zmm0,%zmm0
+	vpshufb	%zmm29,%zmm3,%zmm3
+	vpshufb	%zmm29,%zmm4,%zmm4
+	vpshufb	%zmm29,%zmm5,%zmm5
+	vmovdqu8	0(%rcx,%rax,1),%zmm6
+	vmovdqu8	64(%rcx,%rax,1),%zmm7
+	vmovdqu8	128(%rcx,%rax,1),%zmm10
+	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
+	vbroadcastf64x2	0(%rdi),%zmm15
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm15,%zmm3,%zmm3
+	vpxorq	%zmm15,%zmm4,%zmm4
+	vpxorq	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	16(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	32(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	48(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	64(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	80(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	96(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	112(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	128(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	144(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	160(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	176(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	192(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	208(%rdi),%zmm15
+	vaesenc	%zmm15,%zmm0,%zmm0
+	vaesenc	%zmm15,%zmm3,%zmm3
+	vaesenc	%zmm15,%zmm4,%zmm4
+	vaesenc	%zmm15,%zmm5,%zmm5
+	vbroadcastf64x2	224(%rdi),%zmm15
+	vaesenclast	%zmm15,%zmm0,%zmm0
+	vaesenclast	%zmm15,%zmm3,%zmm3
+	vaesenclast	%zmm15,%zmm4,%zmm4
+	vaesenclast	%zmm15,%zmm5,%zmm5
+	vpxorq	%zmm6,%zmm0,%zmm0
+	vpxorq	%zmm7,%zmm3,%zmm3
+	vpxorq	%zmm10,%zmm4,%zmm4
+	vpxorq	%zmm11,%zmm5,%zmm5
+	vextracti32x4	$3,%zmm5,%xmm12
+	movq	%r9,%r10
+	vmovdqu8	%zmm0,0(%r10,%rax,1)
+	vmovdqu8	%zmm3,64(%r10,%rax,1)
+	vmovdqu8	%zmm4,128(%r10,%rax,1)
+	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
+	vmovdqu8	%zmm5,%zmm5{%k1}{z}
+	vpshufb	%zmm29,%zmm6,%zmm6
+	vpshufb	%zmm29,%zmm7,%zmm7
+	vpshufb	%zmm29,%zmm10,%zmm10
+	vpshufb	%zmm29,%zmm11,%zmm11
+	vextracti32x4	$3,%zmm11,%xmm13
+	leaq	96(%rsi),%r10
+	subq	$16 * (16 - 1),%r8
+.L_small_initial_partial_block_kymhysftDDvmBeF:
+
+
+
+
+
+
+
+
+	movl	%r8d,(%rdx)
+	vmovdqu64	%xmm12,16(%rsi)
+	vpxorq	%zmm14,%zmm6,%zmm6
+	vmovdqu64	16(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
+	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
+	vmovdqu64	80(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
+	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
+	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
+	vmovdqu64	144(%r10),%zmm20
+	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
+	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
+	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
+	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
+	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
+	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
+	vmovdqu64	208(%r10),%ymm20
+	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
+	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
+	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
+	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
+	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3
+
+	vpxorq	%zmm17,%zmm4,%zmm4
+	vpxorq	%zmm19,%zmm5,%zmm5
+	vpxorq	%zmm15,%zmm0,%zmm0
+	vpxorq	%zmm16,%zmm3,%zmm3
+
+	vpxorq	%zmm5,%zmm4,%zmm4
+	vpsrldq	$8,%zmm4,%zmm17
+	vpslldq	$8,%zmm4,%zmm19
+	vpxorq	%zmm17,%zmm0,%zmm0
+	vpxorq	%zmm19,%zmm3,%zmm3
+	vextracti64x4	$1,%zmm0,%ymm17
+	vpxorq	%ymm17,%ymm0,%ymm0
+	vextracti32x4	$1,%ymm0,%xmm17
+	vpxorq	%xmm17,%xmm0,%xmm0
+	vextracti64x4	$1,%zmm3,%ymm19
+	vpxorq	%ymm19,%ymm3,%ymm3
+	vextracti32x4	$1,%ymm3,%xmm19
+	vpxorq	%xmm19,%xmm3,%xmm3
+	vmovdqa64	POLY2(%rip),%xmm20
+
+
+	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
+	vpslldq	$8,%xmm4,%xmm4
+	vpxorq	%xmm4,%xmm3,%xmm4
+
+
+	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
+	vpsrldq	$4,%xmm5,%xmm5
+	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
+	vpslldq	$4,%xmm14,%xmm14
+	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14
+
+.L_small_initial_compute_done_kymhysftDDvmBeF:
+	vpxorq	%xmm13,%xmm14,%xmm14
+.L_after_reduction_kymhysftDDvmBeF:
+.L_small_initial_blocks_encrypted_klqllxxEikEAFFD:
+.L_ghash_done_jitrpnrpcoxxhbG:
+	vmovdqu64	%xmm2,0(%rsi)
+.L_enc_dec_done_jitrpnrpcoxxhbG:
+
+	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
+	vmovdqu64	%xmm14,64(%rsi)
+.L_enc_dec_abort_jitrpnrpcoxxhbG:
+	jmp	.Lexit_gcm_decrypt
+.Lexit_gcm_decrypt:
+	cmpq	$256,%r8
+	jbe	.Lskip_hkeys_cleanup_lbyznGuhihanprD
+	vpxor	%xmm0,%xmm0,%xmm0
+	vmovdqa64	%zmm0,0(%rsp)
+	vmovdqa64	%zmm0,64(%rsp)
+	vmovdqa64	%zmm0,128(%rsp)
+	vmovdqa64	%zmm0,192(%rsp)
+	vmovdqa64	%zmm0,256(%rsp)
+	vmovdqa64	%zmm0,320(%rsp)
+	vmovdqa64	%zmm0,384(%rsp)
+	vmovdqa64	%zmm0,448(%rsp)
+	vmovdqa64	%zmm0,512(%rsp)
+	vmovdqa64	%zmm0,576(%rsp)
+	vmovdqa64	%zmm0,640(%rsp)
+	vmovdqa64	%zmm0,704(%rsp)
+.Lskip_hkeys_cleanup_lbyznGuhihanprD:
+	vzeroupper
+	leaq	(%rbp),%rsp
+.cfi_def_cfa_register	%rsp
+	popq	%r15
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r15
+	popq	%r14
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r14
+	popq	%r13
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r13
+	popq	%r12
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%r12
+	popq	%rbp
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%rbp
+	popq	%rbx
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%rbx
+	.byte	0xf3,0xc3
+.Ldecrypt_seh_end:
+.cfi_endproc	
+.size	aes_gcm_decrypt_avx512, .-aes_gcm_decrypt_avx512
+.data	
+.align	16
+POLY:.quad	0x0000000000000001, 0xC200000000000000
+
+.align	64
+POLY2:
+.quad	0x00000001C2000000, 0xC200000000000000
+.quad	0x00000001C2000000, 0xC200000000000000
+.quad	0x00000001C2000000, 0xC200000000000000
+.quad	0x00000001C2000000, 0xC200000000000000
+
+.align	16
+TWOONE:.quad	0x0000000000000001, 0x0000000100000000
+
+
+
+.align	64
+SHUF_MASK:
+.quad	0x08090A0B0C0D0E0F, 0x0001020304050607
+.quad	0x08090A0B0C0D0E0F, 0x0001020304050607
+.quad	0x08090A0B0C0D0E0F, 0x0001020304050607
+.quad	0x08090A0B0C0D0E0F, 0x0001020304050607
+
+.align	16
+SHIFT_MASK:
+.quad	0x0706050403020100, 0x0f0e0d0c0b0a0908
+
+ALL_F:
+.quad	0xffffffffffffffff, 0xffffffffffffffff
+
+ZERO:
+.quad	0x0000000000000000, 0x0000000000000000
+
+.align	16
+ONE:
+.quad	0x0000000000000001, 0x0000000000000000
+
+.align	16
+ONEf:
+.quad	0x0000000000000000, 0x0100000000000000
+
+.align	64
+ddq_add_1234:
+.quad	0x0000000000000001, 0x0000000000000000
+.quad	0x0000000000000002, 0x0000000000000000
+.quad	0x0000000000000003, 0x0000000000000000
+.quad	0x0000000000000004, 0x0000000000000000
+
+.align	64
+ddq_add_5678:
+.quad	0x0000000000000005, 0x0000000000000000
+.quad	0x0000000000000006, 0x0000000000000000
+.quad	0x0000000000000007, 0x0000000000000000
+.quad	0x0000000000000008, 0x0000000000000000
+
+.align	64
+ddq_add_4444:
+.quad	0x0000000000000004, 0x0000000000000000
+.quad	0x0000000000000004, 0x0000000000000000
+.quad	0x0000000000000004, 0x0000000000000000
+.quad	0x0000000000000004, 0x0000000000000000
+
+.align	64
+ddq_add_8888:
+.quad	0x0000000000000008, 0x0000000000000000
+.quad	0x0000000000000008, 0x0000000000000000
+.quad	0x0000000000000008, 0x0000000000000000
+.quad	0x0000000000000008, 0x0000000000000000
+
+.align	64
+ddq_addbe_1234:
+.quad	0x0000000000000000, 0x0100000000000000
+.quad	0x0000000000000000, 0x0200000000000000
+.quad	0x0000000000000000, 0x0300000000000000
+.quad	0x0000000000000000, 0x0400000000000000
+
+.align	64
+ddq_addbe_4444:
+.quad	0x0000000000000000, 0x0400000000000000
+.quad	0x0000000000000000, 0x0400000000000000
+.quad	0x0000000000000000, 0x0400000000000000
+.quad	0x0000000000000000, 0x0400000000000000
+
+.align	64
+byte_len_to_mask_table:
+.value	0x0000, 0x0001, 0x0003, 0x0007
+.value	0x000f, 0x001f, 0x003f, 0x007f
+.value	0x00ff, 0x01ff, 0x03ff, 0x07ff
+.value	0x0fff, 0x1fff, 0x3fff, 0x7fff
+.value	0xffff
+
+.align	64
+byte64_len_to_mask_table:
+.quad	0x0000000000000000, 0x0000000000000001
+.quad	0x0000000000000003, 0x0000000000000007
+.quad	0x000000000000000f, 0x000000000000001f
+.quad	0x000000000000003f, 0x000000000000007f
+.quad	0x00000000000000ff, 0x00000000000001ff
+.quad	0x00000000000003ff, 0x00000000000007ff
+.quad	0x0000000000000fff, 0x0000000000001fff
+.quad	0x0000000000003fff, 0x0000000000007fff
+.quad	0x000000000000ffff, 0x000000000001ffff
+.quad	0x000000000003ffff, 0x000000000007ffff
+.quad	0x00000000000fffff, 0x00000000001fffff
+.quad	0x00000000003fffff, 0x00000000007fffff
+.quad	0x0000000000ffffff, 0x0000000001ffffff
+.quad	0x0000000003ffffff, 0x0000000007ffffff
+.quad	0x000000000fffffff, 0x000000001fffffff
+.quad	0x000000003fffffff, 0x000000007fffffff
+.quad	0x00000000ffffffff, 0x00000001ffffffff
+.quad	0x00000003ffffffff, 0x00000007ffffffff
+.quad	0x0000000fffffffff, 0x0000001fffffffff
+.quad	0x0000003fffffffff, 0x0000007fffffffff
+.quad	0x000000ffffffffff, 0x000001ffffffffff
+.quad	0x000003ffffffffff, 0x000007ffffffffff
+.quad	0x00000fffffffffff, 0x00001fffffffffff
+.quad	0x00003fffffffffff, 0x00007fffffffffff
+.quad	0x0000ffffffffffff, 0x0001ffffffffffff
+.quad	0x0003ffffffffffff, 0x0007ffffffffffff
+.quad	0x000fffffffffffff, 0x001fffffffffffff
+.quad	0x003fffffffffffff, 0x007fffffffffffff
+.quad	0x00ffffffffffffff, 0x01ffffffffffffff
+.quad	0x03ffffffffffffff, 0x07ffffffffffffff
+.quad	0x0fffffffffffffff, 0x1fffffffffffffff
+.quad	0x3fffffffffffffff, 0x7fffffffffffffff
+.quad	0xffffffffffffffff
+#endif
+.section	.note.GNU-stack,"",@progbits
diff -Naur boringssl/sources.json boringssl-patched/sources.json
--- boringssl/sources.json	2022-07-20 05:35:27.000000000 +0800
+++ boringssl-patched/sources.json	2023-08-07 15:03:54.000000000 +0800
@@ -484,7 +484,8 @@
     "linux-x86_64/crypto/chacha/chacha-x86_64.S", 
     "linux-x86_64/crypto/cipher_extra/aes128gcmsiv-x86_64.S", 
     "linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S", 
-    "linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S", 
+    "linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S",
+    "linux-x86_64/crypto/fipsmodule/aes-gcm-avx512.S", 
     "linux-x86_64/crypto/fipsmodule/aesni-x86_64.S", 
     "linux-x86_64/crypto/fipsmodule/ghash-ssse3-x86_64.S", 
     "linux-x86_64/crypto/fipsmodule/ghash-x86_64.S", 
@@ -1003,4 +1004,4 @@
   "urandom_test": [
     "src/crypto/fipsmodule/rand/urandom_test.cc"
   ]
-}
\ No newline at end of file
+}
diff -Naur boringssl/src/crypto/fipsmodule/CMakeLists.txt boringssl-patched/src/crypto/fipsmodule/CMakeLists.txt
--- boringssl/src/crypto/fipsmodule/CMakeLists.txt	2022-07-20 05:35:27.000000000 +0800
+++ boringssl-patched/src/crypto/fipsmodule/CMakeLists.txt	2023-08-07 15:03:54.000000000 +0800
@@ -4,6 +4,7 @@
   set(
     BCM_ASM_SOURCES
 
+    aes-gcm-avx512.${ASM_EXT}
     aesni-gcm-x86_64.${ASM_EXT}
     aesni-x86_64.${ASM_EXT}
     ghash-ssse3-x86_64.${ASM_EXT}
@@ -83,6 +84,7 @@
 endif()
 
 perlasm(aesni-gcm-x86_64.${ASM_EXT} modes/asm/aesni-gcm-x86_64.pl)
+perlasm(aes-gcm-avx512.${ASM_EXT} modes/asm/aes-gcm-avx512.pl)
 perlasm(aesni-x86_64.${ASM_EXT} aes/asm/aesni-x86_64.pl)
 perlasm(aesni-x86.${ASM_EXT} aes/asm/aesni-x86.pl)
 perlasm(aesp8-ppc.${ASM_EXT} aes/asm/aesp8-ppc.pl)
diff -Naur boringssl/src/crypto/fipsmodule/modes/asm/aes-gcm-avx512.pl boringssl-patched/src/crypto/fipsmodule/modes/asm/aes-gcm-avx512.pl
--- boringssl/src/crypto/fipsmodule/modes/asm/aes-gcm-avx512.pl	1970-01-01 08:00:00.000000000 +0800
+++ boringssl-patched/src/crypto/fipsmodule/modes/asm/aes-gcm-avx512.pl	2023-08-07 15:03:54.000000000 +0800
@@ -0,0 +1,4867 @@
+# Copyright (C) 2022 Intel Corporation
+#
+# SPDX-License-Identifier: Apache-2.0
+#
+# This implementation is based on the AES-GCM code (AVX512VAES + VPCLMULQDQ)
+# from Intel(R) Multi-Buffer Crypto for IPsec Library v1.1
+# (https://github.com/intel/intel-ipsec-mb).
+# Original author is Tomasz Kantecki <tomasz.kantecki@intel.com>.
+#
+# References:
+#  [1] Vinodh Gopal et. al. Optimized Galois-Counter-Mode Implementation on
+#      Intel Architecture Processors. August, 2010.
+#  [2] Erdinc Ozturk et. al. Enabling High-Performance Galois-Counter-Mode on
+#      Intel Architecture Processors. October, 2012.
+#  [3] Shay Gueron et. al. Intel Carry-Less Multiplication Instruction and its
+#      Usage for Computing the GCM Mode. May, 2010.
+#
+#
+# March 2022
+#
+# Initial release.
+#
+# GCM128_CONTEXT structure has storage for 16 hkeys only, but this
+# implementation can use up to 48.  To avoid extending the context size,
+# precompute and store in the context first 16 hkeys only, and compute the rest
+# on demand keeping them in the local frame.
+#
+#======================================================================
+# $output is the last argument if it looks like a file (it has an extension)
+# $flavour is the first argument if it doesn't look like a file
+$output  = $#ARGV >= 0 && $ARGV[$#ARGV] =~ m|\.\w+$| ? pop   : undef;
+$flavour = $#ARGV >= 0 && $ARGV[0] !~ m|\.|          ? shift : undef;
+
+$win64 = 0;
+$win64 = 1 if ($flavour =~ /[nm]asm|mingw64/ || $output =~ /\.asm$/);
+
+$avx512vaes = 1;
+
+$0 =~ m/(.*[\/\\])[^\/\\]+$/;
+$dir = $1;
+($xlate = "${dir}x86_64-xlate.pl" and -f $xlate)
+  or ($xlate = "${dir}../../../perlasm/x86_64-xlate.pl" and -f $xlate)
+  or die "can't locate x86_64-xlate.pl";
+
+open OUT, "| \"$^X\" \"$xlate\" $flavour \"$output\""
+  or die "can't call $xlate: $!";
+*STDOUT = *OUT;
+
+#======================================================================
+if ($avx512vaes>0) { #<<<
+
+# ; Mapping key length -> AES rounds count
+my %aes_rounds = (
+  128 => 9,
+  192 => 11,
+  256 => 13);
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;;; Code generation control switches
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+# ; ABI-aware zeroing of volatile registers in EPILOG().
+# ; Disabled due to performance reasons.
+my $CLEAR_SCRATCH_REGISTERS = 0;
+
+# ; Zero HKeys storage from the stack if they are stored there
+my $CLEAR_HKEYS_STORAGE_ON_EXIT = 1;
+
+# ; Enable / disable check of function arguments for null pointer
+# ; Currently disabled, as this check is handled outside.
+my $CHECK_FUNCTION_ARGUMENTS = 0;
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;;; Global constants
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+# AES block size in bytes
+my $AES_BLOCK_SIZE = 16;
+
+# Storage capacity in elements
+my $HKEYS_STORAGE_CAPACITY = 48;
+my $LOCAL_STORAGE_CAPACITY = 48;
+my $HKEYS_CONTEXT_CAPACITY = 16;
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;;; Stack frame definition
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+# (1) -> +64(Win)/+48(Lin)-byte space for pushed GPRs
+# (2) -> +8-byte space for 16-byte alignment of XMM storage
+# (3) -> Frame pointer (%RBP)
+# (4) -> +160-byte XMM storage (Windows only, zero on Linux)
+# (5) -> +48-byte space for 64-byte alignment of %RSP from p.8
+# (6) -> +768-byte LOCAL storage (optional, can be omitted in some functions)
+# (7) -> +768-byte HKEYS storage
+# (8) -> Stack pointer (%RSP) aligned on 64-byte boundary
+
+my $GP_STORAGE  = $win64 ? 8 * 8     : 8 * 6;    # ; space for saved non-volatile GP registers (pushed on stack)
+my $XMM_STORAGE = $win64 ? (10 * 16) : 0;        # ; space for saved XMM registers
+my $HKEYS_STORAGE = ($HKEYS_STORAGE_CAPACITY * $AES_BLOCK_SIZE);    # ; space for HKeys^i, i=1..48
+my $LOCAL_STORAGE = ($LOCAL_STORAGE_CAPACITY * $AES_BLOCK_SIZE);    # ; space for up to 48 AES blocks
+
+my $STACK_HKEYS_OFFSET = 0;
+my $STACK_LOCAL_OFFSET = ($STACK_HKEYS_OFFSET + $HKEYS_STORAGE);
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;;; Function arguments abstraction
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+my ($arg1, $arg2, $arg3, $arg4, $arg5, $arg6, $arg7, $arg8, $arg9, $arg10, $arg11);
+
+# ; This implementation follows the convention: for non-leaf functions (they
+# ; must call PROLOG) %rbp is used as a frame pointer, and has fixed offset from
+# ; the function entry: $GP_STORAGE + [8 bytes alignment (Windows only)].  This
+# ; helps to facilitate SEH handlers writing.
+#
+# ; Leaf functions here do not use more than 4 input arguments.
+if ($win64) {
+  $arg1  = "%rcx";
+  $arg2  = "%rdx";
+  $arg3  = "%r8";
+  $arg4  = "%r9";
+  $arg5  = "`$GP_STORAGE + 8 + 8*5`(%rbp)";    # +8 - alignment bytes
+  $arg6  = "`$GP_STORAGE + 8 + 8*6`(%rbp)";
+  $arg7  = "`$GP_STORAGE + 8 + 8*7`(%rbp)";
+  $arg8  = "`$GP_STORAGE + 8 + 8*8`(%rbp)";
+  $arg9  = "`$GP_STORAGE + 8 + 8*9`(%rbp)";
+  $arg10 = "`$GP_STORAGE + 8 + 8*10`(%rbp)";
+  $arg11 = "`$GP_STORAGE + 8 + 8*11`(%rbp)";
+} else {
+  $arg1  = "%rdi";
+  $arg2  = "%rsi";
+  $arg3  = "%rdx";
+  $arg4  = "%rcx";
+  $arg5  = "%r8";
+  $arg6  = "%r9";
+  $arg7  = "`$GP_STORAGE + 8*1`(%rbp)";
+  $arg8  = "`$GP_STORAGE + 8*2`(%rbp)";
+  $arg9  = "`$GP_STORAGE + 8*3`(%rbp)";
+  $arg10 = "`$GP_STORAGE + 8*4`(%rbp)";
+  $arg11 = "`$GP_STORAGE + 8*5`(%rbp)";
+}
+
+# ; Offsets in gcm128_context structure (see crypto/fipsmodule/modes/modes.h)
+my $CTX_OFFSET_CurCount  = (16 * 0);          #  ; (Yi) Current counter for generation of encryption key
+my $CTX_OFFSET_PEncBlock = (16 * 1);          #  ; (repurposed EKi field) Partial block buffer
+my $CTX_OFFSET_EK0       = (16 * 2);          #  ; (EK0) Encrypted Y0 counter (see gcm spec notation)
+my $CTX_OFFSET_AadLen    = (16 * 3);          #  ; (len.u[0]) Length of Hash which has been input
+my $CTX_OFFSET_InLen     = ((16 * 3) + 8);    #  ; (len.u[1]) Length of input data which will be encrypted or decrypted
+my $CTX_OFFSET_AadHash   = (16 * 4);          #  ; (Xi) Current hash
+my $CTX_OFFSET_HTable    = (16 * 6);          #  ; (Htable) Precomputed table (allows 16 values)
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;;; Helper functions
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+# ; Generates "random" local labels
+sub random_string() {
+  my @chars  = ('a' .. 'z', 'A' .. 'Z', '0' .. '9', '_');
+  my $length = 15;
+  my $str;
+  map { $str .= $chars[rand(33)] } 1 .. $length;
+  return $str;
+}
+
+sub BYTE {
+  my ($reg) = @_;
+  if ($reg =~ /%r[abcd]x/i) {
+    $reg =~ s/%r([abcd])x/%${1}l/i;
+  } elsif ($reg =~ /%r[sdb][ip]/i) {
+    $reg =~ s/%r([sdb][ip])/%${1}l/i;
+  } elsif ($reg =~ /%r[0-9]{1,2}/i) {
+    $reg =~ s/%(r[0-9]{1,2})/%${1}b/i;
+  } else {
+    die "BYTE: unknown register: $reg\n";
+  }
+  return $reg;
+}
+
+sub WORD {
+  my ($reg) = @_;
+  if ($reg =~ /%r[abcdsdb][xip]/i) {
+    $reg =~ s/%r([abcdsdb])([xip])/%${1}${2}/i;
+  } elsif ($reg =~ /%r[0-9]{1,2}/) {
+    $reg =~ s/%(r[0-9]{1,2})/%${1}w/i;
+  } else {
+    die "WORD: unknown register: $reg\n";
+  }
+  return $reg;
+}
+
+sub DWORD {
+  my ($reg) = @_;
+  if ($reg =~ /%r[abcdsdb][xip]/i) {
+    $reg =~ s/%r([abcdsdb])([xip])/%e${1}${2}/i;
+  } elsif ($reg =~ /%r[0-9]{1,2}/i) {
+    $reg =~ s/%(r[0-9]{1,2})/%${1}d/i;
+  } else {
+    die "DWORD: unknown register: $reg\n";
+  }
+  return $reg;
+}
+
+sub XWORD {
+  my ($reg) = @_;
+  if ($reg =~ /%[xyz]mm/i) {
+    $reg =~ s/%[xyz]mm/%xmm/i;
+  } else {
+    die "XWORD: unknown register: $reg\n";
+  }
+  return $reg;
+}
+
+sub YWORD {
+  my ($reg) = @_;
+  if ($reg =~ /%[xyz]mm/i) {
+    $reg =~ s/%[xyz]mm/%ymm/i;
+  } else {
+    die "YWORD: unknown register: $reg\n";
+  }
+  return $reg;
+}
+
+sub ZWORD {
+  my ($reg) = @_;
+  if ($reg =~ /%[xyz]mm/i) {
+    $reg =~ s/%[xyz]mm/%zmm/i;
+  } else {
+    die "ZWORD: unknown register: $reg\n";
+  }
+  return $reg;
+}
+
+# ; Helper function to construct effective address based on two kinds of
+# ; offsets: numerical or located in the register
+sub EffectiveAddress {
+  my ($base, $offset, $displacement) = @_;
+  $displacement = 0 if (!$displacement);
+
+  if ($offset =~ /^\d+\z/) {    # numerical offset
+    return "`$offset + $displacement`($base)";
+  } else {                      # offset resides in register
+    return "$displacement($base,$offset,1)";
+  }
+}
+
+# ; Provides memory location of corresponding HashKey power
+sub HashKeyByIdx {
+  my ($idx, $base) = @_;
+  my $base_str = ($base eq "%rsp") ? "frame" : "context";
+
+  my $offset = &HashKeyOffsetByIdx($idx, $base_str);
+  return "$offset($base)";
+}
+
+# ; Provides offset (in bytes) of corresponding HashKey power from the highest key in the storage
+sub HashKeyOffsetByIdx {
+  my ($idx, $base) = @_;
+  die "HashKeyOffsetByIdx: base should be either 'frame' or 'context'; base = $base"
+    if (($base ne "frame") && ($base ne "context"));
+
+  my $offset_base;
+  my $offset_idx;
+  if ($base eq "frame") {    # frame storage
+    die "HashKeyOffsetByIdx: idx out of bounds (1..48)! idx = $idx\n" if ($idx > $HKEYS_STORAGE_CAPACITY || $idx < 1);
+    $offset_base = $STACK_HKEYS_OFFSET;
+    $offset_idx  = ($AES_BLOCK_SIZE * ($HKEYS_STORAGE_CAPACITY - $idx));
+  } else {                   # context storage
+    die "HashKeyOffsetByIdx: idx out of bounds (1..16)! idx = $idx\n" if ($idx > $HKEYS_CONTEXT_CAPACITY || $idx < 1);
+    $offset_base = 0;
+    $offset_idx  = ($AES_BLOCK_SIZE * ($HKEYS_CONTEXT_CAPACITY - $idx));
+  }
+  return $offset_base + $offset_idx;
+}
+
+# ; Creates local frame and does back up of non-volatile registers.
+# ; Holds stack unwinding directives.
+sub PROLOG {
+  my ($need_hkeys_stack_storage, $need_aes_stack_storage, $func_name) = @_;
+
+  my $DYNAMIC_STACK_ALLOC_SIZE            = 0;
+  my $DYNAMIC_STACK_ALLOC_ALIGNMENT_SPACE = $win64 ? 48 : 52;
+
+  if ($need_hkeys_stack_storage) {
+    $DYNAMIC_STACK_ALLOC_SIZE += $HKEYS_STORAGE;
+  }
+
+  if ($need_aes_stack_storage) {
+    if (!$need_hkeys_stack_storage) {
+      die "PROLOG: unsupported case - aes storage without hkeys one";
+    }
+    $DYNAMIC_STACK_ALLOC_SIZE += $LOCAL_STORAGE;
+  }
+
+  $code .= <<___;
+    push    %rbx
+.cfi_push   %rbx
+.L${func_name}_seh_push_rbx:
+    push    %rbp
+.cfi_push   %rbp
+.L${func_name}_seh_push_rbp:
+    push    %r12
+.cfi_push   %r12
+.L${func_name}_seh_push_r12:
+    push    %r13
+.cfi_push   %r13
+.L${func_name}_seh_push_r13:
+    push    %r14
+.cfi_push   %r14
+.L${func_name}_seh_push_r14:
+    push    %r15
+.cfi_push   %r15
+.L${func_name}_seh_push_r15:
+___
+
+  if ($win64) {
+    $code .= <<___;
+    push    %rdi
+.L${func_name}_seh_push_rdi:
+    push    %rsi
+.L${func_name}_seh_push_rsi:
+
+    sub     \$`$XMM_STORAGE+8`,%rsp   # +8 alignment
+.L${func_name}_seh_allocstack_xmm:
+___
+  }
+  $code .= <<___;
+    # ; %rbp contains stack pointer right after GP regs pushed at stack + [8
+    # ; bytes of alignment (Windows only)].  It serves as a frame pointer in SEH
+    # ; handlers. The requirement for a frame pointer is that its offset from
+    # ; RSP shall be multiple of 16, and not exceed 240 bytes. The frame pointer
+    # ; itself seems to be reasonable to use here, because later we do 64-byte stack
+    # ; alignment which gives us non-determinate offsets and complicates writing
+    # ; SEH handlers.
+    #
+    # ; It also serves as an anchor for retrieving stack arguments on both Linux
+    # ; and Windows.
+    lea     `$XMM_STORAGE`(%rsp),%rbp
+.cfi_def_cfa_register %rbp
+.L${func_name}_seh_setfp:
+___
+  if ($win64) {
+
+    # ; xmm6:xmm15 need to be preserved on Windows
+    foreach my $reg_idx (6 .. 15) {
+      my $xmm_reg_offset = ($reg_idx - 6) * 16;
+      $code .= <<___;
+        vmovdqu           %xmm${reg_idx},$xmm_reg_offset(%rsp)
+.L${func_name}_seh_save_xmm${reg_idx}:
+___
+    }
+  }
+
+  $code .= <<___;
+# Prolog ends here. Next stack allocation is treated as "dynamic".
+.L${func_name}_seh_prolog_end:
+___
+
+  if ($DYNAMIC_STACK_ALLOC_SIZE) {
+    $code .= <<___;
+        sub               \$`$DYNAMIC_STACK_ALLOC_SIZE + $DYNAMIC_STACK_ALLOC_ALIGNMENT_SPACE`,%rsp
+        and               \$(-64),%rsp
+___
+  }
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;;; Restore register content for the caller.
+# ;;; And cleanup stack.
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+sub EPILOG {
+  my ($hkeys_storage_on_stack, $payload_len) = @_;
+
+  my $rndsuffix = &random_string();
+
+  if ($hkeys_storage_on_stack && $CLEAR_HKEYS_STORAGE_ON_EXIT) {
+
+    # ; There is no need in hkeys cleanup if payload len was small, i.e. no hkeys
+    # ; were stored in the local frame storage
+    $code .= <<___;
+        cmpq              \$`16*16`,$payload_len
+        jbe               .Lskip_hkeys_cleanup_${rndsuffix}
+        vpxor             %xmm0,%xmm0,%xmm0
+___
+    for (my $i = 0; $i < int($HKEYS_STORAGE / 64); $i++) {
+      $code .= "vmovdqa64         %zmm0,`$STACK_HKEYS_OFFSET + 64*$i`(%rsp)\n";
+    }
+    $code .= ".Lskip_hkeys_cleanup_${rndsuffix}:\n";
+  }
+
+  if ($CLEAR_SCRATCH_REGISTERS) {
+    &clear_scratch_gps_asm();
+    &clear_scratch_zmms_asm();
+  } else {
+    $code .= "vzeroupper\n";
+  }
+
+  if ($win64) {
+
+    # ; restore xmm15:xmm6
+    for (my $reg_idx = 15; $reg_idx >= 6; $reg_idx--) {
+      my $xmm_reg_offset = -$XMM_STORAGE + ($reg_idx - 6) * 16;
+      $code .= <<___;
+        vmovdqu           $xmm_reg_offset(%rbp),%xmm${reg_idx},
+___
+    }
+  }
+
+  if ($win64) {
+
+    # Forming valid epilog for SEH with use of frame pointer.
+    # https://docs.microsoft.com/en-us/cpp/build/prolog-and-epilog?view=msvc-160#epilog-code
+    $code .= "lea      8(%rbp),%rsp\n";
+  } else {
+    $code .= "lea      (%rbp),%rsp\n";
+    $code .= ".cfi_def_cfa_register %rsp\n";
+  }
+
+  if ($win64) {
+    $code .= <<___;
+     pop     %rsi
+.cfi_pop     %rsi
+     pop     %rdi
+.cfi_pop     %rdi
+___
+  }
+  $code .= <<___;
+     pop     %r15
+.cfi_pop     %r15
+     pop     %r14
+.cfi_pop     %r14
+     pop     %r13
+.cfi_pop     %r13
+     pop     %r12
+.cfi_pop     %r12
+     pop     %rbp
+.cfi_pop     %rbp
+     pop     %rbx
+.cfi_pop     %rbx
+___
+}
+
+# ; Clears all scratch ZMM registers
+# ;
+# ; It should be called before restoring the XMM registers
+# ; for Windows (XMM6-XMM15).
+# ;
+sub clear_scratch_zmms_asm {
+
+  # ; On Linux, all ZMM registers are scratch registers
+  if (!$win64) {
+    $code .= "vzeroall\n";
+  } else {
+    foreach my $i (0 .. 5) {
+      $code .= "vpxorq  %xmm${i},%xmm${i},%xmm${i}\n";
+    }
+  }
+  foreach my $i (16 .. 31) {
+    $code .= "vpxorq  %xmm${i},%xmm${i},%xmm${i}\n";
+  }
+}
+
+# Clears all scratch GP registers
+sub clear_scratch_gps_asm {
+  foreach my $reg ("%rax", "%rcx", "%rdx", "%r8", "%r9", "%r10", "%r11") {
+    $code .= "xor $reg,$reg\n";
+  }
+  if (!$win64) {
+    foreach my $reg ("%rsi", "%rdi") {
+      $code .= "xor $reg,$reg\n";
+    }
+  }
+}
+
+sub precompute_hkeys_on_stack {
+  my $HTABLE      = $_[0];
+  my $HKEYS_READY = $_[1];
+  my $ZTMP0       = $_[2];
+  my $ZTMP1       = $_[3];
+  my $ZTMP2       = $_[4];
+  my $ZTMP3       = $_[5];
+  my $ZTMP4       = $_[6];
+  my $ZTMP5       = $_[7];
+  my $ZTMP6       = $_[8];
+  my $HKEYS_RANGE = $_[9];    # ; "first16", "mid16", "all", "first32", "last32"
+
+  die "precompute_hkeys_on_stack: Unexpected value of HKEYS_RANGE: $HKEYS_RANGE"
+    if ($HKEYS_RANGE ne "first16"
+    && $HKEYS_RANGE ne "mid16"
+    && $HKEYS_RANGE ne "all"
+    && $HKEYS_RANGE ne "first32"
+    && $HKEYS_RANGE ne "last32");
+
+  my $rndsuffix = &random_string();
+
+  $code .= <<___;
+        test              $HKEYS_READY,$HKEYS_READY
+        jnz               .L_skip_hkeys_precomputation_${rndsuffix}
+___
+
+  if ($HKEYS_RANGE eq "first16" || $HKEYS_RANGE eq "first32" || $HKEYS_RANGE eq "all") {
+
+    # ; Fill the stack with the first 16 hkeys from the context
+    $code .= <<___;
+        # ; Move 16 hkeys from the context to stack
+        vmovdqu64         @{[HashKeyByIdx(4,$HTABLE)]},$ZTMP0
+        vmovdqu64         $ZTMP0,@{[HashKeyByIdx(4,"%rsp")]}
+
+        vmovdqu64         @{[HashKeyByIdx(8,$HTABLE)]},$ZTMP1
+        vmovdqu64         $ZTMP1,@{[HashKeyByIdx(8,"%rsp")]}
+
+        # ; broadcast HashKey^8
+        vshufi64x2        \$0x00,$ZTMP1,$ZTMP1,$ZTMP1
+
+        vmovdqu64         @{[HashKeyByIdx(12,$HTABLE)]},$ZTMP2
+        vmovdqu64         $ZTMP2,@{[HashKeyByIdx(12,"%rsp")]}
+
+        vmovdqu64         @{[HashKeyByIdx(16,$HTABLE)]},$ZTMP3
+        vmovdqu64         $ZTMP3,@{[HashKeyByIdx(16,"%rsp")]}
+___
+  }
+
+  if ($HKEYS_RANGE eq "mid16" || $HKEYS_RANGE eq "last32") {
+    $code .= <<___;
+        vmovdqu64         @{[HashKeyByIdx(8,"%rsp")]},$ZTMP1
+
+        # ; broadcast HashKey^8
+        vshufi64x2        \$0x00,$ZTMP1,$ZTMP1,$ZTMP1
+
+        vmovdqu64         @{[HashKeyByIdx(12,"%rsp")]},$ZTMP2
+        vmovdqu64         @{[HashKeyByIdx(16,"%rsp")]},$ZTMP3
+___
+
+  }
+
+  if ($HKEYS_RANGE eq "mid16" || $HKEYS_RANGE eq "first32" || $HKEYS_RANGE eq "last32" || $HKEYS_RANGE eq "all") {
+
+    # ; Precompute hkeys^i, i=17..32
+    my $i = 20;
+    foreach (1 .. int((32 - 16) / 8)) {
+
+      # ;; compute HashKey^(4 + n), HashKey^(3 + n), ... HashKey^(1 + n)
+      &GHASH_MUL($ZTMP2, $ZTMP1, $ZTMP4, $ZTMP5, $ZTMP6);
+      $code .= "vmovdqu64         $ZTMP2,@{[HashKeyByIdx($i,\"%rsp\")]}\n";
+      $i += 4;
+
+      # ;; compute HashKey^(8 + n), HashKey^(7 + n), ... HashKey^(5 + n)
+      &GHASH_MUL($ZTMP3, $ZTMP1, $ZTMP4, $ZTMP5, $ZTMP6);
+      $code .= "vmovdqu64         $ZTMP3,@{[HashKeyByIdx($i,\"%rsp\")]}\n";
+      $i += 4;
+    }
+  }
+
+  if ($HKEYS_RANGE eq "last32" || $HKEYS_RANGE eq "all") {
+
+    # ; Precompute hkeys^i, i=33..48 (HKEYS_STORAGE_CAPACITY = 48)
+    my $i = 36;
+    foreach (1 .. int((48 - 32) / 8)) {
+
+      # ;; compute HashKey^(4 + n), HashKey^(3 + n), ... HashKey^(1 + n)
+      &GHASH_MUL($ZTMP2, $ZTMP1, $ZTMP4, $ZTMP5, $ZTMP6);
+      $code .= "vmovdqu64         $ZTMP2,@{[HashKeyByIdx($i,\"%rsp\")]}\n";
+      $i += 4;
+
+      # ;; compute HashKey^(8 + n), HashKey^(7 + n), ... HashKey^(5 + n)
+      &GHASH_MUL($ZTMP3, $ZTMP1, $ZTMP4, $ZTMP5, $ZTMP6);
+      $code .= "vmovdqu64         $ZTMP3,@{[HashKeyByIdx($i,\"%rsp\")]}\n";
+      $i += 4;
+    }
+  }
+
+  $code .= ".L_skip_hkeys_precomputation_${rndsuffix}:\n";
+}
+
+# ;; =============================================================================
+# ;; Generic macro to produce code that executes $OPCODE instruction
+# ;; on selected number of AES blocks (16 bytes long ) between 0 and 16.
+# ;; All three operands of the instruction come from registers.
+# ;; Note: if 3 blocks are left at the end instruction is produced to operate all
+# ;;       4 blocks (full width of ZMM)
+sub ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16 {
+  my $NUM_BLOCKS = $_[0];    # [in] numerical value, number of AES blocks (0 to 16)
+  my $OPCODE     = $_[1];    # [in] instruction name
+  my @DST;
+  $DST[0] = $_[2];           # [out] destination ZMM register
+  $DST[1] = $_[3];           # [out] destination ZMM register
+  $DST[2] = $_[4];           # [out] destination ZMM register
+  $DST[3] = $_[5];           # [out] destination ZMM register
+  my @SRC1;
+  $SRC1[0] = $_[6];          # [in] source 1 ZMM register
+  $SRC1[1] = $_[7];          # [in] source 1 ZMM register
+  $SRC1[2] = $_[8];          # [in] source 1 ZMM register
+  $SRC1[3] = $_[9];          # [in] source 1 ZMM register
+  my @SRC2;
+  $SRC2[0] = $_[10];         # [in] source 2 ZMM register
+  $SRC2[1] = $_[11];         # [in] source 2 ZMM register
+  $SRC2[2] = $_[12];         # [in] source 2 ZMM register
+  $SRC2[3] = $_[13];         # [in] source 2 ZMM register
+
+  die "ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16: num_blocks is out of bounds = $NUM_BLOCKS\n"
+    if ($NUM_BLOCKS > 16 || $NUM_BLOCKS < 0);
+
+  my $reg_idx     = 0;
+  my $blocks_left = $NUM_BLOCKS;
+
+  foreach (1 .. ($NUM_BLOCKS / 4)) {
+    $code .= "$OPCODE        $SRC2[$reg_idx],$SRC1[$reg_idx],$DST[$reg_idx]\n";
+    $reg_idx++;
+    $blocks_left -= 4;
+  }
+
+  my $DSTREG  = $DST[$reg_idx];
+  my $SRC1REG = $SRC1[$reg_idx];
+  my $SRC2REG = $SRC2[$reg_idx];
+
+  if ($blocks_left == 1) {
+    $code .= "$OPCODE         @{[XWORD($SRC2REG)]},@{[XWORD($SRC1REG)]},@{[XWORD($DSTREG)]}\n";
+  } elsif ($blocks_left == 2) {
+    $code .= "$OPCODE         @{[YWORD($SRC2REG)]},@{[YWORD($SRC1REG)]},@{[YWORD($DSTREG)]}\n";
+  } elsif ($blocks_left == 3) {
+    $code .= "$OPCODE         $SRC2REG,$SRC1REG,$DSTREG\n";
+  }
+}
+
+# ;; =============================================================================
+# ;; Loads specified number of AES blocks into ZMM registers using mask register
+# ;; for the last loaded register (xmm, ymm or zmm).
+# ;; Loads take place at 1 byte granularity.
+sub ZMM_LOAD_MASKED_BLOCKS_0_16 {
+  my $NUM_BLOCKS  = $_[0];    # [in] numerical value, number of AES blocks (0 to 16)
+  my $INP         = $_[1];    # [in] input data pointer to read from
+  my $DATA_OFFSET = $_[2];    # [in] offset to the output pointer (GP or numerical)
+  my @DST;
+  $DST[0] = $_[3];            # [out] ZMM register with loaded data
+  $DST[1] = $_[4];            # [out] ZMM register with loaded data
+  $DST[2] = $_[5];            # [out] ZMM register with loaded data
+  $DST[3] = $_[6];            # [out] ZMM register with loaded data
+  my $MASK = $_[7];           # [in] mask register
+
+  die "ZMM_LOAD_MASKED_BLOCKS_0_16: num_blocks is out of bounds = $NUM_BLOCKS\n"
+    if ($NUM_BLOCKS > 16 || $NUM_BLOCKS < 0);
+
+  my $src_offset  = 0;
+  my $dst_idx     = 0;
+  my $blocks_left = $NUM_BLOCKS;
+
+  if ($NUM_BLOCKS > 0) {
+    foreach (1 .. (int(($NUM_BLOCKS + 3) / 4) - 1)) {
+      $code .= "vmovdqu8          @{[EffectiveAddress($INP,$DATA_OFFSET,$src_offset)]},$DST[$dst_idx]\n";
+      $src_offset += 64;
+      $dst_idx++;
+      $blocks_left -= 4;
+    }
+  }
+
+  my $DSTREG = $DST[$dst_idx];
+
+  if ($blocks_left == 1) {
+    $code .= "vmovdqu8          @{[EffectiveAddress($INP,$DATA_OFFSET,$src_offset)]},@{[XWORD($DSTREG)]}\{$MASK\}{z}\n";
+  } elsif ($blocks_left == 2) {
+    $code .= "vmovdqu8          @{[EffectiveAddress($INP,$DATA_OFFSET,$src_offset)]},@{[YWORD($DSTREG)]}\{$MASK\}{z}\n";
+  } elsif (($blocks_left == 3 || $blocks_left == 4)) {
+    $code .= "vmovdqu8          @{[EffectiveAddress($INP,$DATA_OFFSET,$src_offset)]},$DSTREG\{$MASK\}{z}\n";
+  }
+}
+
+# ;; =============================================================================
+# ;; Stores specified number of AES blocks from ZMM registers with mask register
+# ;; for the last loaded register (xmm, ymm or zmm).
+# ;; Stores take place at 1 byte granularity.
+sub ZMM_STORE_MASKED_BLOCKS_0_16 {
+  my $NUM_BLOCKS  = $_[0];    # [in] numerical value, number of AES blocks (0 to 16)
+  my $OUTP        = $_[1];    # [in] output data pointer to write to
+  my $DATA_OFFSET = $_[2];    # [in] offset to the output pointer (GP or numerical)
+  my @SRC;
+  $SRC[0] = $_[3];            # [in] ZMM register with data to store
+  $SRC[1] = $_[4];            # [in] ZMM register with data to store
+  $SRC[2] = $_[5];            # [in] ZMM register with data to store
+  $SRC[3] = $_[6];            # [in] ZMM register with data to store
+  my $MASK = $_[7];           # [in] mask register
+
+  die "ZMM_STORE_MASKED_BLOCKS_0_16: num_blocks is out of bounds = $NUM_BLOCKS\n"
+    if ($NUM_BLOCKS > 16 || $NUM_BLOCKS < 0);
+
+  my $dst_offset  = 0;
+  my $src_idx     = 0;
+  my $blocks_left = $NUM_BLOCKS;
+
+  if ($NUM_BLOCKS > 0) {
+    foreach (1 .. (int(($NUM_BLOCKS + 3) / 4) - 1)) {
+      $code .= "vmovdqu8          $SRC[$src_idx],`$dst_offset`($OUTP,$DATA_OFFSET,1)\n";
+      $dst_offset += 64;
+      $src_idx++;
+      $blocks_left -= 4;
+    }
+  }
+
+  my $SRCREG = $SRC[$src_idx];
+
+  if ($blocks_left == 1) {
+    $code .= "vmovdqu8          @{[XWORD($SRCREG)]},`$dst_offset`($OUTP,$DATA_OFFSET,1){$MASK}\n";
+  } elsif ($blocks_left == 2) {
+    $code .= "vmovdqu8          @{[YWORD($SRCREG)]},`$dst_offset`($OUTP,$DATA_OFFSET,1){$MASK}\n";
+  } elsif ($blocks_left == 3 || $blocks_left == 4) {
+    $code .= "vmovdqu8          $SRCREG,`$dst_offset`($OUTP,$DATA_OFFSET,1){$MASK}\n";
+  }
+}
+
+# ;;; ===========================================================================
+# ;;; Handles AES encryption rounds
+# ;;; It handles special cases: the last and first rounds
+# ;;; Optionally, it performs XOR with data after the last AES round.
+# ;;; Uses NROUNDS parameter to check what needs to be done for the current round.
+# ;;; If 3 blocks are trailing then operation on whole ZMM is performed (4 blocks).
+sub ZMM_AESENC_ROUND_BLOCKS_0_16 {
+  my $L0B0_3   = $_[0];     # [in/out] zmm; blocks 0 to 3
+  my $L0B4_7   = $_[1];     # [in/out] zmm; blocks 4 to 7
+  my $L0B8_11  = $_[2];     # [in/out] zmm; blocks 8 to 11
+  my $L0B12_15 = $_[3];     # [in/out] zmm; blocks 12 to 15
+  my $KEY      = $_[4];     # [in] zmm containing round key
+  my $ROUND    = $_[5];     # [in] round number
+  my $D0_3     = $_[6];     # [in] zmm or no_data; plain/cipher text blocks 0-3
+  my $D4_7     = $_[7];     # [in] zmm or no_data; plain/cipher text blocks 4-7
+  my $D8_11    = $_[8];     # [in] zmm or no_data; plain/cipher text blocks 8-11
+  my $D12_15   = $_[9];     # [in] zmm or no_data; plain/cipher text blocks 12-15
+  my $NUMBL    = $_[10];    # [in] number of blocks; numerical value
+  my $NROUNDS  = $_[11];    # [in] number of rounds; numerical value
+
+  # ;;; === first AES round
+  if ($ROUND < 1) {
+
+    # ;;  round 0
+    &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+      $NUMBL,  "vpxorq", $L0B0_3,   $L0B4_7, $L0B8_11, $L0B12_15, $L0B0_3,
+      $L0B4_7, $L0B8_11, $L0B12_15, $KEY,    $KEY,     $KEY,      $KEY);
+  }
+
+  # ;;; === middle AES rounds
+  if ($ROUND >= 1 && $ROUND <= $NROUNDS) {
+
+    # ;; rounds 1 to 9/11/13
+    &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+      $NUMBL,  "vaesenc", $L0B0_3,   $L0B4_7, $L0B8_11, $L0B12_15, $L0B0_3,
+      $L0B4_7, $L0B8_11,  $L0B12_15, $KEY,    $KEY,     $KEY,      $KEY);
+  }
+
+  # ;;; === last AES round
+  if ($ROUND > $NROUNDS) {
+
+    # ;; the last round - mix enclast with text xor's
+    &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+      $NUMBL,  "vaesenclast", $L0B0_3,   $L0B4_7, $L0B8_11, $L0B12_15, $L0B0_3,
+      $L0B4_7, $L0B8_11,      $L0B12_15, $KEY,    $KEY,     $KEY,      $KEY);
+
+    # ;;; === XOR with data
+    if ( ($D0_3 ne "no_data")
+      && ($D4_7 ne "no_data")
+      && ($D8_11 ne "no_data")
+      && ($D12_15 ne "no_data"))
+    {
+      &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+        $NUMBL,  "vpxorq", $L0B0_3,   $L0B4_7, $L0B8_11, $L0B12_15, $L0B0_3,
+        $L0B4_7, $L0B8_11, $L0B12_15, $D0_3,   $D4_7,    $D8_11,    $D12_15);
+    }
+  }
+}
+
+# ;;; Horizontal XOR - 4 x 128bits xored together
+sub VHPXORI4x128 {
+  my $REG = $_[0];    # [in/out] ZMM with 4x128bits to xor; 128bit output
+  my $TMP = $_[1];    # [clobbered] ZMM temporary register
+  $code .= <<___;
+        vextracti64x4     \$1,$REG,@{[YWORD($TMP)]}
+        vpxorq            @{[YWORD($TMP)]},@{[YWORD($REG)]},@{[YWORD($REG)]}
+        vextracti32x4     \$1,@{[YWORD($REG)]},@{[XWORD($TMP)]}
+        vpxorq            @{[XWORD($TMP)]},@{[XWORD($REG)]},@{[XWORD($REG)]}
+___
+}
+
+# ;;; AVX512 reduction macro
+sub VCLMUL_REDUCE {
+  my $OUT   = $_[0];    # [out] zmm/ymm/xmm: result (must not be $TMP1 or $HI128)
+  my $POLY  = $_[1];    # [in] zmm/ymm/xmm: polynomial
+  my $HI128 = $_[2];    # [in] zmm/ymm/xmm: high 128b of hash to reduce
+  my $LO128 = $_[3];    # [in] zmm/ymm/xmm: low 128b of hash to reduce
+  my $TMP0  = $_[4];    # [in] zmm/ymm/xmm: temporary register
+  my $TMP1  = $_[5];    # [in] zmm/ymm/xmm: temporary register
+
+  $code .= <<___;
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; first phase of the reduction
+        vpclmulqdq        \$0x01,$LO128,$POLY,$TMP0
+        vpslldq           \$8,$TMP0,$TMP0         # ; shift-L 2 DWs
+        vpxorq            $TMP0,$LO128,$TMP0      # ; first phase of the reduction complete
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; second phase of the reduction
+        vpclmulqdq        \$0x00,$TMP0,$POLY,$TMP1
+        vpsrldq           \$4,$TMP1,$TMP1          # ; shift-R only 1-DW to obtain 2-DWs shift-R
+        vpclmulqdq        \$0x10,$TMP0,$POLY,$OUT
+        vpslldq           \$4,$OUT,$OUT            # ; shift-L 1-DW to obtain result with no shifts
+        vpternlogq        \$0x96,$HI128,$TMP1,$OUT # ; OUT/GHASH = OUT xor TMP1 xor HI128
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+___
+}
+
+# ;; ===========================================================================
+# ;; schoolbook multiply of 16 blocks (16 x 16 bytes)
+# ;; - it is assumed that data read from $INPTR is already shuffled and
+# ;;   $INPTR address is 64 byte aligned
+# ;; - there is an option to pass ready blocks through ZMM registers too.
+# ;;   4 extra parameters need to be passed in such case and 21st ($ZTMP9) argument can be empty
+sub GHASH_16 {
+  my $TYPE  = $_[0];     # [in] ghash type: start (xor hash), mid, end (same as mid; no reduction),
+                         # end_reduce (end with reduction), start_reduce
+  my $GH    = $_[1];     # [in/out] ZMM ghash sum: high 128-bits
+  my $GM    = $_[2];     # [in/out] ZMM ghash sum: middle 128-bits
+  my $GL    = $_[3];     # [in/out] ZMM ghash sum: low 128-bits
+  my $INPTR = $_[4];     # [in] data input pointer
+  my $INOFF = $_[5];     # [in] data input offset
+  my $INDIS = $_[6];     # [in] data input displacement
+  my $HKPTR = $_[7];     # [in] hash key pointer
+  my $HKOFF = $_[8];     # [in] hash key offset (can be either numerical offset, or register containing offset)
+  my $HKDIS = $_[9];     # [in] hash key displacement
+  my $HASH  = $_[10];    # [in/out] ZMM hash value in/out
+  my $ZTMP0 = $_[11];    # [clobbered] temporary ZMM
+  my $ZTMP1 = $_[12];    # [clobbered] temporary ZMM
+  my $ZTMP2 = $_[13];    # [clobbered] temporary ZMM
+  my $ZTMP3 = $_[14];    # [clobbered] temporary ZMM
+  my $ZTMP4 = $_[15];    # [clobbered] temporary ZMM
+  my $ZTMP5 = $_[16];    # [clobbered] temporary ZMM
+  my $ZTMP6 = $_[17];    # [clobbered] temporary ZMM
+  my $ZTMP7 = $_[18];    # [clobbered] temporary ZMM
+  my $ZTMP8 = $_[19];    # [clobbered] temporary ZMM
+  my $ZTMP9 = $_[20];    # [clobbered] temporary ZMM, can be empty if 4 extra parameters below are provided
+  my $DAT0  = $_[21];    # [in] ZMM with 4 blocks of input data (INPTR, INOFF, INDIS unused)
+  my $DAT1  = $_[22];    # [in] ZMM with 4 blocks of input data (INPTR, INOFF, INDIS unused)
+  my $DAT2  = $_[23];    # [in] ZMM with 4 blocks of input data (INPTR, INOFF, INDIS unused)
+  my $DAT3  = $_[24];    # [in] ZMM with 4 blocks of input data (INPTR, INOFF, INDIS unused)
+
+  my $start_ghash  = 0;
+  my $do_reduction = 0;
+  if ($TYPE eq "start") {
+    $start_ghash = 1;
+  }
+
+  if ($TYPE eq "start_reduce") {
+    $start_ghash  = 1;
+    $do_reduction = 1;
+  }
+
+  if ($TYPE eq "end_reduce") {
+    $do_reduction = 1;
+  }
+
+  # ;; ghash blocks 0-3
+  if (scalar(@_) == 21) {
+    $code .= "vmovdqa64         @{[EffectiveAddress($INPTR,$INOFF,($INDIS+0*64))]},$ZTMP9\n";
+  } else {
+    $ZTMP9 = $DAT0;
+  }
+
+  if ($start_ghash != 0) {
+    $code .= "vpxorq            $HASH,$ZTMP9,$ZTMP9\n";
+  }
+  $code .= <<___;
+        vmovdqu64         @{[EffectiveAddress($HKPTR,$HKOFF,($HKDIS+0*64))]},$ZTMP8
+        vpclmulqdq        \$0x11,$ZTMP8,$ZTMP9,$ZTMP0      # ; T0H = a1*b1
+        vpclmulqdq        \$0x00,$ZTMP8,$ZTMP9,$ZTMP1      # ; T0L = a0*b0
+        vpclmulqdq        \$0x01,$ZTMP8,$ZTMP9,$ZTMP2      # ; T0M1 = a1*b0
+        vpclmulqdq        \$0x10,$ZTMP8,$ZTMP9,$ZTMP3      # ; T0M2 = a0*b1
+___
+
+  # ;; ghash blocks 4-7
+  if (scalar(@_) == 21) {
+    $code .= "vmovdqa64         @{[EffectiveAddress($INPTR,$INOFF,($INDIS+1*64))]},$ZTMP9\n";
+  } else {
+    $ZTMP9 = $DAT1;
+  }
+  $code .= <<___;
+        vmovdqu64         @{[EffectiveAddress($HKPTR,$HKOFF,($HKDIS+1*64))]},$ZTMP8
+        vpclmulqdq        \$0x11,$ZTMP8,$ZTMP9,$ZTMP4      # ; T1H = a1*b1
+        vpclmulqdq        \$0x00,$ZTMP8,$ZTMP9,$ZTMP5      # ; T1L = a0*b0
+        vpclmulqdq        \$0x01,$ZTMP8,$ZTMP9,$ZTMP6      # ; T1M1 = a1*b0
+        vpclmulqdq        \$0x10,$ZTMP8,$ZTMP9,$ZTMP7      # ; T1M2 = a0*b1
+___
+
+  # ;; update sums
+  if ($start_ghash != 0) {
+    $code .= <<___;
+        vpxorq            $ZTMP6,$ZTMP2,$GM             # ; GM = T0M1 + T1M1
+        vpxorq            $ZTMP4,$ZTMP0,$GH             # ; GH = T0H + T1H
+        vpxorq            $ZTMP5,$ZTMP1,$GL             # ; GL = T0L + T1L
+        vpternlogq        \$0x96,$ZTMP7,$ZTMP3,$GM      # ; GM = T0M2 + T1M1
+___
+  } else {    # ;; mid, end, end_reduce
+    $code .= <<___;
+        vpternlogq        \$0x96,$ZTMP6,$ZTMP2,$GM      # ; GM += T0M1 + T1M1
+        vpternlogq        \$0x96,$ZTMP4,$ZTMP0,$GH      # ; GH += T0H + T1H
+        vpternlogq        \$0x96,$ZTMP5,$ZTMP1,$GL      # ; GL += T0L + T1L
+        vpternlogq        \$0x96,$ZTMP7,$ZTMP3,$GM      # ; GM += T0M2 + T1M1
+___
+  }
+
+  # ;; ghash blocks 8-11
+  if (scalar(@_) == 21) {
+    $code .= "vmovdqa64         @{[EffectiveAddress($INPTR,$INOFF,($INDIS+2*64))]},$ZTMP9\n";
+  } else {
+    $ZTMP9 = $DAT2;
+  }
+  $code .= <<___;
+        vmovdqu64         @{[EffectiveAddress($HKPTR,$HKOFF,($HKDIS+2*64))]},$ZTMP8
+        vpclmulqdq        \$0x11,$ZTMP8,$ZTMP9,$ZTMP0      # ; T0H = a1*b1
+        vpclmulqdq        \$0x00,$ZTMP8,$ZTMP9,$ZTMP1      # ; T0L = a0*b0
+        vpclmulqdq        \$0x01,$ZTMP8,$ZTMP9,$ZTMP2      # ; T0M1 = a1*b0
+        vpclmulqdq        \$0x10,$ZTMP8,$ZTMP9,$ZTMP3      # ; T0M2 = a0*b1
+___
+
+  # ;; ghash blocks 12-15
+  if (scalar(@_) == 21) {
+    $code .= "vmovdqa64         @{[EffectiveAddress($INPTR,$INOFF,($INDIS+3*64))]},$ZTMP9\n";
+  } else {
+    $ZTMP9 = $DAT3;
+  }
+  $code .= <<___;
+        vmovdqu64         @{[EffectiveAddress($HKPTR,$HKOFF,($HKDIS+3*64))]},$ZTMP8
+        vpclmulqdq        \$0x11,$ZTMP8,$ZTMP9,$ZTMP4      # ; T1H = a1*b1
+        vpclmulqdq        \$0x00,$ZTMP8,$ZTMP9,$ZTMP5      # ; T1L = a0*b0
+        vpclmulqdq        \$0x01,$ZTMP8,$ZTMP9,$ZTMP6      # ; T1M1 = a1*b0
+        vpclmulqdq        \$0x10,$ZTMP8,$ZTMP9,$ZTMP7      # ; T1M2 = a0*b1
+        # ;; update sums
+        vpternlogq        \$0x96,$ZTMP6,$ZTMP2,$GM         # ; GM += T0M1 + T1M1
+        vpternlogq        \$0x96,$ZTMP4,$ZTMP0,$GH         # ; GH += T0H + T1H
+        vpternlogq        \$0x96,$ZTMP5,$ZTMP1,$GL         # ; GL += T0L + T1L
+        vpternlogq        \$0x96,$ZTMP7,$ZTMP3,$GM         # ; GM += T0M2 + T1M1
+___
+  if ($do_reduction != 0) {
+    $code .= <<___;
+        # ;; integrate GM into GH and GL
+        vpsrldq           \$8,$GM,$ZTMP0
+        vpslldq           \$8,$GM,$ZTMP1
+        vpxorq            $ZTMP0,$GH,$GH
+        vpxorq            $ZTMP1,$GL,$GL
+___
+
+    # ;; add GH and GL 128-bit words horizontally
+    &VHPXORI4x128($GH, $ZTMP0);
+    &VHPXORI4x128($GL, $ZTMP1);
+
+    # ;; reduction
+    $code .= "vmovdqa64         POLY2(%rip),@{[XWORD($ZTMP2)]}\n";
+    &VCLMUL_REDUCE(&XWORD($HASH), &XWORD($ZTMP2), &XWORD($GH), &XWORD($GL), &XWORD($ZTMP0), &XWORD($ZTMP1));
+  }
+}
+
+# ;; ===========================================================================
+# ;; GHASH 1 to 16 blocks of cipher text
+# ;; - performs reduction at the end
+# ;; - it doesn't load the data and it assumed it is already loaded and shuffled
+sub GHASH_1_TO_16 {
+  my $HTABLE      = $_[0];     # [in] pointer to hkeys table
+  my $GHASH       = $_[1];     # [out] ghash output
+  my $T0H         = $_[2];     # [clobbered] temporary ZMM
+  my $T0L         = $_[3];     # [clobbered] temporary ZMM
+  my $T0M1        = $_[4];     # [clobbered] temporary ZMM
+  my $T0M2        = $_[5];     # [clobbered] temporary ZMM
+  my $T1H         = $_[6];     # [clobbered] temporary ZMM
+  my $T1L         = $_[7];     # [clobbered] temporary ZMM
+  my $T1M1        = $_[8];     # [clobbered] temporary ZMM
+  my $T1M2        = $_[9];     # [clobbered] temporary ZMM
+  my $HK          = $_[10];    # [clobbered] temporary ZMM
+  my $AAD_HASH_IN = $_[11];    # [in] input hash value
+  my @CIPHER_IN;
+  $CIPHER_IN[0] = $_[12];      # [in] ZMM with cipher text blocks 0-3
+  $CIPHER_IN[1] = $_[13];      # [in] ZMM with cipher text blocks 4-7
+  $CIPHER_IN[2] = $_[14];      # [in] ZMM with cipher text blocks 8-11
+  $CIPHER_IN[3] = $_[15];      # [in] ZMM with cipher text blocks 12-15
+  my $NUM_BLOCKS = $_[16];     # [in] numerical value, number of blocks
+  my $GH         = $_[17];     # [in] ZMM with hi product part
+  my $GM         = $_[18];     # [in] ZMM with mid product part
+  my $GL         = $_[19];     # [in] ZMM with lo product part
+
+  die "GHASH_1_TO_16: num_blocks is out of bounds = $NUM_BLOCKS\n" if ($NUM_BLOCKS > 16 || $NUM_BLOCKS < 0);
+
+  if (scalar(@_) == 17) {
+    $code .= "vpxorq            $AAD_HASH_IN,$CIPHER_IN[0],$CIPHER_IN[0]\n";
+  }
+
+  if ($NUM_BLOCKS == 16) {
+    $code .= <<___;
+        vmovdqu64         @{[HashKeyByIdx($NUM_BLOCKS, $HTABLE)]},$HK
+        vpclmulqdq        \$0x11,$HK,$CIPHER_IN[0],$T0H        # ; H = a1*b1
+        vpclmulqdq        \$0x00,$HK,$CIPHER_IN[0],$T0L        # ; L = a0*b0
+        vpclmulqdq        \$0x01,$HK,$CIPHER_IN[0],$T0M1       # ; M1 = a1*b0
+        vpclmulqdq        \$0x10,$HK,$CIPHER_IN[0],$T0M2       # ; M2 = a0*b1
+        vmovdqu64         @{[HashKeyByIdx($NUM_BLOCKS-1*4, $HTABLE)]},$HK
+        vpclmulqdq        \$0x11,$HK,$CIPHER_IN[1],$T1H        # ; H = a1*b1
+        vpclmulqdq        \$0x00,$HK,$CIPHER_IN[1],$T1L        # ; L = a0*b0
+        vpclmulqdq        \$0x01,$HK,$CIPHER_IN[1],$T1M1       # ; M1 = a1*b0
+        vpclmulqdq        \$0x10,$HK,$CIPHER_IN[1],$T1M2       # ; M2 = a0*b1
+        vmovdqu64         @{[HashKeyByIdx($NUM_BLOCKS-2*4, $HTABLE)]},$HK
+        vpclmulqdq        \$0x11,$HK,$CIPHER_IN[2],$CIPHER_IN[0] # ; H = a1*b1
+        vpclmulqdq        \$0x00,$HK,$CIPHER_IN[2],$CIPHER_IN[1] # ; L = a0*b0
+        vpternlogq        \$0x96,$T1H,$CIPHER_IN[0],$T0H
+        vpternlogq        \$0x96,$T1L,$CIPHER_IN[1],$T0L
+        vpclmulqdq        \$0x01,$HK,$CIPHER_IN[2],$CIPHER_IN[0] # ; M1 = a1*b0
+        vpclmulqdq        \$0x10,$HK,$CIPHER_IN[2],$CIPHER_IN[1] # ; M2 = a0*b1
+        vpternlogq        \$0x96,$T1M1,$CIPHER_IN[0],$T0M1
+        vpternlogq        \$0x96,$T1M2,$CIPHER_IN[1],$T0M2
+        vmovdqu64         @{[HashKeyByIdx($NUM_BLOCKS-3*4, $HTABLE)]},$HK
+        vpclmulqdq        \$0x11,$HK,$CIPHER_IN[3],$T1H        # ; H = a1*b1
+        vpclmulqdq        \$0x00,$HK,$CIPHER_IN[3],$T1L        # ; L = a0*b0
+        vpclmulqdq        \$0x01,$HK,$CIPHER_IN[3],$T1M1       # ; M1 = a1*b0
+        vpclmulqdq        \$0x10,$HK,$CIPHER_IN[3],$T1M2       # ; M2 = a0*b1
+        vpxorq            $T1H,$T0H,$T1H
+        vpxorq            $T1L,$T0L,$T1L
+        vpxorq            $T1M1,$T0M1,$T1M1
+        vpxorq            $T1M2,$T0M2,$T1M2
+___
+  } elsif ($NUM_BLOCKS >= 12) {
+    $code .= <<___;
+        vmovdqu64         @{[HashKeyByIdx($NUM_BLOCKS, $HTABLE)]},$HK
+        vpclmulqdq        \$0x11,$HK,$CIPHER_IN[0],$T0H        # ; H = a1*b1
+        vpclmulqdq        \$0x00,$HK,$CIPHER_IN[0],$T0L        # ; L = a0*b0
+        vpclmulqdq        \$0x01,$HK,$CIPHER_IN[0],$T0M1       # ; M1 = a1*b0
+        vpclmulqdq        \$0x10,$HK,$CIPHER_IN[0],$T0M2       # ; M2 = a0*b1
+        vmovdqu64         @{[HashKeyByIdx($NUM_BLOCKS-1*4, $HTABLE)]},$HK
+        vpclmulqdq        \$0x11,$HK,$CIPHER_IN[1],$T1H        # ; H = a1*b1
+        vpclmulqdq        \$0x00,$HK,$CIPHER_IN[1],$T1L        # ; L = a0*b0
+        vpclmulqdq        \$0x01,$HK,$CIPHER_IN[1],$T1M1       # ; M1 = a1*b0
+        vpclmulqdq        \$0x10,$HK,$CIPHER_IN[1],$T1M2       # ; M2 = a0*b1
+        vmovdqu64         @{[HashKeyByIdx($NUM_BLOCKS-2*4, $HTABLE)]},$HK
+        vpclmulqdq        \$0x11,$HK,$CIPHER_IN[2],$CIPHER_IN[0] # ; H = a1*b1
+        vpclmulqdq        \$0x00,$HK,$CIPHER_IN[2],$CIPHER_IN[1] # ; L = a0*b0
+        vpternlogq        \$0x96,$T0H,$CIPHER_IN[0],$T1H
+        vpternlogq        \$0x96,$T0L,$CIPHER_IN[1],$T1L
+        vpclmulqdq        \$0x01,$HK,$CIPHER_IN[2],$CIPHER_IN[0] # ; M1 = a1*b0
+        vpclmulqdq        \$0x10,$HK,$CIPHER_IN[2],$CIPHER_IN[1] # ; M2 = a0*b1
+        vpternlogq        \$0x96,$T0M1,$CIPHER_IN[0],$T1M1
+        vpternlogq        \$0x96,$T0M2,$CIPHER_IN[1],$T1M2
+___
+  } elsif ($NUM_BLOCKS >= 8) {
+    $code .= <<___;
+        vmovdqu64         @{[HashKeyByIdx($NUM_BLOCKS, $HTABLE)]},$HK
+        vpclmulqdq        \$0x11,$HK,$CIPHER_IN[0],$T0H        # ; H = a1*b1
+        vpclmulqdq        \$0x00,$HK,$CIPHER_IN[0],$T0L        # ; L = a0*b0
+        vpclmulqdq        \$0x01,$HK,$CIPHER_IN[0],$T0M1       # ; M1 = a1*b0
+        vpclmulqdq        \$0x10,$HK,$CIPHER_IN[0],$T0M2       # ; M2 = a0*b1
+        vmovdqu64         @{[HashKeyByIdx($NUM_BLOCKS-1*4, $HTABLE)]},$HK
+        vpclmulqdq        \$0x11,$HK,$CIPHER_IN[1],$T1H        # ; H = a1*b1
+        vpclmulqdq        \$0x00,$HK,$CIPHER_IN[1],$T1L        # ; L = a0*b0
+        vpclmulqdq        \$0x01,$HK,$CIPHER_IN[1],$T1M1       # ; M1 = a1*b0
+        vpclmulqdq        \$0x10,$HK,$CIPHER_IN[1],$T1M2       # ; M2 = a0*b1
+        vpxorq            $T1H,$T0H,$T1H
+        vpxorq            $T1L,$T0L,$T1L
+        vpxorq            $T1M1,$T0M1,$T1M1
+        vpxorq            $T1M2,$T0M2,$T1M2
+___
+  } elsif ($NUM_BLOCKS >= 4) {
+    $code .= <<___;
+        vmovdqu64         @{[HashKeyByIdx($NUM_BLOCKS, $HTABLE)]},$HK
+        vpclmulqdq        \$0x11,$HK,$CIPHER_IN[0],$T1H        # ; H = a1*b1
+        vpclmulqdq        \$0x00,$HK,$CIPHER_IN[0],$T1L        # ; L = a0*b0
+        vpclmulqdq        \$0x01,$HK,$CIPHER_IN[0],$T1M1       # ; M1 = a1*b0
+        vpclmulqdq        \$0x10,$HK,$CIPHER_IN[0],$T1M2       # ; M2 = a0*b1
+___
+  }
+
+  # ;; T1H/L/M1/M2 - hold current product sums (provided $NUM_BLOCKS >= 4)
+  my $blocks_left = ($NUM_BLOCKS % 4);
+  if ($blocks_left > 0) {
+
+    # ;; =====================================================
+    # ;; There are 1, 2 or 3 blocks left to process.
+    # ;; It may also be that they are the only blocks to process.
+
+    # ;; Set hash key and register index position for the remaining 1 to 3 blocks
+    my $reg_idx = ($NUM_BLOCKS / 4);
+    my $REG_IN  = $CIPHER_IN[$reg_idx];
+
+    if ($blocks_left == 1) {
+      $code .= <<___;
+        vmovdqu64         @{[HashKeyByIdx($blocks_left, $HTABLE)]},@{[XWORD($HK)]}
+        vpclmulqdq        \$0x01,@{[XWORD($HK)]},@{[XWORD($REG_IN)]},@{[XWORD($T0M1)]} # ; M1 = a1*b0
+        vpclmulqdq        \$0x10,@{[XWORD($HK)]},@{[XWORD($REG_IN)]},@{[XWORD($T0M2)]} # ; M2 = a0*b1
+        vpclmulqdq        \$0x11,@{[XWORD($HK)]},@{[XWORD($REG_IN)]},@{[XWORD($T0H)]}  # ; H = a1*b1
+        vpclmulqdq        \$0x00,@{[XWORD($HK)]},@{[XWORD($REG_IN)]},@{[XWORD($T0L)]}  # ; L = a0*b0
+___
+    } elsif ($blocks_left == 2) {
+      $code .= <<___;
+        vmovdqu64         @{[HashKeyByIdx($blocks_left, $HTABLE)]},@{[YWORD($HK)]}
+        vpclmulqdq        \$0x01,@{[YWORD($HK)]},@{[YWORD($REG_IN)]},@{[YWORD($T0M1)]} # ; M1 = a1*b0
+        vpclmulqdq        \$0x10,@{[YWORD($HK)]},@{[YWORD($REG_IN)]},@{[YWORD($T0M2)]} # ; M2 = a0*b1
+        vpclmulqdq        \$0x11,@{[YWORD($HK)]},@{[YWORD($REG_IN)]},@{[YWORD($T0H)]}  # ; H = a1*b1
+        vpclmulqdq        \$0x00,@{[YWORD($HK)]},@{[YWORD($REG_IN)]},@{[YWORD($T0L)]}  # ; L = a0*b0
+___
+    } else {    # ; blocks_left == 3
+      $code .= <<___;
+        vmovdqu64         @{[HashKeyByIdx($blocks_left, $HTABLE)]},@{[YWORD($HK)]}
+        vinserti64x2      \$2,@{[HashKeyByIdx($blocks_left-2, $HTABLE)]},$HK,$HK
+        vpclmulqdq        \$0x01,$HK,$REG_IN,$T0M1                                     # ; M1 = a1*b0
+        vpclmulqdq        \$0x10,$HK,$REG_IN,$T0M2                                     # ; M2 = a0*b1
+        vpclmulqdq        \$0x11,$HK,$REG_IN,$T0H                                      # ; H = a1*b1
+        vpclmulqdq        \$0x00,$HK,$REG_IN,$T0L                                      # ; L = a0*b0
+___
+    }
+
+    if (scalar(@_) == 20) {
+
+      # ;; *** GH/GM/GL passed as arguments
+      if ($NUM_BLOCKS >= 4) {
+        $code .= <<___;
+        # ;; add ghash product sums from the first 4, 8 or 12 blocks
+        vpxorq            $T1M1,$T0M1,$T0M1
+        vpternlogq        \$0x96,$T1M2,$GM,$T0M2
+        vpternlogq        \$0x96,$T1H,$GH,$T0H
+        vpternlogq        \$0x96,$T1L,$GL,$T0L
+___
+      } else {
+        $code .= <<___;
+        vpxorq            $GM,$T0M1,$T0M1
+        vpxorq            $GH,$T0H,$T0H
+        vpxorq            $GL,$T0L,$T0L
+___
+      }
+    } else {
+
+      # ;; *** GH/GM/GL NOT passed as arguments
+      if ($NUM_BLOCKS >= 4) {
+        $code .= <<___;
+        # ;; add ghash product sums from the first 4, 8 or 12 blocks
+        vpxorq            $T1M1,$T0M1,$T0M1
+        vpxorq            $T1M2,$T0M2,$T0M2
+        vpxorq            $T1H,$T0H,$T0H
+        vpxorq            $T1L,$T0L,$T0L
+___
+      }
+    }
+    $code .= <<___;
+        # ;; integrate TM into TH and TL
+        vpxorq            $T0M2,$T0M1,$T0M1
+        vpsrldq           \$8,$T0M1,$T1M1
+        vpslldq           \$8,$T0M1,$T1M2
+        vpxorq            $T1M1,$T0H,$T0H
+        vpxorq            $T1M2,$T0L,$T0L
+___
+  } else {
+
+    # ;; =====================================================
+    # ;; number of blocks is 4, 8, 12 or 16
+    # ;; T1H/L/M1/M2 include product sums not T0H/L/M1/M2
+    if (scalar(@_) == 20) {
+      $code .= <<___;
+        # ;; *** GH/GM/GL passed as arguments
+        vpxorq            $GM,$T1M1,$T1M1
+        vpxorq            $GH,$T1H,$T1H
+        vpxorq            $GL,$T1L,$T1L
+___
+    }
+    $code .= <<___;
+        # ;; integrate TM into TH and TL
+        vpxorq            $T1M2,$T1M1,$T1M1
+        vpsrldq           \$8,$T1M1,$T0M1
+        vpslldq           \$8,$T1M1,$T0M2
+        vpxorq            $T0M1,$T1H,$T0H
+        vpxorq            $T0M2,$T1L,$T0L
+___
+  }
+
+  # ;; add TH and TL 128-bit words horizontally
+  &VHPXORI4x128($T0H, $T1M1);
+  &VHPXORI4x128($T0L, $T1M2);
+
+  # ;; reduction
+  $code .= "vmovdqa64         POLY2(%rip),@{[XWORD($HK)]}\n";
+  &VCLMUL_REDUCE(
+    @{[XWORD($GHASH)]},
+    @{[XWORD($HK)]},
+    @{[XWORD($T0H)]},
+    @{[XWORD($T0L)]},
+    @{[XWORD($T0M1)]},
+    @{[XWORD($T0M2)]});
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;; GHASH_MUL MACRO to implement: Data*HashKey mod (x^128 + x^127 + x^126 +x^121 + 1)
+# ;; Input: A and B (128-bits each, bit-reflected)
+# ;; Output: C = A*B*x mod poly, (i.e. >>1 )
+# ;; To compute GH = GH*HashKey mod poly, give HK = HashKey<<1 mod poly as input
+# ;; GH = GH * HK * x mod poly which is equivalent to GH*HashKey mod poly.
+# ;;
+# ;; Refer to [3] for more detals.
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+sub GHASH_MUL {
+  my $GH = $_[0];    #; [in/out] xmm/ymm/zmm with multiply operand(s) (128-bits)
+  my $HK = $_[1];    #; [in] xmm/ymm/zmm with hash key value(s) (128-bits)
+  my $T1 = $_[2];    #; [clobbered] xmm/ymm/zmm
+  my $T2 = $_[3];    #; [clobbered] xmm/ymm/zmm
+  my $T3 = $_[4];    #; [clobbered] xmm/ymm/zmm
+
+  $code .= <<___;
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        vpclmulqdq        \$0x11,$HK,$GH,$T1 # ; $T1 = a1*b1
+        vpclmulqdq        \$0x00,$HK,$GH,$T2 # ; $T2 = a0*b0
+        vpclmulqdq        \$0x01,$HK,$GH,$T3 # ; $T3 = a1*b0
+        vpclmulqdq        \$0x10,$HK,$GH,$GH # ; $GH = a0*b1
+        vpxorq            $T3,$GH,$GH
+
+        vpsrldq           \$8,$GH,$T3        # ; shift-R $GH 2 DWs
+        vpslldq           \$8,$GH,$GH        # ; shift-L $GH 2 DWs
+        vpxorq            $T3,$T1,$T1
+        vpxorq            $T2,$GH,$GH
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;first phase of the reduction
+        vmovdqu64         POLY2(%rip),$T3
+
+        vpclmulqdq        \$0x01,$GH,$T3,$T2
+        vpslldq           \$8,$T2,$T2        # ; shift-L $T2 2 DWs
+        vpxorq            $T2,$GH,$GH        # ; first phase of the reduction complete
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;second phase of the reduction
+        vpclmulqdq        \$0x00,$GH,$T3,$T2
+        vpsrldq           \$4,$T2,$T2        # ; shift-R only 1-DW to obtain 2-DWs shift-R
+        vpclmulqdq        \$0x10,$GH,$T3,$GH
+        vpslldq           \$4,$GH,$GH        # ; Shift-L 1-DW to obtain result with no shifts
+                                             # ; second phase of the reduction complete, the result is in $GH
+        vpternlogq        \$0x96,$T2,$T1,$GH # ; GH = GH xor T1 xor T2
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+___
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;;; PRECOMPUTE computes HashKey_i
+sub PRECOMPUTE {
+  my $HTABLE = $_[0];    #; [in/out] hkeys table
+  my $HK     = $_[1];    #; [in] xmm, hash key
+  my $T1     = $_[2];    #; [clobbered] xmm
+  my $T2     = $_[3];    #; [clobbered] xmm
+  my $T3     = $_[4];    #; [clobbered] xmm
+  my $T4     = $_[5];    #; [clobbered] xmm
+  my $T5     = $_[6];    #; [clobbered] xmm
+  my $T6     = $_[7];    #; [clobbered] xmm
+
+  my $ZT1 = &ZWORD($T1);
+  my $ZT2 = &ZWORD($T2);
+  my $ZT3 = &ZWORD($T3);
+  my $ZT4 = &ZWORD($T4);
+  my $ZT5 = &ZWORD($T5);
+  my $ZT6 = &ZWORD($T6);
+
+  my $YT1 = &YWORD($T1);
+  my $YT2 = &YWORD($T2);
+  my $YT3 = &YWORD($T3);
+  my $YT4 = &YWORD($T4);
+  my $YT5 = &YWORD($T5);
+  my $YT6 = &YWORD($T6);
+
+  $code .= <<___;
+        vshufi32x4   \$0x00,@{[YWORD($HK)]},@{[YWORD($HK)]},$YT5
+        vmovdqa      $YT5,$YT4
+___
+
+  # ;; calculate HashKey^2<<1 mod poly
+  &GHASH_MUL($YT4, $YT5, $YT1, $YT2, $YT3);
+
+  $code .= <<___;
+        vmovdqu64         $T4,@{[HashKeyByIdx(2,$HTABLE)]}
+        vinserti64x2      \$1,$HK,$YT4,$YT5
+        vmovdqa64         $YT5,$YT6                             # ;; YT6 = HashKey | HashKey^2
+___
+
+  # ;; use 2x128-bit computation
+  # ;; calculate HashKey^4<<1 mod poly, HashKey^3<<1 mod poly
+  &GHASH_MUL($YT5, $YT4, $YT1, $YT2, $YT3);    # ;; YT5 = HashKey^3 | HashKey^4
+
+  $code .= <<___;
+        vmovdqu64         $YT5,@{[HashKeyByIdx(4,$HTABLE)]}
+
+        vinserti64x4      \$1,$YT6,$ZT5,$ZT5                    # ;; ZT5 = YT6 | YT5
+
+        # ;; switch to 4x128-bit computations now
+        vshufi64x2        \$0x00,$ZT5,$ZT5,$ZT4                 # ;; broadcast HashKey^4 across all ZT4
+        vmovdqa64         $ZT5,$ZT6                             # ;; save HashKey^4 to HashKey^1 in ZT6
+___
+
+  # ;; calculate HashKey^5<<1 mod poly, HashKey^6<<1 mod poly, ... HashKey^8<<1 mod poly
+  &GHASH_MUL($ZT5, $ZT4, $ZT1, $ZT2, $ZT3);
+  $code .= <<___;
+        vmovdqu64         $ZT5,@{[HashKeyByIdx(8,$HTABLE)]} # ;; HashKey^8 to HashKey^5 in ZT5 now
+        vshufi64x2        \$0x00,$ZT5,$ZT5,$ZT4                 # ;; broadcast HashKey^8 across all ZT4
+___
+
+  # ;; calculate HashKey^9<<1 mod poly, HashKey^10<<1 mod poly, ... HashKey^16<<1 mod poly
+  # ;; use HashKey^8 as multiplier against ZT6 and ZT5 - this allows deeper ooo execution
+
+  # ;; compute HashKey^(12), HashKey^(11), ... HashKey^(9)
+  &GHASH_MUL($ZT6, $ZT4, $ZT1, $ZT2, $ZT3);
+  $code .= "vmovdqu64         $ZT6,@{[HashKeyByIdx(12,$HTABLE)]}\n";
+
+  # ;; compute HashKey^(16), HashKey^(15), ... HashKey^(13)
+  &GHASH_MUL($ZT5, $ZT4, $ZT1, $ZT2, $ZT3);
+  $code .= "vmovdqu64         $ZT5,@{[HashKeyByIdx(16,$HTABLE)]}\n";
+
+  # ; Hkeys 17..48 will be precomputed somewhere else as context can hold only 16 hkeys
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;; READ_SMALL_DATA_INPUT
+# ;; Packs xmm register with data when data input is less or equal to 16 bytes
+# ;; Returns 0 if data has length 0
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+sub READ_SMALL_DATA_INPUT {
+  my $OUTPUT = $_[0];    # [out] xmm register
+  my $INPUT  = $_[1];    # [in] buffer pointer to read from
+  my $LENGTH = $_[2];    # [in] number of bytes to read
+  my $TMP1   = $_[3];    # [clobbered]
+  my $TMP2   = $_[4];    # [clobbered]
+  my $MASK   = $_[5];    # [out] k1 to k7 register to store the partial block mask
+
+  $code .= <<___;
+        mov               \$16,@{[DWORD($TMP2)]}
+        lea               byte_len_to_mask_table(%rip),$TMP1
+        cmp               $TMP2,$LENGTH
+        cmovc             $LENGTH,$TMP2
+___
+  if ($win64) {
+    $code .= <<___;
+        add               $TMP2,$TMP1
+        add               $TMP2,$TMP1
+        kmovw             ($TMP1),$MASK
+___
+  } else {
+    $code .= "kmovw           ($TMP1,$TMP2,2),$MASK\n";
+  }
+  $code .= "vmovdqu8          ($INPUT),${OUTPUT}{$MASK}{z}\n";
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+#  CALC_AAD_HASH: Calculates the hash of the data which will not be encrypted.
+#  Input: The input data (A_IN), that data's length (A_LEN), and the hash key (HASH_KEY).
+#  Output: The hash of the data (AAD_HASH).
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+sub CALC_AAD_HASH {
+  my $A_IN     = $_[0];     # [in] AAD text pointer
+  my $A_LEN    = $_[1];     # [in] AAD length
+  my $AAD_HASH = $_[2];     # [in/out] xmm ghash value
+  my $HTABLE   = $_[3];     # [in] pointer to hkeys table
+  my $ZT0      = $_[4];     # [clobbered] ZMM register
+  my $ZT1      = $_[5];     # [clobbered] ZMM register
+  my $ZT2      = $_[6];     # [clobbered] ZMM register
+  my $ZT3      = $_[7];     # [clobbered] ZMM register
+  my $ZT4      = $_[8];     # [clobbered] ZMM register
+  my $ZT5      = $_[9];     # [clobbered] ZMM register
+  my $ZT6      = $_[10];    # [clobbered] ZMM register
+  my $ZT7      = $_[11];    # [clobbered] ZMM register
+  my $ZT8      = $_[12];    # [clobbered] ZMM register
+  my $ZT9      = $_[13];    # [clobbered] ZMM register
+  my $ZT10     = $_[14];    # [clobbered] ZMM register
+  my $ZT11     = $_[15];    # [clobbered] ZMM register
+  my $ZT12     = $_[16];    # [clobbered] ZMM register
+  my $ZT13     = $_[17];    # [clobbered] ZMM register
+  my $ZT14     = $_[18];    # [clobbered] ZMM register
+  my $ZT15     = $_[19];    # [clobbered] ZMM register
+  my $ZT16     = $_[20];    # [clobbered] ZMM register
+  my $T1       = $_[21];    # [clobbered] GP register
+  my $T2       = $_[22];    # [clobbered] GP register
+  my $T3       = $_[23];    # [clobbered] GP register
+  my $MASKREG  = $_[24];    # [clobbered] mask register
+
+  my $HKEYS_READY = "%rbx";
+
+  my $SHFMSK = $ZT13;
+
+  my $rndsuffix = &random_string();
+
+  $code .= <<___;
+        mov               $A_IN,$T1      # ; T1 = AAD
+        mov               $A_LEN,$T2     # ; T2 = aadLen
+        or                $T2,$T2
+        jz                .L_CALC_AAD_done_${rndsuffix}
+
+        xor               $HKEYS_READY,$HKEYS_READY
+        vmovdqa64         SHUF_MASK(%rip),$SHFMSK
+
+.L_get_AAD_loop48x16_${rndsuffix}:
+        cmp               \$`(48*16)`,$T2
+        jl                .L_exit_AAD_loop48x16_${rndsuffix}
+___
+
+  $code .= <<___;
+        vmovdqu64         `64*0`($T1),$ZT1      # ; Blocks 0-3
+        vmovdqu64         `64*1`($T1),$ZT2      # ; Blocks 4-7
+        vmovdqu64         `64*2`($T1),$ZT3      # ; Blocks 8-11
+        vmovdqu64         `64*3`($T1),$ZT4      # ; Blocks 12-15
+        vpshufb           $SHFMSK,$ZT1,$ZT1
+        vpshufb           $SHFMSK,$ZT2,$ZT2
+        vpshufb           $SHFMSK,$ZT3,$ZT3
+        vpshufb           $SHFMSK,$ZT4,$ZT4
+___
+
+  &precompute_hkeys_on_stack($HTABLE, $HKEYS_READY, $ZT0, $ZT8, $ZT9, $ZT10, $ZT11, $ZT12, $ZT14, "all");
+  $code .= "mov     \$1,$HKEYS_READY\n";
+
+  &GHASH_16(
+    "start",        $ZT5,           $ZT6,           $ZT7,
+    "NO_INPUT_PTR", "NO_INPUT_PTR", "NO_INPUT_PTR", "%rsp",
+    &HashKeyOffsetByIdx(48, "frame"), 0, "@{[ZWORD($AAD_HASH)]}", $ZT0,
+    $ZT8,     $ZT9,  $ZT10, $ZT11,
+    $ZT12,    $ZT14, $ZT15, $ZT16,
+    "NO_ZMM", $ZT1,  $ZT2,  $ZT3,
+    $ZT4);
+
+  $code .= <<___;
+        vmovdqu64         `16*16 + 64*0`($T1),$ZT1      # ; Blocks 16-19
+        vmovdqu64         `16*16 + 64*1`($T1),$ZT2      # ; Blocks 20-23
+        vmovdqu64         `16*16 + 64*2`($T1),$ZT3      # ; Blocks 24-27
+        vmovdqu64         `16*16 + 64*3`($T1),$ZT4      # ; Blocks 28-31
+        vpshufb           $SHFMSK,$ZT1,$ZT1
+        vpshufb           $SHFMSK,$ZT2,$ZT2
+        vpshufb           $SHFMSK,$ZT3,$ZT3
+        vpshufb           $SHFMSK,$ZT4,$ZT4
+___
+
+  &GHASH_16(
+    "mid",          $ZT5,           $ZT6,           $ZT7,
+    "NO_INPUT_PTR", "NO_INPUT_PTR", "NO_INPUT_PTR", "%rsp",
+    &HashKeyOffsetByIdx(32, "frame"), 0, "NO_HASH_IN_OUT", $ZT0,
+    $ZT8,     $ZT9,  $ZT10, $ZT11,
+    $ZT12,    $ZT14, $ZT15, $ZT16,
+    "NO_ZMM", $ZT1,  $ZT2,  $ZT3,
+    $ZT4);
+
+  $code .= <<___;
+        vmovdqu64         `32*16 + 64*0`($T1),$ZT1      # ; Blocks 32-35
+        vmovdqu64         `32*16 + 64*1`($T1),$ZT2      # ; Blocks 36-39
+        vmovdqu64         `32*16 + 64*2`($T1),$ZT3      # ; Blocks 40-43
+        vmovdqu64         `32*16 + 64*3`($T1),$ZT4      # ; Blocks 44-47
+        vpshufb           $SHFMSK,$ZT1,$ZT1
+        vpshufb           $SHFMSK,$ZT2,$ZT2
+        vpshufb           $SHFMSK,$ZT3,$ZT3
+        vpshufb           $SHFMSK,$ZT4,$ZT4
+___
+
+  &GHASH_16(
+    "end_reduce",   $ZT5,           $ZT6,           $ZT7,
+    "NO_INPUT_PTR", "NO_INPUT_PTR", "NO_INPUT_PTR", "%rsp",
+    &HashKeyOffsetByIdx(16, "frame"), 0, &ZWORD($AAD_HASH), $ZT0,
+    $ZT8,     $ZT9,  $ZT10, $ZT11,
+    $ZT12,    $ZT14, $ZT15, $ZT16,
+    "NO_ZMM", $ZT1,  $ZT2,  $ZT3,
+    $ZT4);
+
+  $code .= <<___;
+        sub               \$`(48*16)`,$T2
+        je                .L_CALC_AAD_done_${rndsuffix}
+
+        add               \$`(48*16)`,$T1
+        jmp               .L_get_AAD_loop48x16_${rndsuffix}
+
+.L_exit_AAD_loop48x16_${rndsuffix}:
+        # ; Less than 48x16 bytes remaining
+        cmp               \$`(32*16)`,$T2
+        jl                .L_less_than_32x16_${rndsuffix}
+___
+
+  $code .= <<___;
+        # ; Get next 16 blocks
+        vmovdqu64         `64*0`($T1),$ZT1
+        vmovdqu64         `64*1`($T1),$ZT2
+        vmovdqu64         `64*2`($T1),$ZT3
+        vmovdqu64         `64*3`($T1),$ZT4
+        vpshufb           $SHFMSK,$ZT1,$ZT1
+        vpshufb           $SHFMSK,$ZT2,$ZT2
+        vpshufb           $SHFMSK,$ZT3,$ZT3
+        vpshufb           $SHFMSK,$ZT4,$ZT4
+___
+
+  &precompute_hkeys_on_stack($HTABLE, $HKEYS_READY, $ZT0, $ZT8, $ZT9, $ZT10, $ZT11, $ZT12, $ZT14, "first32");
+  $code .= "mov     \$1,$HKEYS_READY\n";
+
+  &GHASH_16(
+    "start",        $ZT5,           $ZT6,           $ZT7,
+    "NO_INPUT_PTR", "NO_INPUT_PTR", "NO_INPUT_PTR", "%rsp",
+    &HashKeyOffsetByIdx(32, "frame"), 0, &ZWORD($AAD_HASH), $ZT0,
+    $ZT8,     $ZT9,  $ZT10, $ZT11,
+    $ZT12,    $ZT14, $ZT15, $ZT16,
+    "NO_ZMM", $ZT1,  $ZT2,  $ZT3,
+    $ZT4);
+
+  $code .= <<___;
+        vmovdqu64         `16*16 + 64*0`($T1),$ZT1
+        vmovdqu64         `16*16 + 64*1`($T1),$ZT2
+        vmovdqu64         `16*16 + 64*2`($T1),$ZT3
+        vmovdqu64         `16*16 + 64*3`($T1),$ZT4
+        vpshufb           $SHFMSK,$ZT1,$ZT1
+        vpshufb           $SHFMSK,$ZT2,$ZT2
+        vpshufb           $SHFMSK,$ZT3,$ZT3
+        vpshufb           $SHFMSK,$ZT4,$ZT4
+___
+
+  &GHASH_16(
+    "end_reduce",   $ZT5,           $ZT6,           $ZT7,
+    "NO_INPUT_PTR", "NO_INPUT_PTR", "NO_INPUT_PTR", "%rsp",
+    &HashKeyOffsetByIdx(16, "frame"), 0, &ZWORD($AAD_HASH), $ZT0,
+    $ZT8,     $ZT9,  $ZT10, $ZT11,
+    $ZT12,    $ZT14, $ZT15, $ZT16,
+    "NO_ZMM", $ZT1,  $ZT2,  $ZT3,
+    $ZT4);
+
+  $code .= <<___;
+        sub               \$`(32*16)`,$T2
+        je                .L_CALC_AAD_done_${rndsuffix}
+
+        add               \$`(32*16)`,$T1
+        jmp               .L_less_than_16x16_${rndsuffix}
+
+.L_less_than_32x16_${rndsuffix}:
+        cmp               \$`(16*16)`,$T2
+        jl                .L_less_than_16x16_${rndsuffix}
+        # ; Get next 16 blocks
+        vmovdqu64         `64*0`($T1),$ZT1
+        vmovdqu64         `64*1`($T1),$ZT2
+        vmovdqu64         `64*2`($T1),$ZT3
+        vmovdqu64         `64*3`($T1),$ZT4
+        vpshufb           $SHFMSK,$ZT1,$ZT1
+        vpshufb           $SHFMSK,$ZT2,$ZT2
+        vpshufb           $SHFMSK,$ZT3,$ZT3
+        vpshufb           $SHFMSK,$ZT4,$ZT4
+___
+
+  # ; This code path does not use more than 16 hkeys, so they can be taken from the context
+  # ; (not from the stack storage)
+  &GHASH_16(
+    "start_reduce", $ZT5,           $ZT6,           $ZT7,
+    "NO_INPUT_PTR", "NO_INPUT_PTR", "NO_INPUT_PTR", $HTABLE,
+    &HashKeyOffsetByIdx(16, "context"), 0, &ZWORD($AAD_HASH), $ZT0,
+    $ZT8,     $ZT9,  $ZT10, $ZT11,
+    $ZT12,    $ZT14, $ZT15, $ZT16,
+    "NO_ZMM", $ZT1,  $ZT2,  $ZT3,
+    $ZT4);
+
+  $code .= <<___;
+        sub               \$`(16*16)`,$T2
+        je                .L_CALC_AAD_done_${rndsuffix}
+
+        add               \$`(16*16)`,$T1
+        # ; Less than 16x16 bytes remaining
+.L_less_than_16x16_${rndsuffix}:
+        # ;; prep mask source address
+        lea               byte64_len_to_mask_table(%rip),$T3
+        lea               ($T3,$T2,8),$T3
+
+        # ;; calculate number of blocks to ghash (including partial bytes)
+        add               \$15,@{[DWORD($T2)]}
+        shr               \$4,@{[DWORD($T2)]}
+        cmp               \$2,@{[DWORD($T2)]}
+        jb                .L_AAD_blocks_1_${rndsuffix}
+        je                .L_AAD_blocks_2_${rndsuffix}
+        cmp               \$4,@{[DWORD($T2)]}
+        jb                .L_AAD_blocks_3_${rndsuffix}
+        je                .L_AAD_blocks_4_${rndsuffix}
+        cmp               \$6,@{[DWORD($T2)]}
+        jb                .L_AAD_blocks_5_${rndsuffix}
+        je                .L_AAD_blocks_6_${rndsuffix}
+        cmp               \$8,@{[DWORD($T2)]}
+        jb                .L_AAD_blocks_7_${rndsuffix}
+        je                .L_AAD_blocks_8_${rndsuffix}
+        cmp               \$10,@{[DWORD($T2)]}
+        jb                .L_AAD_blocks_9_${rndsuffix}
+        je                .L_AAD_blocks_10_${rndsuffix}
+        cmp               \$12,@{[DWORD($T2)]}
+        jb                .L_AAD_blocks_11_${rndsuffix}
+        je                .L_AAD_blocks_12_${rndsuffix}
+        cmp               \$14,@{[DWORD($T2)]}
+        jb                .L_AAD_blocks_13_${rndsuffix}
+        je                .L_AAD_blocks_14_${rndsuffix}
+        cmp               \$15,@{[DWORD($T2)]}
+        je                .L_AAD_blocks_15_${rndsuffix}
+___
+
+  # ;; fall through for 16 blocks
+
+  # ;; The flow of each of these cases is identical:
+  # ;; - load blocks plain text
+  # ;; - shuffle loaded blocks
+  # ;; - xor in current hash value into block 0
+  # ;; - perform up multiplications with ghash keys
+  # ;; - jump to reduction code
+
+  for (my $aad_blocks = 16; $aad_blocks > 0; $aad_blocks--) {
+    $code .= ".L_AAD_blocks_${aad_blocks}_${rndsuffix}:\n";
+    if ($aad_blocks > 12) {
+      $code .= "sub               \$`12*16*8`, $T3\n";
+    } elsif ($aad_blocks > 8) {
+      $code .= "sub               \$`8*16*8`, $T3\n";
+    } elsif ($aad_blocks > 4) {
+      $code .= "sub               \$`4*16*8`, $T3\n";
+    }
+    $code .= "kmovq             ($T3),$MASKREG\n";
+
+    &ZMM_LOAD_MASKED_BLOCKS_0_16($aad_blocks, $T1, 0, $ZT1, $ZT2, $ZT3, $ZT4, $MASKREG);
+
+    &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16($aad_blocks, "vpshufb", $ZT1, $ZT2, $ZT3, $ZT4,
+      $ZT1, $ZT2, $ZT3, $ZT4, $SHFMSK, $SHFMSK, $SHFMSK, $SHFMSK);
+
+    &GHASH_1_TO_16($HTABLE, &ZWORD($AAD_HASH),
+      $ZT0, $ZT5, $ZT6, $ZT7, $ZT8, $ZT9, $ZT10, $ZT11, $ZT12, &ZWORD($AAD_HASH), $ZT1, $ZT2, $ZT3, $ZT4, $aad_blocks);
+
+    if ($aad_blocks > 1) {
+
+      # ;; fall through to CALC_AAD_done in 1 block case
+      $code .= "jmp           .L_CALC_AAD_done_${rndsuffix}\n";
+    }
+
+  }
+  $code .= ".L_CALC_AAD_done_${rndsuffix}:\n";
+
+  # ;; result in AAD_HASH
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;; PARTIAL_BLOCK
+# ;; Handles encryption/decryption and the tag partial blocks between
+# ;; update calls.
+# ;; Requires the input data be at least 1 byte long.
+# ;; Output:
+# ;; A cipher/plain of the first partial block (CIPH_PLAIN_OUT),
+# ;; AAD_HASH and updated GCM128_CTX
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+sub PARTIAL_BLOCK {
+  my $GCM128_CTX     = $_[0];     # [in] key pointer
+  my $PBLOCK_LEN     = $_[1];     # [in] partial block length
+  my $CIPH_PLAIN_OUT = $_[2];     # [in] output buffer
+  my $PLAIN_CIPH_IN  = $_[3];     # [in] input buffer
+  my $PLAIN_CIPH_LEN = $_[4];     # [in] buffer length
+  my $DATA_OFFSET    = $_[5];     # [out] data offset (gets set)
+  my $AAD_HASH       = $_[6];     # [out] updated GHASH value
+  my $ENC_DEC        = $_[7];     # [in] cipher direction
+  my $GPTMP0         = $_[8];     # [clobbered] GP temporary register
+  my $GPTMP1         = $_[9];     # [clobbered] GP temporary register
+  my $GPTMP2         = $_[10];    # [clobbered] GP temporary register
+  my $ZTMP0          = $_[11];    # [clobbered] ZMM temporary register
+  my $ZTMP1          = $_[12];    # [clobbered] ZMM temporary register
+  my $ZTMP2          = $_[13];    # [clobbered] ZMM temporary register
+  my $ZTMP3          = $_[14];    # [clobbered] ZMM temporary register
+  my $ZTMP4          = $_[15];    # [clobbered] ZMM temporary register
+  my $ZTMP5          = $_[16];    # [clobbered] ZMM temporary register
+  my $ZTMP6          = $_[17];    # [clobbered] ZMM temporary register
+  my $ZTMP7          = $_[18];    # [clobbered] ZMM temporary register
+  my $MASKREG        = $_[19];    # [clobbered] mask temporary register
+
+  my $XTMP0 = &XWORD($ZTMP0);
+  my $XTMP1 = &XWORD($ZTMP1);
+  my $XTMP2 = &XWORD($ZTMP2);
+  my $XTMP3 = &XWORD($ZTMP3);
+  my $XTMP4 = &XWORD($ZTMP4);
+  my $XTMP5 = &XWORD($ZTMP5);
+  my $XTMP6 = &XWORD($ZTMP6);
+  my $XTMP7 = &XWORD($ZTMP7);
+
+  my $LENGTH = $DATA_OFFSET;
+  my $IA0    = $GPTMP1;
+  my $IA1    = $GPTMP2;
+  my $IA2    = $GPTMP0;
+
+  my $HTABLE = $IA2;
+
+  my $rndsuffix = &random_string();
+
+  $code .= <<___;
+        # ;; if no partial block present then LENGTH/DATA_OFFSET will be set to zero
+        mov             ($PBLOCK_LEN),@{[DWORD($LENGTH)]}
+        or              $LENGTH,$LENGTH
+        je              .L_partial_block_done_${rndsuffix}         #  ;Leave Macro if no partial blocks
+___
+
+  &READ_SMALL_DATA_INPUT($XTMP0, $PLAIN_CIPH_IN, $PLAIN_CIPH_LEN, $IA0, $IA2, $MASKREG);
+
+  $code .= <<___;
+        # ;; XTMP1 = my_ctx_data.partial_block_enc_key
+        vmovdqu64         $CTX_OFFSET_PEncBlock($GCM128_CTX),$XTMP1
+        # ;; Get Htable pointer
+        lea               `$CTX_OFFSET_HTable`($GCM128_CTX),$HTABLE
+        vmovdqu64         @{[HashKeyByIdx(1, $HTABLE)]},$XTMP2
+
+        # ;; adjust the shuffle mask pointer to be able to shift right $LENGTH bytes
+        # ;; (16 - $LENGTH) is the number of bytes in plaintext mod 16)
+        lea               SHIFT_MASK(%rip),$IA0
+        add               $LENGTH,$IA0
+        vmovdqu64         ($IA0),$XTMP3         # ; shift right shuffle mask
+        vpshufb           $XTMP3,$XTMP1,$XTMP1
+___
+
+  if ($ENC_DEC eq "DEC") {
+    $code .= <<___;
+        # ;;  keep copy of cipher text in $XTMP4
+        vmovdqa64         $XTMP0,$XTMP4
+___
+  }
+  $code .= <<___;
+        vpxorq            $XTMP0,$XTMP1,$XTMP1  # ; Ciphertext XOR E(K, Yn)
+        # ;; Set $IA1 to be the amount of data left in CIPH_PLAIN_IN after filling the block
+        # ;; Determine if partial block is not being filled and shift mask accordingly
+___
+  if ($win64) {
+    $code .= <<___;
+        mov               $PLAIN_CIPH_LEN,$IA1
+        add               $LENGTH,$IA1
+___
+  } else {
+    $code .= "lea               ($PLAIN_CIPH_LEN, $LENGTH, 1),$IA1\n";
+  }
+  $code .= <<___;
+        sub               \$16,$IA1
+        jge               .L_no_extra_mask_${rndsuffix}
+        sub               $IA1,$IA0
+.L_no_extra_mask_${rndsuffix}:
+        # ;; get the appropriate mask to mask out bottom $LENGTH bytes of $XTMP1
+        # ;; - mask out bottom $LENGTH bytes of $XTMP1
+        # ;; sizeof(SHIFT_MASK) == 16 bytes
+        vmovdqu64         16($IA0),$XTMP0
+        vpand             $XTMP0,$XTMP1,$XTMP1
+___
+
+  if ($ENC_DEC eq "DEC") {
+    $code .= <<___;
+        vpand             $XTMP0,$XTMP4,$XTMP4
+        vpshufb           SHUF_MASK(%rip),$XTMP4,$XTMP4
+        vpshufb           $XTMP3,$XTMP4,$XTMP4
+        vpxorq            $XTMP4,$AAD_HASH,$AAD_HASH
+___
+  } else {
+    $code .= <<___;
+        vpshufb           SHUF_MASK(%rip),$XTMP1,$XTMP1
+        vpshufb           $XTMP3,$XTMP1,$XTMP1
+        vpxorq            $XTMP1,$AAD_HASH,$AAD_HASH
+___
+  }
+  $code .= <<___;
+        cmp               \$0,$IA1
+        jl                .L_partial_incomplete_${rndsuffix}
+___
+
+  # ;; GHASH computation for the last <16 Byte block
+  &GHASH_MUL($AAD_HASH, $XTMP2, $XTMP5, $XTMP6, $XTMP7);
+
+  $code .= <<___;
+        movl              \$0, ($PBLOCK_LEN)
+        # ;;  Set $LENGTH to be the number of bytes to write out
+        mov               $LENGTH,$IA0
+        mov               \$16,$LENGTH
+        sub               $IA0,$LENGTH
+        jmp               .L_enc_dec_done_${rndsuffix}
+
+.L_partial_incomplete_${rndsuffix}:
+___
+  if ($win64) {
+    $code .= <<___;
+        mov               $PLAIN_CIPH_LEN,$IA0
+        add               @{[DWORD($IA0)]},($PBLOCK_LEN)
+___
+  } else {
+    $code .= "add               @{[DWORD($PLAIN_CIPH_LEN)]},($PBLOCK_LEN)\n";
+  }
+  $code .= <<___;
+        mov               $PLAIN_CIPH_LEN,$LENGTH
+
+.L_enc_dec_done_${rndsuffix}:
+        # ;; output encrypted Bytes
+
+        lea               byte_len_to_mask_table(%rip),$IA0
+        kmovw             ($IA0,$LENGTH,2),$MASKREG
+___
+
+  if ($ENC_DEC eq "ENC") {
+    $code .= <<___;
+        # ;; shuffle XTMP1 back to output as ciphertext
+        vpshufb           SHUF_MASK(%rip),$XTMP1,$XTMP1
+        vpshufb           $XTMP3,$XTMP1,$XTMP1
+___
+  }
+  $code .= <<___;
+        mov               $CIPH_PLAIN_OUT,$IA0
+        vmovdqu8          $XTMP1,($IA0){$MASKREG}
+.L_partial_block_done_${rndsuffix}:
+___
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;; Ciphers 1 to 16 blocks and prepares them for later GHASH compute operation
+sub INITIAL_BLOCKS_PARTIAL_CIPHER {
+  my $AES_KEYS        = $_[0];     # [in] key pointer
+  my $CIPH_PLAIN_OUT  = $_[1];     # [in] text output pointer
+  my $PLAIN_CIPH_IN   = $_[2];     # [in] text input pointer
+  my $LENGTH          = $_[3];     # [in/clobbered] length in bytes
+  my $DATA_OFFSET     = $_[4];     # [in/out] current data offset (updated)
+  my $NUM_BLOCKS      = $_[5];     # [in] can only be 1, 2, 3, 4, 5, ..., 15 or 16 (not 0)
+  my $CTR             = $_[6];     # [in/out] current counter value
+  my $ENC_DEC         = $_[7];     # [in] cipher direction (ENC/DEC)
+  my $DAT0            = $_[8];     # [out] ZMM with cipher text shuffled for GHASH
+  my $DAT1            = $_[9];     # [out] ZMM with cipher text shuffled for GHASH
+  my $DAT2            = $_[10];    # [out] ZMM with cipher text shuffled for GHASH
+  my $DAT3            = $_[11];    # [out] ZMM with cipher text shuffled for GHASH
+  my $LAST_CIPHER_BLK = $_[12];    # [out] XMM to put ciphered counter block partially xor'ed with text
+  my $LAST_GHASH_BLK  = $_[13];    # [out] XMM to put last cipher text block shuffled for GHASH
+  my $CTR0            = $_[14];    # [clobbered] ZMM temporary
+  my $CTR1            = $_[15];    # [clobbered] ZMM temporary
+  my $CTR2            = $_[16];    # [clobbered] ZMM temporary
+  my $CTR3            = $_[17];    # [clobbered] ZMM temporary
+  my $ZT1             = $_[18];    # [clobbered] ZMM temporary
+  my $IA0             = $_[19];    # [clobbered] GP temporary
+  my $IA1             = $_[20];    # [clobbered] GP temporary
+  my $MASKREG         = $_[21];    # [clobbered] mask register
+  my $SHUFMASK        = $_[22];    # [out] ZMM loaded with BE/LE shuffle mask
+
+  if ($NUM_BLOCKS == 1) {
+    $code .= "vmovdqa64         SHUF_MASK(%rip),@{[XWORD($SHUFMASK)]}\n";
+  } elsif ($NUM_BLOCKS == 2) {
+    $code .= "vmovdqa64         SHUF_MASK(%rip),@{[YWORD($SHUFMASK)]}\n";
+  } else {
+    $code .= "vmovdqa64         SHUF_MASK(%rip),$SHUFMASK\n";
+  }
+
+  # ;; prepare AES counter blocks
+  if ($NUM_BLOCKS == 1) {
+    $code .= "vpaddd            ONE(%rip),$CTR,@{[XWORD($CTR0)]}\n";
+  } elsif ($NUM_BLOCKS == 2) {
+    $code .= <<___;
+        vshufi64x2        \$0,@{[YWORD($CTR)]},@{[YWORD($CTR)]},@{[YWORD($CTR0)]}
+        vpaddd            ddq_add_1234(%rip),@{[YWORD($CTR0)]},@{[YWORD($CTR0)]}
+___
+  } else {
+    $code .= <<___;
+        vshufi64x2        \$0,@{[ZWORD($CTR)]},@{[ZWORD($CTR)]},@{[ZWORD($CTR)]}
+        vpaddd            ddq_add_1234(%rip),@{[ZWORD($CTR)]},$CTR0
+___
+    if ($NUM_BLOCKS > 4) {
+      $code .= "vpaddd            ddq_add_5678(%rip),@{[ZWORD($CTR)]},$CTR1\n";
+    }
+    if ($NUM_BLOCKS > 8) {
+      $code .= "vpaddd            ddq_add_8888(%rip),$CTR0,$CTR2\n";
+    }
+    if ($NUM_BLOCKS > 12) {
+      $code .= "vpaddd            ddq_add_8888(%rip),$CTR1,$CTR3\n";
+    }
+  }
+
+  # ;; get load/store mask
+  $code .= <<___;
+        lea               byte64_len_to_mask_table(%rip),$IA0
+        mov               $LENGTH,$IA1
+___
+  if ($NUM_BLOCKS > 12) {
+    $code .= "sub               \$`3*64`,$IA1\n";
+  } elsif ($NUM_BLOCKS > 8) {
+    $code .= "sub               \$`2*64`,$IA1\n";
+  } elsif ($NUM_BLOCKS > 4) {
+    $code .= "sub               \$`1*64`,$IA1\n";
+  }
+  $code .= "kmovq             ($IA0,$IA1,8),$MASKREG\n";
+
+  # ;; extract new counter value
+  # ;; shuffle the counters for AES rounds
+  if ($NUM_BLOCKS <= 4) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 1)`,$CTR0,$CTR\n";
+  } elsif ($NUM_BLOCKS <= 8) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 5)`,$CTR1,$CTR\n";
+  } elsif ($NUM_BLOCKS <= 12) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 9)`,$CTR2,$CTR\n";
+  } else {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 13)`,$CTR3,$CTR\n";
+  }
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vpshufb", $CTR0, $CTR1,     $CTR2,     $CTR3,     $CTR0,
+    $CTR1,       $CTR2,     $CTR3, $SHUFMASK, $SHUFMASK, $SHUFMASK, $SHUFMASK);
+
+  # ;; load plain/cipher text
+  &ZMM_LOAD_MASKED_BLOCKS_0_16($NUM_BLOCKS, $PLAIN_CIPH_IN, $DATA_OFFSET, $DAT0, $DAT1, $DAT2, $DAT3, $MASKREG);
+
+  # ;; AES rounds and XOR with plain/cipher text
+  foreach my $j (0 .. ($NROUNDS + 1)) {
+    $code .= "vbroadcastf64x2    `($j * 16)`($AES_KEYS),$ZT1\n";
+    &ZMM_AESENC_ROUND_BLOCKS_0_16($CTR0, $CTR1, $CTR2, $CTR3, $ZT1, $j,
+      $DAT0, $DAT1, $DAT2, $DAT3, $NUM_BLOCKS, $NROUNDS);
+  }
+
+  # ;; retrieve the last cipher counter block (partially XOR'ed with text)
+  # ;; - this is needed for partial block cases
+  if ($NUM_BLOCKS <= 4) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 1)`,$CTR0,$LAST_CIPHER_BLK\n";
+  } elsif ($NUM_BLOCKS <= 8) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 5)`,$CTR1,$LAST_CIPHER_BLK\n";
+  } elsif ($NUM_BLOCKS <= 12) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 9)`,$CTR2,$LAST_CIPHER_BLK\n";
+  } else {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 13)`,$CTR3,$LAST_CIPHER_BLK\n";
+  }
+
+  # ;; write cipher/plain text back to output and
+  $code .= "mov       $CIPH_PLAIN_OUT,$IA0\n";
+  &ZMM_STORE_MASKED_BLOCKS_0_16($NUM_BLOCKS, $IA0, $DATA_OFFSET, $CTR0, $CTR1, $CTR2, $CTR3, $MASKREG);
+
+  # ;; zero bytes outside the mask before hashing
+  if ($NUM_BLOCKS <= 4) {
+    $code .= "vmovdqu8          $CTR0,${CTR0}{$MASKREG}{z}\n";
+  } elsif ($NUM_BLOCKS <= 8) {
+    $code .= "vmovdqu8          $CTR1,${CTR1}{$MASKREG}{z}\n";
+  } elsif ($NUM_BLOCKS <= 12) {
+    $code .= "vmovdqu8          $CTR2,${CTR2}{$MASKREG}{z}\n";
+  } else {
+    $code .= "vmovdqu8          $CTR3,${CTR3}{$MASKREG}{z}\n";
+  }
+
+  # ;; Shuffle the cipher text blocks for hashing part
+  # ;; ZT5 and ZT6 are expected outputs with blocks for hashing
+  if ($ENC_DEC eq "DEC") {
+
+    # ;; Decrypt case
+    # ;; - cipher blocks are in ZT5 & ZT6
+    &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+      $NUM_BLOCKS, "vpshufb", $DAT0, $DAT1,     $DAT2,     $DAT3,     $DAT0,
+      $DAT1,       $DAT2,     $DAT3, $SHUFMASK, $SHUFMASK, $SHUFMASK, $SHUFMASK);
+  } else {
+
+    # ;; Encrypt case
+    # ;; - cipher blocks are in CTR0-CTR3
+    &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+      $NUM_BLOCKS, "vpshufb", $DAT0, $DAT1,     $DAT2,     $DAT3,     $CTR0,
+      $CTR1,       $CTR2,     $CTR3, $SHUFMASK, $SHUFMASK, $SHUFMASK, $SHUFMASK);
+  }
+
+  # ;; Extract the last block for partials and multi_call cases
+  if ($NUM_BLOCKS <= 4) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS-1)`,$DAT0,$LAST_GHASH_BLK\n";
+  } elsif ($NUM_BLOCKS <= 8) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS-5)`,$DAT1,$LAST_GHASH_BLK\n";
+  } elsif ($NUM_BLOCKS <= 12) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS-9)`,$DAT2,$LAST_GHASH_BLK\n";
+  } else {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS-13)`,$DAT3,$LAST_GHASH_BLK\n";
+  }
+
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;; Computes GHASH on 1 to 16 blocks
+sub INITIAL_BLOCKS_PARTIAL_GHASH {
+  my $AES_KEYS        = $_[0];     # [in] key pointer
+  my $GCM128_CTX      = $_[1];     # [in] context pointer
+  my $LENGTH          = $_[2];     # [in/clobbered] length in bytes
+  my $NUM_BLOCKS      = $_[3];     # [in] can only be 1, 2, 3, 4, 5, ..., 15 or 16 (not 0)
+  my $HASH_IN_OUT     = $_[4];     # [in/out] XMM ghash in/out value
+  my $ENC_DEC         = $_[5];     # [in] cipher direction (ENC/DEC)
+  my $DAT0            = $_[6];     # [in] ZMM with cipher text shuffled for GHASH
+  my $DAT1            = $_[7];     # [in] ZMM with cipher text shuffled for GHASH
+  my $DAT2            = $_[8];     # [in] ZMM with cipher text shuffled for GHASH
+  my $DAT3            = $_[9];     # [in] ZMM with cipher text shuffled for GHASH
+  my $LAST_CIPHER_BLK = $_[10];    # [in] XMM with ciphered counter block partially xor'ed with text
+  my $LAST_GHASH_BLK  = $_[11];    # [in] XMM with last cipher text block shuffled for GHASH
+  my $ZT0             = $_[12];    # [clobbered] ZMM temporary
+  my $ZT1             = $_[13];    # [clobbered] ZMM temporary
+  my $ZT2             = $_[14];    # [clobbered] ZMM temporary
+  my $ZT3             = $_[15];    # [clobbered] ZMM temporary
+  my $ZT4             = $_[16];    # [clobbered] ZMM temporary
+  my $ZT5             = $_[17];    # [clobbered] ZMM temporary
+  my $ZT6             = $_[18];    # [clobbered] ZMM temporary
+  my $ZT7             = $_[19];    # [clobbered] ZMM temporary
+  my $ZT8             = $_[20];    # [clobbered] ZMM temporary
+  my $GPR1            = $_[21];    # [clobbered] GPR register
+  my $PBLOCK_LEN      = $_[22];    # [in] partial block length
+  my $GH              = $_[23];    # [in] ZMM with hi product part
+  my $GM              = $_[24];    # [in] ZMM with mid prodcut part
+  my $GL              = $_[25];    # [in] ZMM with lo product part
+
+  my $HTABLE = $GPR1;
+
+  my $rndsuffix = &random_string();
+
+  # ;; Get Htable pointer
+  $code .= "lea               `$CTX_OFFSET_HTable`($GCM128_CTX),$HTABLE\n";
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;;; - Hash all but the last partial block of data
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+  # ;; update data offset
+  if ($NUM_BLOCKS > 1) {
+
+    # ;; The final block of data may be <16B
+    $code .= "sub               \$16 * ($NUM_BLOCKS - 1),$LENGTH\n";
+  }
+
+  if ($NUM_BLOCKS < 16) {
+    $code .= <<___;
+        # ;; NOTE: the 'jl' is always taken for num_initial_blocks = 16.
+        # ;;      This is run in the context of GCM_ENC_DEC_SMALL for length < 256.
+        cmp               \$16,$LENGTH
+        jl                .L_small_initial_partial_block_${rndsuffix}
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;;; Handle a full length final block - encrypt and hash all blocks
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+        sub               \$16,$LENGTH
+        movl              \$0,($PBLOCK_LEN)
+___
+
+    # ;; Hash all of the data
+    if (scalar(@_) == 23) {
+
+      # ;; start GHASH compute
+      &GHASH_1_TO_16($HTABLE, $HASH_IN_OUT, $ZT0, $ZT1, $ZT2, $ZT3, $ZT4,
+        $ZT5, $ZT6, $ZT7, $ZT8, &ZWORD($HASH_IN_OUT), $DAT0, $DAT1, $DAT2, $DAT3, $NUM_BLOCKS);
+    } elsif (scalar(@_) == 26) {
+
+      # ;; continue GHASH compute
+      &GHASH_1_TO_16($HTABLE, $HASH_IN_OUT, $ZT0, $ZT1, $ZT2, $ZT3, $ZT4,
+        $ZT5, $ZT6, $ZT7, $ZT8, &ZWORD($HASH_IN_OUT), $DAT0, $DAT1, $DAT2, $DAT3, $NUM_BLOCKS, $GH, $GM, $GL);
+    }
+    $code .= "jmp           .L_small_initial_compute_done_${rndsuffix}\n";
+  }
+
+  $code .= <<___;
+.L_small_initial_partial_block_${rndsuffix}:
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;;; Handle ghash for a <16B final block
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+        # ;; As it's an init / update / finalize series we need to leave the
+        # ;; last block if it's less than a full block of data.
+
+        mov               @{[DWORD($LENGTH)]},($PBLOCK_LEN)
+        vmovdqu64         $LAST_CIPHER_BLK,$CTX_OFFSET_PEncBlock($GCM128_CTX)
+___
+
+  my $k                  = ($NUM_BLOCKS - 1);
+  my $last_block_to_hash = 1;
+  if (($NUM_BLOCKS > $last_block_to_hash)) {
+
+    # ;; ZT12-ZT20 - temporary registers
+    if (scalar(@_) == 23) {
+
+      # ;; start GHASH compute
+      &GHASH_1_TO_16($HTABLE, $HASH_IN_OUT, $ZT0, $ZT1, $ZT2, $ZT3, $ZT4,
+        $ZT5, $ZT6, $ZT7, $ZT8, &ZWORD($HASH_IN_OUT), $DAT0, $DAT1, $DAT2, $DAT3, $k);
+    } elsif (scalar(@_) == 26) {
+
+      # ;; continue GHASH compute
+      &GHASH_1_TO_16($HTABLE, $HASH_IN_OUT, $ZT0, $ZT1, $ZT2, $ZT3, $ZT4,
+        $ZT5, $ZT6, $ZT7, $ZT8, &ZWORD($HASH_IN_OUT), $DAT0, $DAT1, $DAT2, $DAT3, $k, $GH, $GM, $GL);
+    }
+
+    # ;; just fall through no jmp needed
+  } else {
+
+    if (scalar(@_) == 26) {
+      $code .= <<___;
+        # ;; Reduction is required in this case.
+        # ;; Integrate GM into GH and GL.
+        vpsrldq           \$8,$GM,$ZT0
+        vpslldq           \$8,$GM,$ZT1
+        vpxorq            $ZT0,$GH,$GH
+        vpxorq            $ZT1,$GL,$GL
+___
+
+      # ;; Add GH and GL 128-bit words horizontally
+      &VHPXORI4x128($GH, $ZT0);
+      &VHPXORI4x128($GL, $ZT1);
+
+      # ;; 256-bit to 128-bit reduction
+      $code .= "vmovdqa64         POLY2(%rip),@{[XWORD($ZT0)]}\n";
+      &VCLMUL_REDUCE(&XWORD($HASH_IN_OUT), &XWORD($ZT0), &XWORD($GH), &XWORD($GL), &XWORD($ZT1), &XWORD($ZT2));
+    }
+    $code .= <<___;
+        # ;; Record that a reduction is not needed -
+        # ;; In this case no hashes are computed because there
+        # ;; is only one initial block and it is < 16B in length.
+        # ;; We only need to check if a reduction is needed if
+        # ;; initial_blocks == 1 and init/update/final is being used.
+        # ;; In this case we may just have a partial block, and that
+        # ;; gets hashed in finalize.
+
+        # ;; The hash should end up in HASH_IN_OUT.
+        # ;; The only way we should get here is if there is
+        # ;; a partial block of data, so xor that into the hash.
+        vpxorq            $LAST_GHASH_BLK,$HASH_IN_OUT,$HASH_IN_OUT
+        # ;; The result is in $HASH_IN_OUT
+        jmp               .L_after_reduction_${rndsuffix}
+___
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;;; After GHASH reduction
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+  $code .= ".L_small_initial_compute_done_${rndsuffix}:\n";
+
+  # ;; If using init/update/finalize, we need to xor any partial block data
+  # ;; into the hash.
+  if ($NUM_BLOCKS > 1) {
+
+    # ;; NOTE: for $NUM_BLOCKS = 0 the xor never takes place
+    if ($NUM_BLOCKS != 16) {
+      $code .= <<___;
+        # ;; NOTE: for $NUM_BLOCKS = 16, $LENGTH, stored in [PBlockLen] is never zero
+        or                $LENGTH,$LENGTH
+        je                .L_after_reduction_${rndsuffix}
+___
+    }
+    $code .= "vpxorq            $LAST_GHASH_BLK,$HASH_IN_OUT,$HASH_IN_OUT\n";
+  }
+
+  $code .= ".L_after_reduction_${rndsuffix}:\n";
+
+  # ;; Final hash is now in HASH_IN_OUT
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;; INITIAL_BLOCKS_PARTIAL macro with support for a partial final block.
+# ;; It may look similar to INITIAL_BLOCKS but its usage is different:
+# ;; - first encrypts/decrypts required number of blocks and then
+# ;;   ghashes these blocks
+# ;; - Small packets or left over data chunks (<256 bytes)
+# ;; - Remaining data chunks below 256 bytes (multi buffer code)
+# ;;
+# ;; num_initial_blocks is expected to include the partial final block
+# ;; in the count.
+sub INITIAL_BLOCKS_PARTIAL {
+  my $AES_KEYS        = $_[0];     # [in] key pointer
+  my $GCM128_CTX      = $_[1];     # [in] context pointer
+  my $CIPH_PLAIN_OUT  = $_[2];     # [in] text output pointer
+  my $PLAIN_CIPH_IN   = $_[3];     # [in] text input pointer
+  my $LENGTH          = $_[4];     # [in/clobbered] length in bytes
+  my $DATA_OFFSET     = $_[5];     # [in/out] current data offset (updated)
+  my $NUM_BLOCKS      = $_[6];     # [in] can only be 1, 2, 3, 4, 5, ..., 15 or 16 (not 0)
+  my $CTR             = $_[7];     # [in/out] current counter value
+  my $HASH_IN_OUT     = $_[8];     # [in/out] XMM ghash in/out value
+  my $ENC_DEC         = $_[9];     # [in] cipher direction (ENC/DEC)
+  my $CTR0            = $_[10];    # [clobbered] ZMM temporary
+  my $CTR1            = $_[11];    # [clobbered] ZMM temporary
+  my $CTR2            = $_[12];    # [clobbered] ZMM temporary
+  my $CTR3            = $_[13];    # [clobbered] ZMM temporary
+  my $DAT0            = $_[14];    # [clobbered] ZMM temporary
+  my $DAT1            = $_[15];    # [clobbered] ZMM temporary
+  my $DAT2            = $_[16];    # [clobbered] ZMM temporary
+  my $DAT3            = $_[17];    # [clobbered] ZMM temporary
+  my $LAST_CIPHER_BLK = $_[18];    # [clobbered] ZMM temporary
+  my $LAST_GHASH_BLK  = $_[19];    # [clobbered] ZMM temporary
+  my $ZT0             = $_[20];    # [clobbered] ZMM temporary
+  my $ZT1             = $_[21];    # [clobbered] ZMM temporary
+  my $ZT2             = $_[22];    # [clobbered] ZMM temporary
+  my $ZT3             = $_[23];    # [clobbered] ZMM temporary
+  my $ZT4             = $_[24];    # [clobbered] ZMM temporary
+  my $IA0             = $_[25];    # [clobbered] GP temporary
+  my $IA1             = $_[26];    # [clobbered] GP temporary
+  my $MASKREG         = $_[27];    # [clobbered] mask register
+  my $SHUFMASK        = $_[28];    # [clobbered] ZMM for BE/LE shuffle mask
+  my $PBLOCK_LEN      = $_[29];    # [in] partial block length
+
+  &INITIAL_BLOCKS_PARTIAL_CIPHER(
+    $AES_KEYS,                $CIPH_PLAIN_OUT,         $PLAIN_CIPH_IN, $LENGTH,
+    $DATA_OFFSET,             $NUM_BLOCKS,             $CTR,           $ENC_DEC,
+    $DAT0,                    $DAT1,                   $DAT2,          $DAT3,
+    &XWORD($LAST_CIPHER_BLK), &XWORD($LAST_GHASH_BLK), $CTR0,          $CTR1,
+    $CTR2,                    $CTR3,                   $ZT0,           $IA0,
+    $IA1,                     $MASKREG,                $SHUFMASK);
+
+  &INITIAL_BLOCKS_PARTIAL_GHASH($AES_KEYS, $GCM128_CTX, $LENGTH, $NUM_BLOCKS, $HASH_IN_OUT, $ENC_DEC, $DAT0,
+    $DAT1, $DAT2, $DAT3, &XWORD($LAST_CIPHER_BLK),
+    &XWORD($LAST_GHASH_BLK), $CTR0, $CTR1, $CTR2, $CTR3, $ZT0, $ZT1, $ZT2, $ZT3, $ZT4, $IA0, $PBLOCK_LEN);
+}
+
+# ;; ===========================================================================
+# ;; Stitched GHASH of 16 blocks (with reduction) with encryption of N blocks
+# ;; followed with GHASH of the N blocks.
+sub GHASH_16_ENCRYPT_N_GHASH_N {
+  my $AES_KEYS           = $_[0];     # [in] key pointer
+  my $GCM128_CTX         = $_[1];     # [in] context pointer
+  my $CIPH_PLAIN_OUT     = $_[2];     # [in] pointer to output buffer
+  my $PLAIN_CIPH_IN      = $_[3];     # [in] pointer to input buffer
+  my $DATA_OFFSET        = $_[4];     # [in] data offset
+  my $LENGTH             = $_[5];     # [in] data length
+  my $CTR_BE             = $_[6];     # [in/out] ZMM counter blocks (last 4) in big-endian
+  my $CTR_CHECK          = $_[7];     # [in/out] GP with 8-bit counter for overflow check
+  my $HASHKEY_OFFSET     = $_[8];     # [in] numerical offset for the highest hash key
+                                      # (can be in form of register or numerical value)
+  my $GHASHIN_BLK_OFFSET = $_[9];     # [in] numerical offset for GHASH blocks in
+  my $SHFMSK             = $_[10];    # [in] ZMM with byte swap mask for pshufb
+  my $B00_03             = $_[11];    # [clobbered] temporary ZMM
+  my $B04_07             = $_[12];    # [clobbered] temporary ZMM
+  my $B08_11             = $_[13];    # [clobbered] temporary ZMM
+  my $B12_15             = $_[14];    # [clobbered] temporary ZMM
+  my $GH1H_UNUSED        = $_[15];    # [clobbered] temporary ZMM
+  my $GH1L               = $_[16];    # [clobbered] temporary ZMM
+  my $GH1M               = $_[17];    # [clobbered] temporary ZMM
+  my $GH1T               = $_[18];    # [clobbered] temporary ZMM
+  my $GH2H               = $_[19];    # [clobbered] temporary ZMM
+  my $GH2L               = $_[20];    # [clobbered] temporary ZMM
+  my $GH2M               = $_[21];    # [clobbered] temporary ZMM
+  my $GH2T               = $_[22];    # [clobbered] temporary ZMM
+  my $GH3H               = $_[23];    # [clobbered] temporary ZMM
+  my $GH3L               = $_[24];    # [clobbered] temporary ZMM
+  my $GH3M               = $_[25];    # [clobbered] temporary ZMM
+  my $GH3T               = $_[26];    # [clobbered] temporary ZMM
+  my $AESKEY1            = $_[27];    # [clobbered] temporary ZMM
+  my $AESKEY2            = $_[28];    # [clobbered] temporary ZMM
+  my $GHKEY1             = $_[29];    # [clobbered] temporary ZMM
+  my $GHKEY2             = $_[30];    # [clobbered] temporary ZMM
+  my $GHDAT1             = $_[31];    # [clobbered] temporary ZMM
+  my $GHDAT2             = $_[32];    # [clobbered] temporary ZMM
+  my $ZT01               = $_[33];    # [clobbered] temporary ZMM
+  my $ADDBE_4x4          = $_[34];    # [in] ZMM with 4x128bits 4 in big-endian
+  my $ADDBE_1234         = $_[35];    # [in] ZMM with 4x128bits 1, 2, 3 and 4 in big-endian
+  my $GHASH_TYPE         = $_[36];    # [in] "start", "start_reduce", "mid", "end_reduce"
+  my $TO_REDUCE_L        = $_[37];    # [in] ZMM for low 4x128-bit GHASH sum
+  my $TO_REDUCE_H        = $_[38];    # [in] ZMM for hi 4x128-bit GHASH sum
+  my $TO_REDUCE_M        = $_[39];    # [in] ZMM for medium 4x128-bit GHASH sum
+  my $ENC_DEC            = $_[40];    # [in] cipher direction
+  my $HASH_IN_OUT        = $_[41];    # [in/out] XMM ghash in/out value
+  my $IA0                = $_[42];    # [clobbered] GP temporary
+  my $IA1                = $_[43];    # [clobbered] GP temporary
+  my $MASKREG            = $_[44];    # [clobbered] mask register
+  my $NUM_BLOCKS         = $_[45];    # [in] numerical value with number of blocks to be encrypted/ghashed (1 to 16)
+  my $PBLOCK_LEN         = $_[46];    # [in] partial block length
+
+  die "GHASH_16_ENCRYPT_N_GHASH_N: num_blocks is out of bounds = $NUM_BLOCKS\n"
+    if ($NUM_BLOCKS > 16 || $NUM_BLOCKS < 0);
+
+  my $rndsuffix = &random_string();
+
+  my $GH1H = $HASH_IN_OUT;
+
+  # ; this is to avoid additional move in do_reduction case
+
+  my $LAST_GHASH_BLK  = $GH1L;
+  my $LAST_CIPHER_BLK = $GH1T;
+
+  my $RED_POLY = $GH2T;
+  my $RED_P1   = $GH2L;
+  my $RED_T1   = $GH2H;
+  my $RED_T2   = $GH2M;
+
+  my $DATA1 = $GH3H;
+  my $DATA2 = $GH3L;
+  my $DATA3 = $GH3M;
+  my $DATA4 = $GH3T;
+
+  # ;; do reduction after the 16 blocks ?
+  my $do_reduction = 0;
+
+  # ;; is 16 block chunk a start?
+  my $is_start = 0;
+
+  if ($GHASH_TYPE eq "start_reduce") {
+    $is_start     = 1;
+    $do_reduction = 1;
+  }
+
+  if ($GHASH_TYPE eq "start") {
+    $is_start = 1;
+  }
+
+  if ($GHASH_TYPE eq "end_reduce") {
+    $do_reduction = 1;
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; - get load/store mask
+  # ;; - load plain/cipher text
+  # ;; get load/store mask
+  $code .= <<___;
+        lea               byte64_len_to_mask_table(%rip),$IA0
+        mov               $LENGTH,$IA1
+___
+  if ($NUM_BLOCKS > 12) {
+    $code .= "sub               \$`3*64`,$IA1\n";
+  } elsif ($NUM_BLOCKS > 8) {
+    $code .= "sub               \$`2*64`,$IA1\n";
+  } elsif ($NUM_BLOCKS > 4) {
+    $code .= "sub               \$`1*64`,$IA1\n";
+  }
+  $code .= "kmovq             ($IA0,$IA1,8),$MASKREG\n";
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; prepare counter blocks
+
+  $code .= <<___;
+        cmp               \$`(256 - $NUM_BLOCKS)`,@{[DWORD($CTR_CHECK)]}
+        jae               .L_16_blocks_overflow_${rndsuffix}
+___
+
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vpaddd", $B00_03, $B04_07,     $B08_11,    $B12_15,    $CTR_BE,
+    $B00_03,     $B04_07,  $B08_11, $ADDBE_1234, $ADDBE_4x4, $ADDBE_4x4, $ADDBE_4x4);
+  $code .= <<___;
+        jmp               .L_16_blocks_ok_${rndsuffix}
+
+.L_16_blocks_overflow_${rndsuffix}:
+        vpshufb           $SHFMSK,$CTR_BE,$CTR_BE
+        vpaddd            ddq_add_1234(%rip),$CTR_BE,$B00_03
+___
+  if ($NUM_BLOCKS > 4) {
+    $code .= <<___;
+        vmovdqa64         ddq_add_4444(%rip),$B12_15
+        vpaddd            $B12_15,$B00_03,$B04_07
+___
+  }
+  if ($NUM_BLOCKS > 8) {
+    $code .= "vpaddd            $B12_15,$B04_07,$B08_11\n";
+  }
+  if ($NUM_BLOCKS > 12) {
+    $code .= "vpaddd            $B12_15,$B08_11,$B12_15\n";
+  }
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vpshufb", $B00_03, $B04_07, $B08_11, $B12_15, $B00_03,
+    $B04_07,     $B08_11,   $B12_15, $SHFMSK, $SHFMSK, $SHFMSK, $SHFMSK);
+  $code .= <<___;
+.L_16_blocks_ok_${rndsuffix}:
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; - pre-load constants
+        # ;; - add current hash into the 1st block
+        vbroadcastf64x2    `(16 * 0)`($AES_KEYS),$AESKEY1
+___
+  if ($is_start != 0) {
+    $code .= "vpxorq            `$GHASHIN_BLK_OFFSET + (0*64)`(%rsp),$HASH_IN_OUT,$GHDAT1\n";
+  } else {
+    $code .= "vmovdqa64         `$GHASHIN_BLK_OFFSET + (0*64)`(%rsp),$GHDAT1\n";
+  }
+
+  $code .= "vmovdqu64         @{[EffectiveAddress(\"%rsp\",$HASHKEY_OFFSET,0*64)]},$GHKEY1\n";
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; save counter for the next round
+  # ;; increment counter overflow check register
+  if ($NUM_BLOCKS <= 4) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 1)`,$B00_03,@{[XWORD($CTR_BE)]}\n";
+  } elsif ($NUM_BLOCKS <= 8) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 5)`,$B04_07,@{[XWORD($CTR_BE)]}\n";
+  } elsif ($NUM_BLOCKS <= 12) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 9)`,$B08_11,@{[XWORD($CTR_BE)]}\n";
+  } else {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 13)`,$B12_15,@{[XWORD($CTR_BE)]}\n";
+  }
+  $code .= "vshufi64x2        \$0b00000000,$CTR_BE,$CTR_BE,$CTR_BE\n";
+
+  $code .= <<___;
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; pre-load constants
+        vbroadcastf64x2    `(16 * 1)`($AES_KEYS),$AESKEY2
+        vmovdqu64         @{[EffectiveAddress("%rsp",$HASHKEY_OFFSET,1*64)]},$GHKEY2
+        vmovdqa64         `$GHASHIN_BLK_OFFSET + (1*64)`(%rsp),$GHDAT2
+___
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; stitch AES rounds with GHASH
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES round 0 - ARK
+
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vpxorq", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+    $B04_07,     $B08_11,  $B12_15, $AESKEY1, $AESKEY1, $AESKEY1, $AESKEY1);
+  $code .= "vbroadcastf64x2    `(16 * 2)`($AES_KEYS),$AESKEY1\n";
+
+  $code .= <<___;
+        # ;;==================================================
+        # ;; GHASH 4 blocks (15 to 12)
+        vpclmulqdq        \$0x11,$GHKEY1,$GHDAT1,$GH1H      # ; a1*b1
+        vpclmulqdq        \$0x00,$GHKEY1,$GHDAT1,$GH1L      # ; a0*b0
+        vpclmulqdq        \$0x01,$GHKEY1,$GHDAT1,$GH1M      # ; a1*b0
+        vpclmulqdq        \$0x10,$GHKEY1,$GHDAT1,$GH1T      # ; a0*b1
+        vmovdqu64         @{[EffectiveAddress("%rsp",$HASHKEY_OFFSET,2*64)]},$GHKEY1
+        vmovdqa64         `$GHASHIN_BLK_OFFSET + (2*64)`(%rsp),$GHDAT1
+___
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES round 1
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vaesenc", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+    $B04_07,     $B08_11,   $B12_15, $AESKEY2, $AESKEY2, $AESKEY2, $AESKEY2);
+  $code .= "vbroadcastf64x2    `(16 * 3)`($AES_KEYS),$AESKEY2\n";
+
+  $code .= <<___;
+        # ;; =================================================
+        # ;; GHASH 4 blocks (11 to 8)
+        vpclmulqdq        \$0x10,$GHKEY2,$GHDAT2,$GH2M      # ; a0*b1
+        vpclmulqdq        \$0x01,$GHKEY2,$GHDAT2,$GH2T      # ; a1*b0
+        vpclmulqdq        \$0x11,$GHKEY2,$GHDAT2,$GH2H      # ; a1*b1
+        vpclmulqdq        \$0x00,$GHKEY2,$GHDAT2,$GH2L      # ; a0*b0
+        vmovdqu64         @{[EffectiveAddress("%rsp",$HASHKEY_OFFSET,3*64)]},$GHKEY2
+        vmovdqa64         `$GHASHIN_BLK_OFFSET + (3*64)`(%rsp),$GHDAT2
+___
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES round 2
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vaesenc", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+    $B04_07,     $B08_11,   $B12_15, $AESKEY1, $AESKEY1, $AESKEY1, $AESKEY1);
+  $code .= "vbroadcastf64x2    `(16 * 4)`($AES_KEYS),$AESKEY1\n";
+
+  $code .= <<___;
+        # ;; =================================================
+        # ;; GHASH 4 blocks (7 to 4)
+        vpclmulqdq        \$0x10,$GHKEY1,$GHDAT1,$GH3M      # ; a0*b1
+        vpclmulqdq        \$0x01,$GHKEY1,$GHDAT1,$GH3T      # ; a1*b0
+        vpclmulqdq        \$0x11,$GHKEY1,$GHDAT1,$GH3H      # ; a1*b1
+        vpclmulqdq        \$0x00,$GHKEY1,$GHDAT1,$GH3L      # ; a0*b0
+___
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES rounds 3
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vaesenc", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+    $B04_07,     $B08_11,   $B12_15, $AESKEY2, $AESKEY2, $AESKEY2, $AESKEY2);
+  $code .= "vbroadcastf64x2    `(16 * 5)`($AES_KEYS),$AESKEY2\n";
+
+  $code .= <<___;
+        # ;; =================================================
+        # ;; Gather (XOR) GHASH for 12 blocks
+        vpternlogq        \$0x96,$GH3H,$GH2H,$GH1H
+        vpternlogq        \$0x96,$GH3L,$GH2L,$GH1L
+        vpternlogq        \$0x96,$GH3T,$GH2T,$GH1T
+        vpternlogq        \$0x96,$GH3M,$GH2M,$GH1M
+___
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES rounds 4
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vaesenc", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+    $B04_07,     $B08_11,   $B12_15, $AESKEY1, $AESKEY1, $AESKEY1, $AESKEY1);
+  $code .= "vbroadcastf64x2    `(16 * 6)`($AES_KEYS),$AESKEY1\n";
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; load plain/cipher text
+  &ZMM_LOAD_MASKED_BLOCKS_0_16($NUM_BLOCKS, $PLAIN_CIPH_IN, $DATA_OFFSET, $DATA1, $DATA2, $DATA3, $DATA4, $MASKREG);
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES rounds 5
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vaesenc", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+    $B04_07,     $B08_11,   $B12_15, $AESKEY2, $AESKEY2, $AESKEY2, $AESKEY2);
+  $code .= "vbroadcastf64x2    `(16 * 7)`($AES_KEYS),$AESKEY2\n";
+
+  $code .= <<___;
+        # ;; =================================================
+        # ;; GHASH 4 blocks (3 to 0)
+        vpclmulqdq        \$0x10,$GHKEY2,$GHDAT2,$GH2M      # ; a0*b1
+        vpclmulqdq        \$0x01,$GHKEY2,$GHDAT2,$GH2T      # ; a1*b0
+        vpclmulqdq        \$0x11,$GHKEY2,$GHDAT2,$GH2H      # ; a1*b1
+        vpclmulqdq        \$0x00,$GHKEY2,$GHDAT2,$GH2L      # ; a0*b0
+___
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES round 6
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vaesenc", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+    $B04_07,     $B08_11,   $B12_15, $AESKEY1, $AESKEY1, $AESKEY1, $AESKEY1);
+  $code .= "vbroadcastf64x2    `(16 * 8)`($AES_KEYS),$AESKEY1\n";
+
+  # ;; =================================================
+  # ;; gather GHASH in GH1L (low), GH1H (high), GH1M (mid)
+  # ;; - add GH2[MTLH] to GH1[MTLH]
+  $code .= "vpternlogq        \$0x96,$GH2T,$GH1T,$GH1M\n";
+  if ($do_reduction != 0) {
+
+    if ($is_start != 0) {
+      $code .= "vpxorq            $GH2M,$GH1M,$GH1M\n";
+    } else {
+      $code .= <<___;
+        vpternlogq        \$0x96,$GH2H,$TO_REDUCE_H,$GH1H
+        vpternlogq        \$0x96,$GH2L,$TO_REDUCE_L,$GH1L
+        vpternlogq        \$0x96,$GH2M,$TO_REDUCE_M,$GH1M
+___
+    }
+
+  } else {
+
+    # ;; Update H/M/L hash sums if not carrying reduction
+    if ($is_start != 0) {
+      $code .= <<___;
+        vpxorq            $GH2H,$GH1H,$TO_REDUCE_H
+        vpxorq            $GH2L,$GH1L,$TO_REDUCE_L
+        vpxorq            $GH2M,$GH1M,$TO_REDUCE_M
+___
+    } else {
+      $code .= <<___;
+        vpternlogq        \$0x96,$GH2H,$GH1H,$TO_REDUCE_H
+        vpternlogq        \$0x96,$GH2L,$GH1L,$TO_REDUCE_L
+        vpternlogq        \$0x96,$GH2M,$GH1M,$TO_REDUCE_M
+___
+    }
+
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES round 7
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vaesenc", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+    $B04_07,     $B08_11,   $B12_15, $AESKEY2, $AESKEY2, $AESKEY2, $AESKEY2);
+  $code .= "vbroadcastf64x2    `(16 * 9)`($AES_KEYS),$AESKEY2\n";
+
+  # ;; =================================================
+  # ;; prepare mid sum for adding to high & low
+  # ;; load polynomial constant for reduction
+  if ($do_reduction != 0) {
+    $code .= <<___;
+        vpsrldq           \$8,$GH1M,$GH2M
+        vpslldq           \$8,$GH1M,$GH1M
+
+        vmovdqa64         POLY2(%rip),@{[XWORD($RED_POLY)]}
+___
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES round 8
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vaesenc", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+    $B04_07,     $B08_11,   $B12_15, $AESKEY1, $AESKEY1, $AESKEY1, $AESKEY1);
+  $code .= "vbroadcastf64x2    `(16 * 10)`($AES_KEYS),$AESKEY1\n";
+
+  # ;; =================================================
+  # ;; Add mid product to high and low
+  if ($do_reduction != 0) {
+    if ($is_start != 0) {
+      $code .= <<___;
+        vpternlogq        \$0x96,$GH2M,$GH2H,$GH1H      # ; TH = TH1 + TH2 + TM>>64
+        vpternlogq        \$0x96,$GH1M,$GH2L,$GH1L      # ; TL = TL1 + TL2 + TM<<64
+___
+    } else {
+      $code .= <<___;
+        vpxorq            $GH2M,$GH1H,$GH1H      # ; TH = TH1 + TM>>64
+        vpxorq            $GH1M,$GH1L,$GH1L      # ; TL = TL1 + TM<<64
+___
+    }
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES round 9
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vaesenc", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+    $B04_07,     $B08_11,   $B12_15, $AESKEY2, $AESKEY2, $AESKEY2, $AESKEY2);
+
+  # ;; =================================================
+  # ;; horizontal xor of low and high 4x128
+  if ($do_reduction != 0) {
+    &VHPXORI4x128($GH1H, $GH2H);
+    &VHPXORI4x128($GH1L, $GH2L);
+  }
+
+  if (($NROUNDS >= 11)) {
+    $code .= "vbroadcastf64x2    `(16 * 11)`($AES_KEYS),$AESKEY2\n";
+  }
+
+  # ;; =================================================
+  # ;; first phase of reduction
+  if ($do_reduction != 0) {
+    $code .= <<___;
+        vpclmulqdq        \$0x01,@{[XWORD($GH1L)]},@{[XWORD($RED_POLY)]},@{[XWORD($RED_P1)]}
+        vpslldq           \$8,@{[XWORD($RED_P1)]},@{[XWORD($RED_P1)]}                    # ; shift-L 2 DWs
+        vpxorq            @{[XWORD($RED_P1)]},@{[XWORD($GH1L)]},@{[XWORD($RED_P1)]}      # ; first phase of the reduct
+___
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES rounds up to 11 (AES192) or 13 (AES256)
+  # ;; AES128 is done
+  if (($NROUNDS >= 11)) {
+    &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+      $NUM_BLOCKS, "vaesenc", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+      $B04_07,     $B08_11,   $B12_15, $AESKEY1, $AESKEY1, $AESKEY1, $AESKEY1);
+    $code .= "vbroadcastf64x2    `(16 * 12)`($AES_KEYS),$AESKEY1\n";
+
+    &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+      $NUM_BLOCKS, "vaesenc", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+      $B04_07,     $B08_11,   $B12_15, $AESKEY2, $AESKEY2, $AESKEY2, $AESKEY2);
+    if (($NROUNDS == 13)) {
+      $code .= "vbroadcastf64x2    `(16 * 13)`($AES_KEYS),$AESKEY2\n";
+
+      &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+        $NUM_BLOCKS, "vaesenc", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+        $B04_07,     $B08_11,   $B12_15, $AESKEY1, $AESKEY1, $AESKEY1, $AESKEY1);
+      $code .= "vbroadcastf64x2    `(16 * 14)`($AES_KEYS),$AESKEY1\n";
+
+      &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+        $NUM_BLOCKS, "vaesenc", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+        $B04_07,     $B08_11,   $B12_15, $AESKEY2, $AESKEY2, $AESKEY2, $AESKEY2);
+    }
+  }
+
+  # ;; =================================================
+  # ;; second phase of the reduction
+  if ($do_reduction != 0) {
+    $code .= <<___;
+        vpclmulqdq        \$0x00,@{[XWORD($RED_P1)]},@{[XWORD($RED_POLY)]},@{[XWORD($RED_T1)]}
+        vpsrldq           \$4,@{[XWORD($RED_T1)]},@{[XWORD($RED_T1)]}      # ; shift-R 1-DW to obtain 2-DWs shift-R
+        vpclmulqdq        \$0x10,@{[XWORD($RED_P1)]},@{[XWORD($RED_POLY)]},@{[XWORD($RED_T2)]}
+        vpslldq           \$4,@{[XWORD($RED_T2)]},@{[XWORD($RED_T2)]}      # ; shift-L 1-DW for result without shifts
+        # ;; GH1H = GH1H + RED_T1 + RED_T2
+        vpternlogq        \$0x96,@{[XWORD($RED_T1)]},@{[XWORD($RED_T2)]},@{[XWORD($GH1H)]}
+___
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; the last AES round
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vaesenclast", $B00_03, $B04_07,  $B08_11,  $B12_15,  $B00_03,
+    $B04_07,     $B08_11,       $B12_15, $AESKEY1, $AESKEY1, $AESKEY1, $AESKEY1);
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; XOR against plain/cipher text
+  &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+    $NUM_BLOCKS, "vpxorq", $B00_03, $B04_07, $B08_11, $B12_15, $B00_03,
+    $B04_07,     $B08_11,  $B12_15, $DATA1,  $DATA2,  $DATA3,  $DATA4);
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; retrieve the last cipher counter block (partially XOR'ed with text)
+  # ;; - this is needed for partial block cases
+  if ($NUM_BLOCKS <= 4) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 1)`,$B00_03,@{[XWORD($LAST_CIPHER_BLK)]}\n";
+  } elsif ($NUM_BLOCKS <= 8) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 5)`,$B04_07,@{[XWORD($LAST_CIPHER_BLK)]}\n";
+  } elsif ($NUM_BLOCKS <= 12) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 9)`,$B08_11,@{[XWORD($LAST_CIPHER_BLK)]}\n";
+  } else {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS - 13)`,$B12_15,@{[XWORD($LAST_CIPHER_BLK)]}\n";
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; store cipher/plain text
+  $code .= "mov       $CIPH_PLAIN_OUT,$IA0\n";
+  &ZMM_STORE_MASKED_BLOCKS_0_16($NUM_BLOCKS, $IA0, $DATA_OFFSET, $B00_03, $B04_07, $B08_11, $B12_15, $MASKREG);
+
+  # ;; =================================================
+  # ;; shuffle cipher text blocks for GHASH computation
+  if ($ENC_DEC eq "ENC") {
+
+    # ;; zero bytes outside the mask before hashing
+    if ($NUM_BLOCKS <= 4) {
+      $code .= "vmovdqu8           $B00_03,${B00_03}{$MASKREG}{z}\n";
+    } elsif ($NUM_BLOCKS <= 8) {
+      $code .= "vmovdqu8          $B04_07,${B04_07}{$MASKREG}{z}\n";
+    } elsif ($NUM_BLOCKS <= 12) {
+      $code .= "vmovdqu8          $B08_11,${B08_11}{$MASKREG}{z}\n";
+    } else {
+      $code .= "vmovdqu8          $B12_15,${B12_15}{$MASKREG}{z}\n";
+    }
+
+    &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+      $NUM_BLOCKS, "vpshufb", $DATA1,  $DATA2,  $DATA3,  $DATA4,  $B00_03,
+      $B04_07,     $B08_11,   $B12_15, $SHFMSK, $SHFMSK, $SHFMSK, $SHFMSK);
+  } else {
+
+    # ;; zero bytes outside the mask before hashing
+    if ($NUM_BLOCKS <= 4) {
+      $code .= "vmovdqu8          $DATA1,${DATA1}{$MASKREG}{z}\n";
+    } elsif ($NUM_BLOCKS <= 8) {
+      $code .= "vmovdqu8          $DATA2,${DATA2}{$MASKREG}{z}\n";
+    } elsif ($NUM_BLOCKS <= 12) {
+      $code .= "vmovdqu8          $DATA3,${DATA3}{$MASKREG}{z}\n";
+    } else {
+      $code .= "vmovdqu8          $DATA4,${DATA4}{$MASKREG}{z}\n";
+    }
+
+    &ZMM_OPCODE3_DSTR_SRC1R_SRC2R_BLOCKS_0_16(
+      $NUM_BLOCKS, "vpshufb", $DATA1, $DATA2,  $DATA3,  $DATA4,  $DATA1,
+      $DATA2,      $DATA3,    $DATA4, $SHFMSK, $SHFMSK, $SHFMSK, $SHFMSK);
+  }
+
+  # ;; =================================================
+  # ;; Extract the last block for partial / multi_call cases
+  if ($NUM_BLOCKS <= 4) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS-1)`,$DATA1,@{[XWORD($LAST_GHASH_BLK)]}\n";
+  } elsif ($NUM_BLOCKS <= 8) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS-5)`,$DATA2,@{[XWORD($LAST_GHASH_BLK)]}\n";
+  } elsif ($NUM_BLOCKS <= 12) {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS-9)`,$DATA3,@{[XWORD($LAST_GHASH_BLK)]}\n";
+  } else {
+    $code .= "vextracti32x4     \$`($NUM_BLOCKS-13)`,$DATA4,@{[XWORD($LAST_GHASH_BLK)]}\n";
+  }
+
+  if ($do_reduction != 0) {
+
+    # ;; GH1H holds reduced hash value
+    # ;; - normally do "vmovdqa64 &XWORD($GH1H), &XWORD($HASH_IN_OUT)"
+    # ;; - register rename trick obsoletes the above move
+  }
+
+  # ;; =================================================
+  # ;; GHASH last N blocks
+  # ;; - current hash value in HASH_IN_OUT or
+  # ;;   product parts in TO_REDUCE_H/M/L
+  # ;; - DATA1-DATA4 include blocks for GHASH
+
+  if ($do_reduction == 0) {
+    &INITIAL_BLOCKS_PARTIAL_GHASH(
+      $AES_KEYS,            $GCM128_CTX, $LENGTH,                  $NUM_BLOCKS,
+      &XWORD($HASH_IN_OUT), $ENC_DEC,    $DATA1,                   $DATA2,
+      $DATA3,               $DATA4,      &XWORD($LAST_CIPHER_BLK), &XWORD($LAST_GHASH_BLK),
+      $B00_03,              $B04_07,     $B08_11,                  $B12_15,
+      $GHDAT1,              $GHDAT2,     $AESKEY1,                 $AESKEY2,
+      $GHKEY1,              $IA0,        $PBLOCK_LEN,              $TO_REDUCE_H,
+      $TO_REDUCE_M,         $TO_REDUCE_L);
+  } else {
+    &INITIAL_BLOCKS_PARTIAL_GHASH(
+      $AES_KEYS,            $GCM128_CTX, $LENGTH,                  $NUM_BLOCKS,
+      &XWORD($HASH_IN_OUT), $ENC_DEC,    $DATA1,                   $DATA2,
+      $DATA3,               $DATA4,      &XWORD($LAST_CIPHER_BLK), &XWORD($LAST_GHASH_BLK),
+      $B00_03,              $B04_07,     $B08_11,                  $B12_15,
+      $GHDAT1,              $GHDAT2,     $AESKEY1,                 $AESKEY2,
+      $GHKEY1,              $IA0,        $PBLOCK_LEN);
+  }
+}
+
+# ;; ===========================================================================
+# ;; ===========================================================================
+# ;; Stitched GHASH of 16 blocks (with reduction) with encryption of N blocks
+# ;; followed with GHASH of the N blocks.
+sub GCM_ENC_DEC_LAST {
+  my $AES_KEYS           = $_[0];     # [in] key pointer
+  my $GCM128_CTX         = $_[1];     # [in] context pointer
+  my $CIPH_PLAIN_OUT     = $_[2];     # [in] pointer to output buffer
+  my $PLAIN_CIPH_IN      = $_[3];     # [in] pointer to input buffer
+  my $DATA_OFFSET        = $_[4];     # [in] data offset
+  my $LENGTH             = $_[5];     # [in/clobbered] data length
+  my $CTR_BE             = $_[6];     # [in/out] ZMM counter blocks (last 4) in big-endian
+  my $CTR_CHECK          = $_[7];     # [in/out] GP with 8-bit counter for overflow check
+  my $HASHKEY_OFFSET     = $_[8];     # [in] numerical offset for the highest hash key
+                                      # (can be register or numerical offset)
+  my $GHASHIN_BLK_OFFSET = $_[9];     # [in] numerical offset for GHASH blocks in
+  my $SHFMSK             = $_[10];    # [in] ZMM with byte swap mask for pshufb
+  my $ZT00               = $_[11];    # [clobbered] temporary ZMM
+  my $ZT01               = $_[12];    # [clobbered] temporary ZMM
+  my $ZT02               = $_[13];    # [clobbered] temporary ZMM
+  my $ZT03               = $_[14];    # [clobbered] temporary ZMM
+  my $ZT04               = $_[15];    # [clobbered] temporary ZMM
+  my $ZT05               = $_[16];    # [clobbered] temporary ZMM
+  my $ZT06               = $_[17];    # [clobbered] temporary ZMM
+  my $ZT07               = $_[18];    # [clobbered] temporary ZMM
+  my $ZT08               = $_[19];    # [clobbered] temporary ZMM
+  my $ZT09               = $_[20];    # [clobbered] temporary ZMM
+  my $ZT10               = $_[21];    # [clobbered] temporary ZMM
+  my $ZT11               = $_[22];    # [clobbered] temporary ZMM
+  my $ZT12               = $_[23];    # [clobbered] temporary ZMM
+  my $ZT13               = $_[24];    # [clobbered] temporary ZMM
+  my $ZT14               = $_[25];    # [clobbered] temporary ZMM
+  my $ZT15               = $_[26];    # [clobbered] temporary ZMM
+  my $ZT16               = $_[27];    # [clobbered] temporary ZMM
+  my $ZT17               = $_[28];    # [clobbered] temporary ZMM
+  my $ZT18               = $_[29];    # [clobbered] temporary ZMM
+  my $ZT19               = $_[30];    # [clobbered] temporary ZMM
+  my $ZT20               = $_[31];    # [clobbered] temporary ZMM
+  my $ZT21               = $_[32];    # [clobbered] temporary ZMM
+  my $ZT22               = $_[33];    # [clobbered] temporary ZMM
+  my $ADDBE_4x4          = $_[34];    # [in] ZMM with 4x128bits 4 in big-endian
+  my $ADDBE_1234         = $_[35];    # [in] ZMM with 4x128bits 1, 2, 3 and 4 in big-endian
+  my $GHASH_TYPE         = $_[36];    # [in] "start", "start_reduce", "mid", "end_reduce"
+  my $TO_REDUCE_L        = $_[37];    # [in] ZMM for low 4x128-bit GHASH sum
+  my $TO_REDUCE_H        = $_[38];    # [in] ZMM for hi 4x128-bit GHASH sum
+  my $TO_REDUCE_M        = $_[39];    # [in] ZMM for medium 4x128-bit GHASH sum
+  my $ENC_DEC            = $_[40];    # [in] cipher direction
+  my $HASH_IN_OUT        = $_[41];    # [in/out] XMM ghash in/out value
+  my $IA0                = $_[42];    # [clobbered] GP temporary
+  my $IA1                = $_[43];    # [clobbered] GP temporary
+  my $MASKREG            = $_[44];    # [clobbered] mask register
+  my $PBLOCK_LEN         = $_[45];    # [in] partial block length
+
+  my $rndsuffix = &random_string();
+
+  $code .= <<___;
+        mov               @{[DWORD($LENGTH)]},@{[DWORD($IA0)]}
+        add               \$15,@{[DWORD($IA0)]}
+        shr               \$4,@{[DWORD($IA0)]}
+        je                .L_last_num_blocks_is_0_${rndsuffix}
+
+        cmp               \$8,@{[DWORD($IA0)]}
+        je                .L_last_num_blocks_is_8_${rndsuffix}
+        jb                .L_last_num_blocks_is_7_1_${rndsuffix}
+
+
+        cmp               \$12,@{[DWORD($IA0)]}
+        je                .L_last_num_blocks_is_12_${rndsuffix}
+        jb                .L_last_num_blocks_is_11_9_${rndsuffix}
+
+        # ;; 16, 15, 14 or 13
+        cmp               \$15,@{[DWORD($IA0)]}
+        je                .L_last_num_blocks_is_15_${rndsuffix}
+        ja                .L_last_num_blocks_is_16_${rndsuffix}
+        cmp               \$14,@{[DWORD($IA0)]}
+        je                .L_last_num_blocks_is_14_${rndsuffix}
+        jmp               .L_last_num_blocks_is_13_${rndsuffix}
+
+.L_last_num_blocks_is_11_9_${rndsuffix}:
+        # ;; 11, 10 or 9
+        cmp               \$10,@{[DWORD($IA0)]}
+        je                .L_last_num_blocks_is_10_${rndsuffix}
+        ja                .L_last_num_blocks_is_11_${rndsuffix}
+        jmp               .L_last_num_blocks_is_9_${rndsuffix}
+
+.L_last_num_blocks_is_7_1_${rndsuffix}:
+        cmp               \$4,@{[DWORD($IA0)]}
+        je                .L_last_num_blocks_is_4_${rndsuffix}
+        jb                .L_last_num_blocks_is_3_1_${rndsuffix}
+        # ;; 7, 6 or 5
+        cmp               \$6,@{[DWORD($IA0)]}
+        ja                .L_last_num_blocks_is_7_${rndsuffix}
+        je                .L_last_num_blocks_is_6_${rndsuffix}
+        jmp               .L_last_num_blocks_is_5_${rndsuffix}
+
+.L_last_num_blocks_is_3_1_${rndsuffix}:
+        # ;; 3, 2 or 1
+        cmp               \$2,@{[DWORD($IA0)]}
+        ja                .L_last_num_blocks_is_3_${rndsuffix}
+        je                .L_last_num_blocks_is_2_${rndsuffix}
+___
+
+  # ;; fall through for `jmp .L_last_num_blocks_is_1`
+
+  # ;; Use rep to generate different block size variants
+  # ;; - one block size has to be the first one
+  for my $num_blocks (1 .. 16) {
+    $code .= ".L_last_num_blocks_is_${num_blocks}_${rndsuffix}:\n";
+    &GHASH_16_ENCRYPT_N_GHASH_N(
+      $AES_KEYS,   $GCM128_CTX,  $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN,  $DATA_OFFSET,
+      $LENGTH,     $CTR_BE,      $CTR_CHECK,      $HASHKEY_OFFSET, $GHASHIN_BLK_OFFSET,
+      $SHFMSK,     $ZT00,        $ZT01,           $ZT02,           $ZT03,
+      $ZT04,       $ZT05,        $ZT06,           $ZT07,           $ZT08,
+      $ZT09,       $ZT10,        $ZT11,           $ZT12,           $ZT13,
+      $ZT14,       $ZT15,        $ZT16,           $ZT17,           $ZT18,
+      $ZT19,       $ZT20,        $ZT21,           $ZT22,           $ADDBE_4x4,
+      $ADDBE_1234, $GHASH_TYPE,  $TO_REDUCE_L,    $TO_REDUCE_H,    $TO_REDUCE_M,
+      $ENC_DEC,    $HASH_IN_OUT, $IA0,            $IA1,            $MASKREG,
+      $num_blocks, $PBLOCK_LEN);
+
+    $code .= "jmp           .L_last_blocks_done_${rndsuffix}\n";
+  }
+
+  $code .= ".L_last_num_blocks_is_0_${rndsuffix}:\n";
+
+  # ;; if there is 0 blocks to cipher then there are only 16 blocks for ghash and reduction
+  # ;; - convert mid into end_reduce
+  # ;; - convert start into start_reduce
+  if ($GHASH_TYPE eq "mid") {
+    $GHASH_TYPE = "end_reduce";
+  }
+  if ($GHASH_TYPE eq "start") {
+    $GHASH_TYPE = "start_reduce";
+  }
+
+  &GHASH_16($GHASH_TYPE, $TO_REDUCE_H, $TO_REDUCE_M, $TO_REDUCE_L, "%rsp",
+    $GHASHIN_BLK_OFFSET, 0, "%rsp", $HASHKEY_OFFSET, 0, $HASH_IN_OUT, $ZT00, $ZT01,
+    $ZT02, $ZT03, $ZT04, $ZT05, $ZT06, $ZT07, $ZT08, $ZT09);
+
+  $code .= ".L_last_blocks_done_${rndsuffix}:\n";
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;; Main GCM macro stitching cipher with GHASH
+# ;; - operates on single stream
+# ;; - encrypts 16 blocks at a time
+# ;; - ghash the 16 previously encrypted ciphertext blocks
+# ;; - no partial block or multi_call handling here
+sub GHASH_16_ENCRYPT_16_PARALLEL {
+  my $AES_KEYS           = $_[0];     # [in] key pointer
+  my $CIPH_PLAIN_OUT     = $_[1];     # [in] pointer to output buffer
+  my $PLAIN_CIPH_IN      = $_[2];     # [in] pointer to input buffer
+  my $DATA_OFFSET        = $_[3];     # [in] data offset
+  my $CTR_BE             = $_[4];     # [in/out] ZMM counter blocks (last 4) in big-endian
+  my $CTR_CHECK          = $_[5];     # [in/out] GP with 8-bit counter for overflow check
+  my $HASHKEY_OFFSET     = $_[6];     # [in] numerical offset for the highest hash key (hash key index value)
+  my $AESOUT_BLK_OFFSET  = $_[7];     # [in] numerical offset for AES-CTR out
+  my $GHASHIN_BLK_OFFSET = $_[8];     # [in] numerical offset for GHASH blocks in
+  my $SHFMSK             = $_[9];     # [in] ZMM with byte swap mask for pshufb
+  my $ZT1                = $_[10];    # [clobbered] temporary ZMM (cipher)
+  my $ZT2                = $_[11];    # [clobbered] temporary ZMM (cipher)
+  my $ZT3                = $_[12];    # [clobbered] temporary ZMM (cipher)
+  my $ZT4                = $_[13];    # [clobbered] temporary ZMM (cipher)
+  my $ZT5                = $_[14];    # [clobbered/out] temporary ZMM or GHASH OUT (final_reduction)
+  my $ZT6                = $_[15];    # [clobbered] temporary ZMM (cipher)
+  my $ZT7                = $_[16];    # [clobbered] temporary ZMM (cipher)
+  my $ZT8                = $_[17];    # [clobbered] temporary ZMM (cipher)
+  my $ZT9                = $_[18];    # [clobbered] temporary ZMM (cipher)
+  my $ZT10               = $_[19];    # [clobbered] temporary ZMM (ghash)
+  my $ZT11               = $_[20];    # [clobbered] temporary ZMM (ghash)
+  my $ZT12               = $_[21];    # [clobbered] temporary ZMM (ghash)
+  my $ZT13               = $_[22];    # [clobbered] temporary ZMM (ghash)
+  my $ZT14               = $_[23];    # [clobbered] temporary ZMM (ghash)
+  my $ZT15               = $_[24];    # [clobbered] temporary ZMM (ghash)
+  my $ZT16               = $_[25];    # [clobbered] temporary ZMM (ghash)
+  my $ZT17               = $_[26];    # [clobbered] temporary ZMM (ghash)
+  my $ZT18               = $_[27];    # [clobbered] temporary ZMM (ghash)
+  my $ZT19               = $_[28];    # [clobbered] temporary ZMM
+  my $ZT20               = $_[29];    # [clobbered] temporary ZMM
+  my $ZT21               = $_[30];    # [clobbered] temporary ZMM
+  my $ZT22               = $_[31];    # [clobbered] temporary ZMM
+  my $ZT23               = $_[32];    # [clobbered] temporary ZMM
+  my $ADDBE_4x4          = $_[33];    # [in] ZMM with 4x128bits 4 in big-endian
+  my $ADDBE_1234         = $_[34];    # [in] ZMM with 4x128bits 1, 2, 3 and 4 in big-endian
+  my $TO_REDUCE_L        = $_[35];    # [in/out] ZMM for low 4x128-bit GHASH sum
+  my $TO_REDUCE_H        = $_[36];    # [in/out] ZMM for hi 4x128-bit GHASH sum
+  my $TO_REDUCE_M        = $_[37];    # [in/out] ZMM for medium 4x128-bit GHASH sum
+  my $DO_REDUCTION       = $_[38];    # [in] "no_reduction", "final_reduction", "first_time"
+  my $ENC_DEC            = $_[39];    # [in] cipher direction
+  my $DATA_DISPL         = $_[40];    # [in] fixed numerical data displacement/offset
+  my $GHASH_IN           = $_[41];    # [in] current GHASH value or "no_ghash_in"
+  my $IA0                = $_[42];    # [clobbered] temporary GPR
+
+  my $B00_03 = $ZT1;
+  my $B04_07 = $ZT2;
+  my $B08_11 = $ZT3;
+  my $B12_15 = $ZT4;
+
+  my $GH1H = $ZT5;
+
+  # ; @note: do not change this mapping
+  my $GH1L = $ZT6;
+  my $GH1M = $ZT7;
+  my $GH1T = $ZT8;
+
+  my $GH2H = $ZT9;
+  my $GH2L = $ZT10;
+  my $GH2M = $ZT11;
+  my $GH2T = $ZT12;
+
+  my $RED_POLY = $GH2T;
+  my $RED_P1   = $GH2L;
+  my $RED_T1   = $GH2H;
+  my $RED_T2   = $GH2M;
+
+  my $GH3H = $ZT13;
+  my $GH3L = $ZT14;
+  my $GH3M = $ZT15;
+  my $GH3T = $ZT16;
+
+  my $DATA1 = $ZT13;
+  my $DATA2 = $ZT14;
+  my $DATA3 = $ZT15;
+  my $DATA4 = $ZT16;
+
+  my $AESKEY1 = $ZT17;
+  my $AESKEY2 = $ZT18;
+
+  my $GHKEY1 = $ZT19;
+  my $GHKEY2 = $ZT20;
+  my $GHDAT1 = $ZT21;
+  my $GHDAT2 = $ZT22;
+
+  my $rndsuffix = &random_string();
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; prepare counter blocks
+
+  $code .= <<___;
+        cmpb              \$`(256 - 16)`,@{[BYTE($CTR_CHECK)]}
+        jae               .L_16_blocks_overflow_${rndsuffix}
+        vpaddd            $ADDBE_1234,$CTR_BE,$B00_03
+        vpaddd            $ADDBE_4x4,$B00_03,$B04_07
+        vpaddd            $ADDBE_4x4,$B04_07,$B08_11
+        vpaddd            $ADDBE_4x4,$B08_11,$B12_15
+        jmp               .L_16_blocks_ok_${rndsuffix}
+.L_16_blocks_overflow_${rndsuffix}:
+        vpshufb           $SHFMSK,$CTR_BE,$CTR_BE
+        vmovdqa64         ddq_add_4444(%rip),$B12_15
+        vpaddd            ddq_add_1234(%rip),$CTR_BE,$B00_03
+        vpaddd            $B12_15,$B00_03,$B04_07
+        vpaddd            $B12_15,$B04_07,$B08_11
+        vpaddd            $B12_15,$B08_11,$B12_15
+        vpshufb           $SHFMSK,$B00_03,$B00_03
+        vpshufb           $SHFMSK,$B04_07,$B04_07
+        vpshufb           $SHFMSK,$B08_11,$B08_11
+        vpshufb           $SHFMSK,$B12_15,$B12_15
+.L_16_blocks_ok_${rndsuffix}:
+___
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; pre-load constants
+  $code .= "vbroadcastf64x2    `(16 * 0)`($AES_KEYS),$AESKEY1\n";
+  if ($GHASH_IN ne "no_ghash_in") {
+    $code .= "vpxorq            `$GHASHIN_BLK_OFFSET + (0*64)`(%rsp),$GHASH_IN,$GHDAT1\n";
+  } else {
+    $code .= "vmovdqa64         `$GHASHIN_BLK_OFFSET + (0*64)`(%rsp),$GHDAT1\n";
+  }
+
+  $code .= <<___;
+        vmovdqu64         @{[HashKeyByIdx(($HASHKEY_OFFSET - (0*4)),"%rsp")]},$GHKEY1
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; save counter for the next round
+        # ;; increment counter overflow check register
+        vshufi64x2        \$0b11111111,$B12_15,$B12_15,$CTR_BE
+        addb              \$16,@{[BYTE($CTR_CHECK)]}
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; pre-load constants
+        vbroadcastf64x2    `(16 * 1)`($AES_KEYS),$AESKEY2
+        vmovdqu64         @{[HashKeyByIdx(($HASHKEY_OFFSET - (1*4)),"%rsp")]},$GHKEY2
+        vmovdqa64         `$GHASHIN_BLK_OFFSET + (1*64)`(%rsp),$GHDAT2
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; stitch AES rounds with GHASH
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; AES round 0 - ARK
+
+        vpxorq            $AESKEY1,$B00_03,$B00_03
+        vpxorq            $AESKEY1,$B04_07,$B04_07
+        vpxorq            $AESKEY1,$B08_11,$B08_11
+        vpxorq            $AESKEY1,$B12_15,$B12_15
+        vbroadcastf64x2    `(16 * 2)`($AES_KEYS),$AESKEY1
+
+        # ;;==================================================
+        # ;; GHASH 4 blocks (15 to 12)
+        vpclmulqdq        \$0x11,$GHKEY1,$GHDAT1,$GH1H      # ; a1*b1
+        vpclmulqdq        \$0x00,$GHKEY1,$GHDAT1,$GH1L      # ; a0*b0
+        vpclmulqdq        \$0x01,$GHKEY1,$GHDAT1,$GH1M      # ; a1*b0
+        vpclmulqdq        \$0x10,$GHKEY1,$GHDAT1,$GH1T      # ; a0*b1
+        vmovdqu64         @{[HashKeyByIdx(($HASHKEY_OFFSET - (2*4)),"%rsp")]},$GHKEY1
+        vmovdqa64         `$GHASHIN_BLK_OFFSET + (2*64)`(%rsp),$GHDAT1
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; AES round 1
+        vaesenc           $AESKEY2,$B00_03,$B00_03
+        vaesenc           $AESKEY2,$B04_07,$B04_07
+        vaesenc           $AESKEY2,$B08_11,$B08_11
+        vaesenc           $AESKEY2,$B12_15,$B12_15
+        vbroadcastf64x2    `(16 * 3)`($AES_KEYS),$AESKEY2
+
+        # ;; =================================================
+        # ;; GHASH 4 blocks (11 to 8)
+        vpclmulqdq        \$0x10,$GHKEY2,$GHDAT2,$GH2M      # ; a0*b1
+        vpclmulqdq        \$0x01,$GHKEY2,$GHDAT2,$GH2T      # ; a1*b0
+        vpclmulqdq        \$0x11,$GHKEY2,$GHDAT2,$GH2H      # ; a1*b1
+        vpclmulqdq        \$0x00,$GHKEY2,$GHDAT2,$GH2L      # ; a0*b0
+        vmovdqu64         @{[HashKeyByIdx(($HASHKEY_OFFSET - (3*4)),"%rsp")]},$GHKEY2
+        vmovdqa64         `$GHASHIN_BLK_OFFSET + (3*64)`(%rsp),$GHDAT2
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; AES round 2
+        vaesenc           $AESKEY1,$B00_03,$B00_03
+        vaesenc           $AESKEY1,$B04_07,$B04_07
+        vaesenc           $AESKEY1,$B08_11,$B08_11
+        vaesenc           $AESKEY1,$B12_15,$B12_15
+        vbroadcastf64x2    `(16 * 4)`($AES_KEYS),$AESKEY1
+
+        # ;; =================================================
+        # ;; GHASH 4 blocks (7 to 4)
+        vpclmulqdq        \$0x10,$GHKEY1,$GHDAT1,$GH3M      # ; a0*b1
+        vpclmulqdq        \$0x01,$GHKEY1,$GHDAT1,$GH3T      # ; a1*b0
+        vpclmulqdq        \$0x11,$GHKEY1,$GHDAT1,$GH3H      # ; a1*b1
+        vpclmulqdq        \$0x00,$GHKEY1,$GHDAT1,$GH3L      # ; a0*b0
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; AES rounds 3
+        vaesenc           $AESKEY2,$B00_03,$B00_03
+        vaesenc           $AESKEY2,$B04_07,$B04_07
+        vaesenc           $AESKEY2,$B08_11,$B08_11
+        vaesenc           $AESKEY2,$B12_15,$B12_15
+        vbroadcastf64x2    `(16 * 5)`($AES_KEYS),$AESKEY2
+
+        # ;; =================================================
+        # ;; Gather (XOR) GHASH for 12 blocks
+        vpternlogq        \$0x96,$GH3H,$GH2H,$GH1H
+        vpternlogq        \$0x96,$GH3L,$GH2L,$GH1L
+        vpternlogq        \$0x96,$GH3T,$GH2T,$GH1T
+        vpternlogq        \$0x96,$GH3M,$GH2M,$GH1M
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; AES rounds 4
+        vaesenc           $AESKEY1,$B00_03,$B00_03
+        vaesenc           $AESKEY1,$B04_07,$B04_07
+        vaesenc           $AESKEY1,$B08_11,$B08_11
+        vaesenc           $AESKEY1,$B12_15,$B12_15
+        vbroadcastf64x2    `(16 * 6)`($AES_KEYS),$AESKEY1
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; load plain/cipher text (recycle GH3xx registers)
+        vmovdqu8          `$DATA_DISPL + (0 * 64)`($PLAIN_CIPH_IN,$DATA_OFFSET),$DATA1
+        vmovdqu8          `$DATA_DISPL + (1 * 64)`($PLAIN_CIPH_IN,$DATA_OFFSET),$DATA2
+        vmovdqu8          `$DATA_DISPL + (2 * 64)`($PLAIN_CIPH_IN,$DATA_OFFSET),$DATA3
+        vmovdqu8          `$DATA_DISPL + (3 * 64)`($PLAIN_CIPH_IN,$DATA_OFFSET),$DATA4
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; AES rounds 5
+        vaesenc           $AESKEY2,$B00_03,$B00_03
+        vaesenc           $AESKEY2,$B04_07,$B04_07
+        vaesenc           $AESKEY2,$B08_11,$B08_11
+        vaesenc           $AESKEY2,$B12_15,$B12_15
+        vbroadcastf64x2    `(16 * 7)`($AES_KEYS),$AESKEY2
+
+        # ;; =================================================
+        # ;; GHASH 4 blocks (3 to 0)
+        vpclmulqdq        \$0x10,$GHKEY2,$GHDAT2,$GH2M      # ; a0*b1
+        vpclmulqdq        \$0x01,$GHKEY2,$GHDAT2,$GH2T      # ; a1*b0
+        vpclmulqdq        \$0x11,$GHKEY2,$GHDAT2,$GH2H      # ; a1*b1
+        vpclmulqdq        \$0x00,$GHKEY2,$GHDAT2,$GH2L      # ; a0*b0
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; AES round 6
+        vaesenc           $AESKEY1,$B00_03,$B00_03
+        vaesenc           $AESKEY1,$B04_07,$B04_07
+        vaesenc           $AESKEY1,$B08_11,$B08_11
+        vaesenc           $AESKEY1,$B12_15,$B12_15
+        vbroadcastf64x2    `(16 * 8)`($AES_KEYS),$AESKEY1
+___
+
+  # ;; =================================================
+  # ;; gather GHASH in GH1L (low) and GH1H (high)
+  if ($DO_REDUCTION eq "first_time") {
+    $code .= <<___;
+        vpternlogq        \$0x96,$GH2T,$GH1T,$GH1M      # ; TM
+        vpxorq            $GH2M,$GH1M,$TO_REDUCE_M      # ; TM
+        vpxorq            $GH2H,$GH1H,$TO_REDUCE_H      # ; TH
+        vpxorq            $GH2L,$GH1L,$TO_REDUCE_L      # ; TL
+___
+  }
+  if ($DO_REDUCTION eq "no_reduction") {
+    $code .= <<___;
+        vpternlogq        \$0x96,$GH2T,$GH1T,$GH1M             # ; TM
+        vpternlogq        \$0x96,$GH2M,$GH1M,$TO_REDUCE_M      # ; TM
+        vpternlogq        \$0x96,$GH2H,$GH1H,$TO_REDUCE_H      # ; TH
+        vpternlogq        \$0x96,$GH2L,$GH1L,$TO_REDUCE_L      # ; TL
+___
+  }
+  if ($DO_REDUCTION eq "final_reduction") {
+    $code .= <<___;
+        # ;; phase 1: add mid products together
+        # ;; also load polynomial constant for reduction
+        vpternlogq        \$0x96,$GH2T,$GH1T,$GH1M      # ; TM
+        vpternlogq        \$0x96,$GH2M,$TO_REDUCE_M,$GH1M
+
+        vpsrldq           \$8,$GH1M,$GH2M
+        vpslldq           \$8,$GH1M,$GH1M
+
+        vmovdqa64         POLY2(%rip),@{[XWORD($RED_POLY)]}
+___
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES round 7
+  $code .= <<___;
+        vaesenc           $AESKEY2,$B00_03,$B00_03
+        vaesenc           $AESKEY2,$B04_07,$B04_07
+        vaesenc           $AESKEY2,$B08_11,$B08_11
+        vaesenc           $AESKEY2,$B12_15,$B12_15
+        vbroadcastf64x2    `(16 * 9)`($AES_KEYS),$AESKEY2
+___
+
+  # ;; =================================================
+  # ;; Add mid product to high and low
+  if ($DO_REDUCTION eq "final_reduction") {
+    $code .= <<___;
+        vpternlogq        \$0x96,$GH2M,$GH2H,$GH1H      # ; TH = TH1 + TH2 + TM>>64
+        vpxorq            $TO_REDUCE_H,$GH1H,$GH1H
+        vpternlogq        \$0x96,$GH1M,$GH2L,$GH1L      # ; TL = TL1 + TL2 + TM<<64
+        vpxorq            $TO_REDUCE_L,$GH1L,$GH1L
+___
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES round 8
+  $code .= <<___;
+        vaesenc           $AESKEY1,$B00_03,$B00_03
+        vaesenc           $AESKEY1,$B04_07,$B04_07
+        vaesenc           $AESKEY1,$B08_11,$B08_11
+        vaesenc           $AESKEY1,$B12_15,$B12_15
+        vbroadcastf64x2    `(16 * 10)`($AES_KEYS),$AESKEY1
+___
+
+  # ;; =================================================
+  # ;; horizontal xor of low and high 4x128
+  if ($DO_REDUCTION eq "final_reduction") {
+    &VHPXORI4x128($GH1H, $GH2H);
+    &VHPXORI4x128($GH1L, $GH2L);
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES round 9
+  $code .= <<___;
+        vaesenc           $AESKEY2,$B00_03,$B00_03
+        vaesenc           $AESKEY2,$B04_07,$B04_07
+        vaesenc           $AESKEY2,$B08_11,$B08_11
+        vaesenc           $AESKEY2,$B12_15,$B12_15
+___
+  if (($NROUNDS >= 11)) {
+    $code .= "vbroadcastf64x2    `(16 * 11)`($AES_KEYS),$AESKEY2\n";
+  }
+
+  # ;; =================================================
+  # ;; first phase of reduction
+  if ($DO_REDUCTION eq "final_reduction") {
+    $code .= <<___;
+        vpclmulqdq        \$0x01,@{[XWORD($GH1L)]},@{[XWORD($RED_POLY)]},@{[XWORD($RED_P1)]}
+        vpslldq           \$8,@{[XWORD($RED_P1)]},@{[XWORD($RED_P1)]}                    # ; shift-L 2 DWs
+        vpxorq            @{[XWORD($RED_P1)]},@{[XWORD($GH1L)]},@{[XWORD($RED_P1)]}      # ; first phase of the reduct
+___
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; AES rounds up to 11 (AES192) or 13 (AES256)
+  # ;; AES128 is done
+  if (($NROUNDS >= 11)) {
+    $code .= <<___;
+        vaesenc           $AESKEY1,$B00_03,$B00_03
+        vaesenc           $AESKEY1,$B04_07,$B04_07
+        vaesenc           $AESKEY1,$B08_11,$B08_11
+        vaesenc           $AESKEY1,$B12_15,$B12_15
+        vbroadcastf64x2    `(16 * 12)`($AES_KEYS),$AESKEY1
+
+        vaesenc           $AESKEY2,$B00_03,$B00_03
+        vaesenc           $AESKEY2,$B04_07,$B04_07
+        vaesenc           $AESKEY2,$B08_11,$B08_11
+        vaesenc           $AESKEY2,$B12_15,$B12_15
+___
+    if (($NROUNDS == 13)) {
+      $code .= <<___;
+        vbroadcastf64x2    `(16 * 13)`($AES_KEYS),$AESKEY2
+
+        vaesenc           $AESKEY1,$B00_03,$B00_03
+        vaesenc           $AESKEY1,$B04_07,$B04_07
+        vaesenc           $AESKEY1,$B08_11,$B08_11
+        vaesenc           $AESKEY1,$B12_15,$B12_15
+        vbroadcastf64x2    `(16 * 14)`($AES_KEYS),$AESKEY1
+
+        vaesenc           $AESKEY2,$B00_03,$B00_03
+        vaesenc           $AESKEY2,$B04_07,$B04_07
+        vaesenc           $AESKEY2,$B08_11,$B08_11
+        vaesenc           $AESKEY2,$B12_15,$B12_15
+___
+    }
+  }
+
+  # ;; =================================================
+  # ;; second phase of the reduction
+  if ($DO_REDUCTION eq "final_reduction") {
+    $code .= <<___;
+        vpclmulqdq        \$0x00,@{[XWORD($RED_P1)]},@{[XWORD($RED_POLY)]},@{[XWORD($RED_T1)]}
+        vpsrldq           \$4,@{[XWORD($RED_T1)]},@{[XWORD($RED_T1)]}      # ; shift-R 1-DW to obtain 2-DWs shift-R
+        vpclmulqdq        \$0x10,@{[XWORD($RED_P1)]},@{[XWORD($RED_POLY)]},@{[XWORD($RED_T2)]}
+        vpslldq           \$4,@{[XWORD($RED_T2)]},@{[XWORD($RED_T2)]}      # ; shift-L 1-DW for result without shifts
+        # ;; GH1H = GH1H x RED_T1 x RED_T2
+        vpternlogq        \$0x96,@{[XWORD($RED_T1)]},@{[XWORD($RED_T2)]},@{[XWORD($GH1H)]}
+___
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;; the last AES round
+  $code .= <<___;
+        vaesenclast       $AESKEY1,$B00_03,$B00_03
+        vaesenclast       $AESKEY1,$B04_07,$B04_07
+        vaesenclast       $AESKEY1,$B08_11,$B08_11
+        vaesenclast       $AESKEY1,$B12_15,$B12_15
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; XOR against plain/cipher text
+        vpxorq            $DATA1,$B00_03,$B00_03
+        vpxorq            $DATA2,$B04_07,$B04_07
+        vpxorq            $DATA3,$B08_11,$B08_11
+        vpxorq            $DATA4,$B12_15,$B12_15
+
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; store cipher/plain text
+        mov               $CIPH_PLAIN_OUT,$IA0
+        vmovdqu8          $B00_03,`$DATA_DISPL + (0 * 64)`($IA0,$DATA_OFFSET,1)
+        vmovdqu8          $B04_07,`$DATA_DISPL + (1 * 64)`($IA0,$DATA_OFFSET,1)
+        vmovdqu8          $B08_11,`$DATA_DISPL + (2 * 64)`($IA0,$DATA_OFFSET,1)
+        vmovdqu8          $B12_15,`$DATA_DISPL + (3 * 64)`($IA0,$DATA_OFFSET,1)
+___
+
+  # ;; =================================================
+  # ;; shuffle cipher text blocks for GHASH computation
+  if ($ENC_DEC eq "ENC") {
+    $code .= <<___;
+        vpshufb           $SHFMSK,$B00_03,$B00_03
+        vpshufb           $SHFMSK,$B04_07,$B04_07
+        vpshufb           $SHFMSK,$B08_11,$B08_11
+        vpshufb           $SHFMSK,$B12_15,$B12_15
+___
+  } else {
+    $code .= <<___;
+        vpshufb           $SHFMSK,$DATA1,$B00_03
+        vpshufb           $SHFMSK,$DATA2,$B04_07
+        vpshufb           $SHFMSK,$DATA3,$B08_11
+        vpshufb           $SHFMSK,$DATA4,$B12_15
+___
+  }
+
+  # ;; =================================================
+  # ;; store shuffled cipher text for ghashing
+  $code .= <<___;
+        vmovdqa64         $B00_03,`$AESOUT_BLK_OFFSET + (0*64)`(%rsp)
+        vmovdqa64         $B04_07,`$AESOUT_BLK_OFFSET + (1*64)`(%rsp)
+        vmovdqa64         $B08_11,`$AESOUT_BLK_OFFSET + (2*64)`(%rsp)
+        vmovdqa64         $B12_15,`$AESOUT_BLK_OFFSET + (3*64)`(%rsp)
+___
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;;; Encryption of a single block
+sub ENCRYPT_SINGLE_BLOCK {
+  my $AES_KEY = $_[0];    # ; [in]
+  my $XMM0    = $_[1];    # ; [in/out]
+  my $GPR1    = $_[2];    # ; [clobbered]
+
+  my $rndsuffix = &random_string();
+
+  $code .= <<___;
+        # ; load number of rounds from AES_KEY structure (offset in bytes is
+        # ; size of the |rd_key| buffer)
+        mov             `4*15*4`($AES_KEY),@{[DWORD($GPR1)]}
+        cmp             \$9,@{[DWORD($GPR1)]}
+        je              .Laes_128_${rndsuffix}
+        cmp             \$11,@{[DWORD($GPR1)]}
+        je              .Laes_192_${rndsuffix}
+        cmp             \$13,@{[DWORD($GPR1)]}
+        je              .Laes_256_${rndsuffix}
+        jmp             .Lexit_aes_${rndsuffix}
+___
+  for my $keylen (sort keys %aes_rounds) {
+    my $nr = $aes_rounds{$keylen};
+    $code .= <<___;
+.align 32
+.Laes_${keylen}_${rndsuffix}:
+___
+    $code .= "vpxorq          `16*0`($AES_KEY),$XMM0, $XMM0\n\n";
+    for (my $i = 1; $i <= $nr; $i++) {
+      $code .= "vaesenc         `16*$i`($AES_KEY),$XMM0,$XMM0\n\n";
+    }
+    $code .= <<___;
+        vaesenclast     `16*($nr+1)`($AES_KEY),$XMM0,$XMM0
+        jmp .Lexit_aes_${rndsuffix}
+___
+  }
+  $code .= ".Lexit_aes_${rndsuffix}:\n\n";
+}
+
+sub CALC_J0 {
+  my $GCM128_CTX = $_[0];     #; [in] Pointer to context
+  my $IV         = $_[1];     #; [in] Pointer to IV
+  my $IV_LEN     = $_[2];     #; [in] IV length
+  my $J0         = $_[3];     #; [out] XMM reg to contain J0
+  my $ZT0        = $_[4];     #; [clobbered] ZMM register
+  my $ZT1        = $_[5];     #; [clobbered] ZMM register
+  my $ZT2        = $_[6];     #; [clobbered] ZMM register
+  my $ZT3        = $_[7];     #; [clobbered] ZMM register
+  my $ZT4        = $_[8];     #; [clobbered] ZMM register
+  my $ZT5        = $_[9];     #; [clobbered] ZMM register
+  my $ZT6        = $_[10];    #; [clobbered] ZMM register
+  my $ZT7        = $_[11];    #; [clobbered] ZMM register
+  my $ZT8        = $_[12];    #; [clobbered] ZMM register
+  my $ZT9        = $_[13];    #; [clobbered] ZMM register
+  my $ZT10       = $_[14];    #; [clobbered] ZMM register
+  my $ZT11       = $_[15];    #; [clobbered] ZMM register
+  my $ZT12       = $_[16];    #; [clobbered] ZMM register
+  my $ZT13       = $_[17];    #; [clobbered] ZMM register
+  my $ZT14       = $_[18];    #; [clobbered] ZMM register
+  my $ZT15       = $_[19];    #; [clobbered] ZMM register
+  my $ZT16       = $_[20];    #; [clobbered] ZMM register
+  my $T1         = $_[21];    #; [clobbered] GP register
+  my $T2         = $_[22];    #; [clobbered] GP register
+  my $T3         = $_[23];    #; [clobbered] GP register
+  my $T4         = $_[24];    #; [clobbered] GP register
+  my $MASKREG    = $_[25];    #; [clobbered] mask register
+
+  my $HTABLE = $T4;
+
+  # ;; J0 = GHASH(IV || 0s+64 || len(IV)64)
+  # ;; s = 16 * RoundUp(len(IV)/16) -  len(IV) */
+
+  # ;; Calculate GHASH of (IV || 0s)
+  $code .= "vpxor             $J0,$J0,$J0\n";
+
+  # ;; Get Htable pointer
+  $code .= "lea               `$CTX_OFFSET_HTable`($GCM128_CTX),$HTABLE\n";
+
+  &CALC_AAD_HASH(
+    $IV,   $IV_LEN, $J0,   $HTABLE, $ZT0, $ZT1,  $ZT2,  $ZT3,  $ZT4,
+    $ZT5,  $ZT6,    $ZT7,  $ZT8,    $ZT9, $ZT10, $ZT11, $ZT12, $ZT13,
+    $ZT14, $ZT15,   $ZT16, $T1,     $T2,  $T3,   $MASKREG);
+
+  # ;; Calculate GHASH of last 16-byte block (0 || len(IV)64)
+  $code .= <<___;
+        mov               $IV_LEN,$T1
+        shl               \$3,$T1      # ; IV length in bits
+        vmovq             $T1,@{[XWORD($ZT2)]}
+
+        # ;; Might need shuffle of ZT2
+        vpxorq            $J0,@{[XWORD($ZT2)]},$J0
+
+        vmovdqu64         @{[HashKeyByIdx(1, $HTABLE)]},@{[XWORD($ZT0)]}
+___
+  &GHASH_MUL($J0, @{[XWORD($ZT0)]}, @{[XWORD($ZT1)]}, @{[XWORD($ZT2)]}, @{[XWORD($ZT3)]});
+
+  $code .= "vpshufb           SHUF_MASK(%rip),$J0,$J0      # ; perform a 16Byte swap\n";
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;;; GCM_INIT_IV performs an initialization of gcm128_ctx struct to prepare for
+# ;;; encoding/decoding.
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+sub GCM_INIT_IV {
+  my $AES_KEYS   = $_[0];     # [in] AES key schedule
+  my $GCM128_CTX = $_[1];     # [in/out] GCM context
+  my $IV         = $_[2];     # [in] IV pointer
+  my $IV_LEN     = $_[3];     # [in] IV length
+  my $GPR1       = $_[4];     # [clobbered] GP register
+  my $GPR2       = $_[5];     # [clobbered] GP register
+  my $GPR3       = $_[6];     # [clobbered] GP register
+  my $GPR4       = $_[7];     # [clobbered] GP register
+  my $MASKREG    = $_[8];     # [clobbered] mask register
+  my $CUR_COUNT  = $_[9];     # [out] XMM with current counter
+  my $ZT0        = $_[10];    # [clobbered] ZMM register
+  my $ZT1        = $_[11];    # [clobbered] ZMM register
+  my $ZT2        = $_[12];    # [clobbered] ZMM register
+  my $ZT3        = $_[13];    # [clobbered] ZMM register
+  my $ZT4        = $_[14];    # [clobbered] ZMM register
+  my $ZT5        = $_[15];    # [clobbered] ZMM register
+  my $ZT6        = $_[16];    # [clobbered] ZMM register
+  my $ZT7        = $_[17];    # [clobbered] ZMM register
+  my $ZT8        = $_[18];    # [clobbered] ZMM register
+  my $ZT9        = $_[19];    # [clobbered] ZMM register
+  my $ZT10       = $_[20];    # [clobbered] ZMM register
+  my $ZT11       = $_[21];    # [clobbered] ZMM register
+  my $ZT12       = $_[22];    # [clobbered] ZMM register
+  my $ZT13       = $_[23];    # [clobbered] ZMM register
+  my $ZT14       = $_[24];    # [clobbered] ZMM register
+  my $ZT15       = $_[25];    # [clobbered] ZMM register
+  my $ZT16       = $_[26];    # [clobbered] ZMM register
+
+  my $ZT0x = $ZT0;
+  $ZT0x =~ s/zmm/xmm/;
+
+  $code .= <<___;
+        cmpq    \$12,$IV_LEN
+        je      iv_len_12_init_IV
+___
+
+  # ;; IV is different than 12 bytes
+  &CALC_J0($GCM128_CTX, $IV, $IV_LEN, $CUR_COUNT, $ZT0, $ZT1, $ZT2, $ZT3, $ZT4,
+    $ZT5, $ZT6, $ZT7, $ZT8, $ZT9, $ZT10, $ZT11, $ZT12, $ZT13,
+    $ZT14, $ZT15, $ZT16, $GPR1, $GPR2, $GPR3, $GPR4, $MASKREG);
+  $code .= <<___;
+       jmp      skip_iv_len_12_init_IV
+iv_len_12_init_IV:   # ;; IV is 12 bytes
+        # ;; read 12 IV bytes and pad with 0x00000001
+        vmovdqu8          ONEf(%rip),$CUR_COUNT
+        mov               $IV,$GPR2
+        mov               \$0x0000000000000fff,@{[DWORD($GPR1)]}
+        kmovq             $GPR1,$MASKREG
+        vmovdqu8          ($GPR2),${CUR_COUNT}{$MASKREG}         # ; ctr = IV | 0x1
+skip_iv_len_12_init_IV:
+        vmovdqu           $CUR_COUNT,$ZT0x
+___
+  &ENCRYPT_SINGLE_BLOCK($AES_KEYS, "$ZT0x", "$GPR1");    # ; E(K, Y0)
+  $code .= <<___;
+        vmovdqu           $ZT0x,`$CTX_OFFSET_EK0`($GCM128_CTX)   # ; save EK0 for finalization stage
+
+        # ;; store IV as counter in LE format
+        vpshufb           SHUF_MASK(%rip),$CUR_COUNT,$CUR_COUNT
+        vmovdqu           $CUR_COUNT,`$CTX_OFFSET_CurCount`($GCM128_CTX)   # ; save current counter Yi
+___
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;;; Cipher and ghash of payloads shorter than 256 bytes
+# ;;; - number of blocks in the message comes as argument
+# ;;; - depending on the number of blocks an optimized variant of
+# ;;;   INITIAL_BLOCKS_PARTIAL is invoked
+sub GCM_ENC_DEC_SMALL {
+  my $AES_KEYS       = $_[0];     # [in] key pointer
+  my $GCM128_CTX     = $_[1];     # [in] context pointer
+  my $CIPH_PLAIN_OUT = $_[2];     # [in] output buffer
+  my $PLAIN_CIPH_IN  = $_[3];     # [in] input buffer
+  my $PLAIN_CIPH_LEN = $_[4];     # [in] buffer length
+  my $ENC_DEC        = $_[5];     # [in] cipher direction
+  my $DATA_OFFSET    = $_[6];     # [in] data offset
+  my $LENGTH         = $_[7];     # [in] data length
+  my $NUM_BLOCKS     = $_[8];     # [in] number of blocks to process 1 to 16
+  my $CTR            = $_[9];     # [in/out] XMM counter block
+  my $HASH_IN_OUT    = $_[10];    # [in/out] XMM GHASH value
+  my $ZTMP0          = $_[11];    # [clobbered] ZMM register
+  my $ZTMP1          = $_[12];    # [clobbered] ZMM register
+  my $ZTMP2          = $_[13];    # [clobbered] ZMM register
+  my $ZTMP3          = $_[14];    # [clobbered] ZMM register
+  my $ZTMP4          = $_[15];    # [clobbered] ZMM register
+  my $ZTMP5          = $_[16];    # [clobbered] ZMM register
+  my $ZTMP6          = $_[17];    # [clobbered] ZMM register
+  my $ZTMP7          = $_[18];    # [clobbered] ZMM register
+  my $ZTMP8          = $_[19];    # [clobbered] ZMM register
+  my $ZTMP9          = $_[20];    # [clobbered] ZMM register
+  my $ZTMP10         = $_[21];    # [clobbered] ZMM register
+  my $ZTMP11         = $_[22];    # [clobbered] ZMM register
+  my $ZTMP12         = $_[23];    # [clobbered] ZMM register
+  my $ZTMP13         = $_[24];    # [clobbered] ZMM register
+  my $ZTMP14         = $_[25];    # [clobbered] ZMM register
+  my $IA0            = $_[26];    # [clobbered] GP register
+  my $IA1            = $_[27];    # [clobbered] GP register
+  my $MASKREG        = $_[28];    # [clobbered] mask register
+  my $SHUFMASK       = $_[29];    # [in] ZMM with BE/LE shuffle mask
+  my $PBLOCK_LEN     = $_[30];    # [in] partial block length
+
+  my $rndsuffix = &random_string();
+
+  $code .= <<___;
+        cmp               \$8,$NUM_BLOCKS
+        je                .L_small_initial_num_blocks_is_8_${rndsuffix}
+        jl                .L_small_initial_num_blocks_is_7_1_${rndsuffix}
+
+
+        cmp               \$12,$NUM_BLOCKS
+        je                .L_small_initial_num_blocks_is_12_${rndsuffix}
+        jl                .L_small_initial_num_blocks_is_11_9_${rndsuffix}
+
+        # ;; 16, 15, 14 or 13
+        cmp               \$16,$NUM_BLOCKS
+        je                .L_small_initial_num_blocks_is_16_${rndsuffix}
+        cmp               \$15,$NUM_BLOCKS
+        je                .L_small_initial_num_blocks_is_15_${rndsuffix}
+        cmp               \$14,$NUM_BLOCKS
+        je                .L_small_initial_num_blocks_is_14_${rndsuffix}
+        jmp               .L_small_initial_num_blocks_is_13_${rndsuffix}
+
+.L_small_initial_num_blocks_is_11_9_${rndsuffix}:
+        # ;; 11, 10 or 9
+        cmp               \$11,$NUM_BLOCKS
+        je                .L_small_initial_num_blocks_is_11_${rndsuffix}
+        cmp               \$10,$NUM_BLOCKS
+        je                .L_small_initial_num_blocks_is_10_${rndsuffix}
+        jmp               .L_small_initial_num_blocks_is_9_${rndsuffix}
+
+.L_small_initial_num_blocks_is_7_1_${rndsuffix}:
+        cmp               \$4,$NUM_BLOCKS
+        je                .L_small_initial_num_blocks_is_4_${rndsuffix}
+        jl                .L_small_initial_num_blocks_is_3_1_${rndsuffix}
+        # ;; 7, 6 or 5
+        cmp               \$7,$NUM_BLOCKS
+        je                .L_small_initial_num_blocks_is_7_${rndsuffix}
+        cmp               \$6,$NUM_BLOCKS
+        je                .L_small_initial_num_blocks_is_6_${rndsuffix}
+        jmp               .L_small_initial_num_blocks_is_5_${rndsuffix}
+
+.L_small_initial_num_blocks_is_3_1_${rndsuffix}:
+        # ;; 3, 2 or 1
+        cmp               \$3,$NUM_BLOCKS
+        je                .L_small_initial_num_blocks_is_3_${rndsuffix}
+        cmp               \$2,$NUM_BLOCKS
+        je                .L_small_initial_num_blocks_is_2_${rndsuffix}
+
+        # ;; for $NUM_BLOCKS == 1, just fall through and no 'jmp' needed
+
+        # ;; Generation of different block size variants
+        # ;; - one block size has to be the first one
+___
+
+  for (my $num_blocks = 1; $num_blocks <= 16; $num_blocks++) {
+    $code .= ".L_small_initial_num_blocks_is_${num_blocks}_${rndsuffix}:\n";
+    &INITIAL_BLOCKS_PARTIAL(
+      $AES_KEYS,   $GCM128_CTX, $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN, $LENGTH,   $DATA_OFFSET,
+      $num_blocks, $CTR,        $HASH_IN_OUT,    $ENC_DEC,       $ZTMP0,    $ZTMP1,
+      $ZTMP2,      $ZTMP3,      $ZTMP4,          $ZTMP5,         $ZTMP6,    $ZTMP7,
+      $ZTMP8,      $ZTMP9,      $ZTMP10,         $ZTMP11,        $ZTMP12,   $ZTMP13,
+      $ZTMP14,     $IA0,        $IA1,            $MASKREG,       $SHUFMASK, $PBLOCK_LEN);
+
+    if ($num_blocks != 16) {
+      $code .= "jmp           .L_small_initial_blocks_encrypted_${rndsuffix}\n";
+    }
+  }
+
+  $code .= ".L_small_initial_blocks_encrypted_${rndsuffix}:\n";
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ; GCM_ENC_DEC Encrypts/Decrypts given data. Assumes that the passed gcm128_context
+# ; struct has been initialized by GCM_INIT_IV
+# ; Requires the input data be at least 1 byte long because of READ_SMALL_INPUT_DATA.
+# ; Clobbers rax, r10-r15, and zmm0-zmm31, k1
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+sub GCM_ENC_DEC {
+  my $AES_KEYS       = $_[0];    # [in] AES Key schedule
+  my $GCM128_CTX     = $_[1];    # [in] context pointer
+  my $PBLOCK_LEN     = $_[2];    # [in/out] length of partial block at the moment of previous update
+  my $PLAIN_CIPH_IN  = $_[3];    # [in] input buffer pointer
+  my $PLAIN_CIPH_LEN = $_[4];    # [in] buffer length
+  my $CIPH_PLAIN_OUT = $_[5];    # [in] output buffer pointer
+  my $ENC_DEC        = $_[6];    # [in] cipher direction
+
+  my $IA0 = "%r10";
+  my $IA1 = "%r12";
+  my $IA2 = "%r13";
+  my $IA3 = "%r15";
+  my $IA4 = "%rax";
+  my $IA5 = "%r11";
+  my $IA6 = "%rbx";
+  my $IA7 = "%r14";
+
+  my $LENGTH = $win64 ? $IA2 : $PLAIN_CIPH_LEN;
+
+  my $CTR_CHECK   = $IA3;
+  my $DATA_OFFSET = $IA4;
+  my $HASHK_PTR   = $IA6;
+
+  my $HKEYS_READY = $IA7;
+
+  my $CTR_BLOCKz = "%zmm2";
+  my $CTR_BLOCKx = "%xmm2";
+
+  # ; hardcoded in GCM_INIT
+
+  my $AAD_HASHz = "%zmm14";
+  my $AAD_HASHx = "%xmm14";
+
+  my $ZTMP0  = "%zmm0";
+  my $ZTMP1  = "%zmm3";
+  my $ZTMP2  = "%zmm4";
+  my $ZTMP3  = "%zmm5";
+  my $ZTMP4  = "%zmm6";
+  my $ZTMP5  = "%zmm7";
+  my $ZTMP6  = "%zmm10";
+  my $ZTMP7  = "%zmm11";
+  my $ZTMP8  = "%zmm12";
+  my $ZTMP9  = "%zmm13";
+  my $ZTMP10 = "%zmm15";
+  my $ZTMP11 = "%zmm16";
+  my $ZTMP12 = "%zmm17";
+
+  my $ZTMP13 = "%zmm19";
+  my $ZTMP14 = "%zmm20";
+  my $ZTMP15 = "%zmm21";
+  my $ZTMP16 = "%zmm30";
+  my $ZTMP17 = "%zmm31";
+  my $ZTMP18 = "%zmm1";
+  my $ZTMP19 = "%zmm18";
+  my $ZTMP20 = "%zmm8";
+  my $ZTMP21 = "%zmm22";
+  my $ZTMP22 = "%zmm23";
+
+  my $GH        = "%zmm24";
+  my $GL        = "%zmm25";
+  my $GM        = "%zmm26";
+  my $SHUF_MASK = "%zmm29";
+
+  # ; Unused in the small packet path
+  my $ADDBE_4x4  = "%zmm27";
+  my $ADDBE_1234 = "%zmm28";
+
+  my $MASKREG = "%k1";
+
+  my $rndsuffix = &random_string();
+
+  # ;; reduction every 48 blocks, depth 32 blocks
+  # ;; @note 48 blocks is the maximum capacity of the stack frame
+  my $big_loop_nblocks = 48;
+  my $big_loop_depth   = 32;
+
+  # ;;; Macro flow depending on packet size
+  # ;;; - LENGTH <= 16 blocks
+  # ;;;   - cipher followed by hashing (reduction)
+  # ;;; - 16 blocks < LENGTH < 32 blocks
+  # ;;;   - cipher 16 blocks
+  # ;;;   - cipher N blocks & hash 16 blocks, hash N blocks (reduction)
+  # ;;; - 32 blocks < LENGTH < 48 blocks
+  # ;;;   - cipher 2 x 16 blocks
+  # ;;;   - hash 16 blocks
+  # ;;;   - cipher N blocks & hash 16 blocks, hash N blocks (reduction)
+  # ;;; - LENGTH >= 48 blocks
+  # ;;;   - cipher 2 x 16 blocks
+  # ;;;   - while (data_to_cipher >= 48 blocks):
+  # ;;;     - cipher 16 blocks & hash 16 blocks
+  # ;;;     - cipher 16 blocks & hash 16 blocks
+  # ;;;     - cipher 16 blocks & hash 16 blocks (reduction)
+  # ;;;   - if (data_to_cipher >= 32 blocks):
+  # ;;;     - cipher 16 blocks & hash 16 blocks
+  # ;;;     - cipher 16 blocks & hash 16 blocks
+  # ;;;     - hash 16 blocks (reduction)
+  # ;;;     - cipher N blocks & hash 16 blocks, hash N blocks (reduction)
+  # ;;;   - elif (data_to_cipher >= 16 blocks):
+  # ;;;     - cipher 16 blocks & hash 16 blocks
+  # ;;;     - hash 16 blocks
+  # ;;;     - cipher N blocks & hash 16 blocks, hash N blocks (reduction)
+  # ;;;   - else:
+  # ;;;     - hash 16 blocks
+  # ;;;     - cipher N blocks & hash 16 blocks, hash N blocks (reduction)
+
+  if ($win64) {
+    $code .= "cmpq              \$0,$PLAIN_CIPH_LEN\n";
+  } else {
+    $code .= "or                $PLAIN_CIPH_LEN,$PLAIN_CIPH_LEN\n";
+  }
+  $code .= "je                 .L_enc_dec_abort_${rndsuffix}\n";
+
+  $code .= "xor                $HKEYS_READY, $HKEYS_READY\n";
+
+  $code .= "vmovdqu64         `$CTX_OFFSET_AadHash`($GCM128_CTX),$AAD_HASHx\n";
+
+  # BE -> LE conversion
+  $code .= "vpshufb           SHUF_MASK(%rip),$AAD_HASHx,$AAD_HASHx\n";
+
+  # ;; Used for the update flow - if there was a previous partial
+  # ;; block fill the remaining bytes here.
+  &PARTIAL_BLOCK(
+    $GCM128_CTX,  $PBLOCK_LEN, $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN, $PLAIN_CIPH_LEN,
+    $DATA_OFFSET, $AAD_HASHx,  $ENC_DEC,        $IA0,           $IA1,
+    $IA2,         $ZTMP0,      $ZTMP1,          $ZTMP2,         $ZTMP3,
+    $ZTMP4,       $ZTMP5,      $ZTMP6,          $ZTMP7,         $MASKREG);
+
+  $code .= "vmovdqu64         `$CTX_OFFSET_CurCount`($GCM128_CTX),$CTR_BLOCKx\n";
+
+  # ;; Save the amount of data left to process in $LENGTH
+  # ;; NOTE: PLAIN_CIPH_LEN is a register on linux;
+  if ($win64) {
+    $code .= "mov               $PLAIN_CIPH_LEN,$LENGTH\n";
+  }
+
+  # ;; There may be no more data if it was consumed in the partial block.
+  $code .= <<___;
+        sub               $DATA_OFFSET,$LENGTH
+        je                .L_enc_dec_done_${rndsuffix}
+___
+
+  $code .= <<___;
+        cmp               \$`(16 * 16)`,$LENGTH
+        jbe              .L_message_below_equal_16_blocks_${rndsuffix}
+
+        vmovdqa64         SHUF_MASK(%rip),$SHUF_MASK
+        vmovdqa64         ddq_addbe_4444(%rip),$ADDBE_4x4
+        vmovdqa64         ddq_addbe_1234(%rip),$ADDBE_1234
+
+        # ;; start the pipeline
+        # ;; - 32 blocks aes-ctr
+        # ;; - 16 blocks ghash + aes-ctr
+
+        # ;; set up CTR_CHECK
+        vmovd             $CTR_BLOCKx,@{[DWORD($CTR_CHECK)]}
+        and               \$255,@{[DWORD($CTR_CHECK)]}
+        # ;; in LE format after init, convert to BE
+        vshufi64x2        \$0,$CTR_BLOCKz,$CTR_BLOCKz,$CTR_BLOCKz
+        vpshufb           $SHUF_MASK,$CTR_BLOCKz,$CTR_BLOCKz
+___
+
+  # ;; ==== AES-CTR - first 16 blocks
+  my $aesout_offset      = ($STACK_LOCAL_OFFSET + (0 * 16));
+  my $data_in_out_offset = 0;
+  &INITIAL_BLOCKS_16(
+    $PLAIN_CIPH_IN, $CIPH_PLAIN_OUT, $AES_KEYS,      $DATA_OFFSET,        "no_ghash", $CTR_BLOCKz,
+    $CTR_CHECK,     $ADDBE_4x4,      $ADDBE_1234,    $ZTMP0,              $ZTMP1,     $ZTMP2,
+    $ZTMP3,         $ZTMP4,          $ZTMP5,         $ZTMP6,              $ZTMP7,     $ZTMP8,
+    $SHUF_MASK,     $ENC_DEC,        $aesout_offset, $data_in_out_offset, $IA0);
+
+  # ;; Get Htable pointer
+  $code .= "lea               `$CTX_OFFSET_HTable`($GCM128_CTX),$IA1\n";
+  &precompute_hkeys_on_stack($IA1, $HKEYS_READY, $ZTMP0, $ZTMP1, $ZTMP2, $ZTMP3, $ZTMP4, $ZTMP5, $ZTMP6, "first16");
+
+  $code .= <<___;
+        cmp               \$`(32 * 16)`,$LENGTH
+        jb                .L_message_below_32_blocks_${rndsuffix}
+___
+
+  # ;; ==== AES-CTR - next 16 blocks
+  $aesout_offset      = ($STACK_LOCAL_OFFSET + (16 * 16));
+  $data_in_out_offset = (16 * 16);
+  &INITIAL_BLOCKS_16(
+    $PLAIN_CIPH_IN, $CIPH_PLAIN_OUT, $AES_KEYS,      $DATA_OFFSET,        "no_ghash", $CTR_BLOCKz,
+    $CTR_CHECK,     $ADDBE_4x4,      $ADDBE_1234,    $ZTMP0,              $ZTMP1,     $ZTMP2,
+    $ZTMP3,         $ZTMP4,          $ZTMP5,         $ZTMP6,              $ZTMP7,     $ZTMP8,
+    $SHUF_MASK,     $ENC_DEC,        $aesout_offset, $data_in_out_offset, $IA0);
+
+  &precompute_hkeys_on_stack($IA1, $HKEYS_READY, $ZTMP0, $ZTMP1, $ZTMP2, $ZTMP3, $ZTMP4, $ZTMP5, $ZTMP6, "last32");
+  $code .= "mov     \$1,$HKEYS_READY\n";
+
+  $code .= <<___;
+        add               \$`(32 * 16)`,$DATA_OFFSET
+        sub               \$`(32 * 16)`,$LENGTH
+
+        cmp               \$`($big_loop_nblocks * 16)`,$LENGTH
+        jb                .L_no_more_big_nblocks_${rndsuffix}
+___
+
+  # ;; ====
+  # ;; ==== AES-CTR + GHASH - 48 blocks loop
+  # ;; ====
+  $code .= ".L_encrypt_big_nblocks_${rndsuffix}:\n";
+
+  # ;; ==== AES-CTR + GHASH - 16 blocks, start
+  $aesout_offset      = ($STACK_LOCAL_OFFSET + (32 * 16));
+  $data_in_out_offset = (0 * 16);
+  my $ghashin_offset = ($STACK_LOCAL_OFFSET + (0 * 16));
+  &GHASH_16_ENCRYPT_16_PARALLEL(
+    $AES_KEYS, $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN,  $DATA_OFFSET, $CTR_BLOCKz,         $CTR_CHECK,
+    48,        $aesout_offset,  $ghashin_offset, $SHUF_MASK,   $ZTMP0,              $ZTMP1,
+    $ZTMP2,    $ZTMP3,          $ZTMP4,          $ZTMP5,       $ZTMP6,              $ZTMP7,
+    $ZTMP8,    $ZTMP9,          $ZTMP10,         $ZTMP11,      $ZTMP12,             $ZTMP13,
+    $ZTMP14,   $ZTMP15,         $ZTMP16,         $ZTMP17,      $ZTMP18,             $ZTMP19,
+    $ZTMP20,   $ZTMP21,         $ZTMP22,         $ADDBE_4x4,   $ADDBE_1234,         $GL,
+    $GH,       $GM,             "first_time",    $ENC_DEC,     $data_in_out_offset, $AAD_HASHz,
+    $IA0);
+
+  # ;; ==== AES-CTR + GHASH - 16 blocks, no reduction
+  $aesout_offset      = ($STACK_LOCAL_OFFSET + (0 * 16));
+  $data_in_out_offset = (16 * 16);
+  $ghashin_offset     = ($STACK_LOCAL_OFFSET + (16 * 16));
+  &GHASH_16_ENCRYPT_16_PARALLEL(
+    $AES_KEYS, $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN,  $DATA_OFFSET, $CTR_BLOCKz,         $CTR_CHECK,
+    32,        $aesout_offset,  $ghashin_offset, $SHUF_MASK,   $ZTMP0,              $ZTMP1,
+    $ZTMP2,    $ZTMP3,          $ZTMP4,          $ZTMP5,       $ZTMP6,              $ZTMP7,
+    $ZTMP8,    $ZTMP9,          $ZTMP10,         $ZTMP11,      $ZTMP12,             $ZTMP13,
+    $ZTMP14,   $ZTMP15,         $ZTMP16,         $ZTMP17,      $ZTMP18,             $ZTMP19,
+    $ZTMP20,   $ZTMP21,         $ZTMP22,         $ADDBE_4x4,   $ADDBE_1234,         $GL,
+    $GH,       $GM,             "no_reduction",  $ENC_DEC,     $data_in_out_offset, "no_ghash_in",
+    $IA0);
+
+  # ;; ==== AES-CTR + GHASH - 16 blocks, reduction
+  $aesout_offset      = ($STACK_LOCAL_OFFSET + (16 * 16));
+  $data_in_out_offset = (32 * 16);
+  $ghashin_offset     = ($STACK_LOCAL_OFFSET + (32 * 16));
+  &GHASH_16_ENCRYPT_16_PARALLEL(
+    $AES_KEYS, $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN,    $DATA_OFFSET, $CTR_BLOCKz,         $CTR_CHECK,
+    16,        $aesout_offset,  $ghashin_offset,   $SHUF_MASK,   $ZTMP0,              $ZTMP1,
+    $ZTMP2,    $ZTMP3,          $ZTMP4,            $ZTMP5,       $ZTMP6,              $ZTMP7,
+    $ZTMP8,    $ZTMP9,          $ZTMP10,           $ZTMP11,      $ZTMP12,             $ZTMP13,
+    $ZTMP14,   $ZTMP15,         $ZTMP16,           $ZTMP17,      $ZTMP18,             $ZTMP19,
+    $ZTMP20,   $ZTMP21,         $ZTMP22,           $ADDBE_4x4,   $ADDBE_1234,         $GL,
+    $GH,       $GM,             "final_reduction", $ENC_DEC,     $data_in_out_offset, "no_ghash_in",
+    $IA0);
+
+  # ;; === xor cipher block 0 with GHASH (ZT4)
+  $code .= <<___;
+        vmovdqa64         $ZTMP4,$AAD_HASHz
+
+        add               \$`($big_loop_nblocks * 16)`,$DATA_OFFSET
+        sub               \$`($big_loop_nblocks * 16)`,$LENGTH
+        cmp               \$`($big_loop_nblocks * 16)`,$LENGTH
+        jae               .L_encrypt_big_nblocks_${rndsuffix}
+
+.L_no_more_big_nblocks_${rndsuffix}:
+
+        cmp               \$`(32 * 16)`,$LENGTH
+        jae               .L_encrypt_32_blocks_${rndsuffix}
+
+        cmp               \$`(16 * 16)`,$LENGTH
+        jae               .L_encrypt_16_blocks_${rndsuffix}
+___
+
+  # ;; =====================================================
+  # ;; =====================================================
+  # ;; ==== GHASH 1 x 16 blocks
+  # ;; ==== GHASH 1 x 16 blocks (reduction) & encrypt N blocks
+  # ;; ====      then GHASH N blocks
+  $code .= ".L_encrypt_0_blocks_ghash_32_${rndsuffix}:\n";
+
+  # ;; calculate offset to the right hash key
+  $code .= <<___;
+mov               @{[DWORD($LENGTH)]},@{[DWORD($IA0)]}
+and               \$~15,@{[DWORD($IA0)]}
+mov               \$`@{[HashKeyOffsetByIdx(32,"frame")]}`,@{[DWORD($HASHK_PTR)]}
+sub               @{[DWORD($IA0)]},@{[DWORD($HASHK_PTR)]}
+___
+
+  # ;; ==== GHASH 32 blocks and follow with reduction
+  &GHASH_16("start", $GH, $GM, $GL, "%rsp", $STACK_LOCAL_OFFSET, (0 * 16),
+    "%rsp", $HASHK_PTR, 0, $AAD_HASHz, $ZTMP0, $ZTMP1, $ZTMP2, $ZTMP3, $ZTMP4, $ZTMP5, $ZTMP6, $ZTMP7, $ZTMP8, $ZTMP9);
+
+  # ;; ==== GHASH 1 x 16 blocks with reduction + cipher and ghash on the reminder
+  $ghashin_offset = ($STACK_LOCAL_OFFSET + (16 * 16));
+  $code .= "add               \$`(16 * 16)`,@{[DWORD($HASHK_PTR)]}\n";
+  &GCM_ENC_DEC_LAST(
+    $AES_KEYS,   $GCM128_CTX, $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN,  $DATA_OFFSET, $LENGTH,
+    $CTR_BLOCKz, $CTR_CHECK,  $HASHK_PTR,      $ghashin_offset, $SHUF_MASK,   $ZTMP0,
+    $ZTMP1,      $ZTMP2,      $ZTMP3,          $ZTMP4,          $ZTMP5,       $ZTMP6,
+    $ZTMP7,      $ZTMP8,      $ZTMP9,          $ZTMP10,         $ZTMP11,      $ZTMP12,
+    $ZTMP13,     $ZTMP14,     $ZTMP15,         $ZTMP16,         $ZTMP17,      $ZTMP18,
+    $ZTMP19,     $ZTMP20,     $ZTMP21,         $ZTMP22,         $ADDBE_4x4,   $ADDBE_1234,
+    "mid",       $GL,         $GH,             $GM,             $ENC_DEC,     $AAD_HASHz,
+    $IA0,        $IA5,        $MASKREG,        $PBLOCK_LEN);
+
+  $code .= "vpshufb           @{[XWORD($SHUF_MASK)]},$CTR_BLOCKx,$CTR_BLOCKx\n";
+  $code .= "jmp           .L_ghash_done_${rndsuffix}\n";
+
+  # ;; =====================================================
+  # ;; =====================================================
+  # ;; ==== GHASH & encrypt 1 x 16 blocks
+  # ;; ==== GHASH & encrypt 1 x 16 blocks
+  # ;; ==== GHASH 1 x 16 blocks (reduction)
+  # ;; ==== GHASH 1 x 16 blocks (reduction) & encrypt N blocks
+  # ;; ====      then GHASH N blocks
+  $code .= ".L_encrypt_32_blocks_${rndsuffix}:\n";
+
+  # ;; ==== AES-CTR + GHASH - 16 blocks, start
+  $aesout_offset  = ($STACK_LOCAL_OFFSET + (32 * 16));
+  $ghashin_offset = ($STACK_LOCAL_OFFSET + (0 * 16));
+  $data_in_out_offset = (0 * 16);
+  &GHASH_16_ENCRYPT_16_PARALLEL(
+    $AES_KEYS, $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN,  $DATA_OFFSET, $CTR_BLOCKz,         $CTR_CHECK,
+    48,        $aesout_offset,  $ghashin_offset, $SHUF_MASK,   $ZTMP0,              $ZTMP1,
+    $ZTMP2,    $ZTMP3,          $ZTMP4,          $ZTMP5,       $ZTMP6,              $ZTMP7,
+    $ZTMP8,    $ZTMP9,          $ZTMP10,         $ZTMP11,      $ZTMP12,             $ZTMP13,
+    $ZTMP14,   $ZTMP15,         $ZTMP16,         $ZTMP17,      $ZTMP18,             $ZTMP19,
+    $ZTMP20,   $ZTMP21,         $ZTMP22,         $ADDBE_4x4,   $ADDBE_1234,         $GL,
+    $GH,       $GM,             "first_time",    $ENC_DEC,     $data_in_out_offset, $AAD_HASHz,
+    $IA0);
+
+  # ;; ==== AES-CTR + GHASH - 16 blocks, no reduction
+  $aesout_offset  = ($STACK_LOCAL_OFFSET + (0 * 16));
+  $ghashin_offset = ($STACK_LOCAL_OFFSET + (16 * 16));
+  $data_in_out_offset = (16 * 16);
+  &GHASH_16_ENCRYPT_16_PARALLEL(
+    $AES_KEYS, $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN,  $DATA_OFFSET, $CTR_BLOCKz,         $CTR_CHECK,
+    32,        $aesout_offset,  $ghashin_offset, $SHUF_MASK,   $ZTMP0,              $ZTMP1,
+    $ZTMP2,    $ZTMP3,          $ZTMP4,          $ZTMP5,       $ZTMP6,              $ZTMP7,
+    $ZTMP8,    $ZTMP9,          $ZTMP10,         $ZTMP11,      $ZTMP12,             $ZTMP13,
+    $ZTMP14,   $ZTMP15,         $ZTMP16,         $ZTMP17,      $ZTMP18,             $ZTMP19,
+    $ZTMP20,   $ZTMP21,         $ZTMP22,         $ADDBE_4x4,   $ADDBE_1234,         $GL,
+    $GH,       $GM,             "no_reduction",  $ENC_DEC,     $data_in_out_offset, "no_ghash_in",
+    $IA0);
+
+  # ;; ==== GHASH 16 blocks with reduction
+  &GHASH_16(
+    "end_reduce", $GH, $GM, $GL, "%rsp", $STACK_LOCAL_OFFSET, (32 * 16),
+    "%rsp", &HashKeyOffsetByIdx(16, "frame"),
+    0, $AAD_HASHz, $ZTMP0, $ZTMP1, $ZTMP2, $ZTMP3, $ZTMP4, $ZTMP5, $ZTMP6, $ZTMP7, $ZTMP8, $ZTMP9);
+
+  # ;; ==== GHASH 1 x 16 blocks with reduction + cipher and ghash on the reminder
+  $ghashin_offset = ($STACK_LOCAL_OFFSET + (0 * 16));
+  $code .= <<___;
+        sub               \$`(32 * 16)`,$LENGTH
+        add               \$`(32 * 16)`,$DATA_OFFSET
+___
+
+  # ;; calculate offset to the right hash key
+  $code .= "mov               @{[DWORD($LENGTH)]},@{[DWORD($IA0)]}\n";
+  $code .= <<___;
+        and               \$~15,@{[DWORD($IA0)]}
+        mov               \$`@{[HashKeyOffsetByIdx(16,"frame")]}`,@{[DWORD($HASHK_PTR)]}
+        sub               @{[DWORD($IA0)]},@{[DWORD($HASHK_PTR)]}
+___
+  &GCM_ENC_DEC_LAST(
+    $AES_KEYS,   $GCM128_CTX, $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN,  $DATA_OFFSET, $LENGTH,
+    $CTR_BLOCKz, $CTR_CHECK,  $HASHK_PTR,      $ghashin_offset, $SHUF_MASK,   $ZTMP0,
+    $ZTMP1,      $ZTMP2,      $ZTMP3,          $ZTMP4,          $ZTMP5,       $ZTMP6,
+    $ZTMP7,      $ZTMP8,      $ZTMP9,          $ZTMP10,         $ZTMP11,      $ZTMP12,
+    $ZTMP13,     $ZTMP14,     $ZTMP15,         $ZTMP16,         $ZTMP17,      $ZTMP18,
+    $ZTMP19,     $ZTMP20,     $ZTMP21,         $ZTMP22,         $ADDBE_4x4,   $ADDBE_1234,
+    "start",     $GL,         $GH,             $GM,             $ENC_DEC,     $AAD_HASHz,
+    $IA0,        $IA5,        $MASKREG,        $PBLOCK_LEN);
+
+  $code .= "vpshufb           @{[XWORD($SHUF_MASK)]},$CTR_BLOCKx,$CTR_BLOCKx\n";
+  $code .= "jmp           .L_ghash_done_${rndsuffix}\n";
+
+  # ;; =====================================================
+  # ;; =====================================================
+  # ;; ==== GHASH & encrypt 16 blocks (done before)
+  # ;; ==== GHASH 1 x 16 blocks
+  # ;; ==== GHASH 1 x 16 blocks (reduction) & encrypt N blocks
+  # ;; ====      then GHASH N blocks
+  $code .= ".L_encrypt_16_blocks_${rndsuffix}:\n";
+
+  # ;; ==== AES-CTR + GHASH - 16 blocks, start
+  $aesout_offset  = ($STACK_LOCAL_OFFSET + (32 * 16));
+  $ghashin_offset = ($STACK_LOCAL_OFFSET + (0 * 16));
+  $data_in_out_offset = (0 * 16);
+  &GHASH_16_ENCRYPT_16_PARALLEL(
+    $AES_KEYS, $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN,  $DATA_OFFSET, $CTR_BLOCKz,         $CTR_CHECK,
+    48,        $aesout_offset,  $ghashin_offset, $SHUF_MASK,   $ZTMP0,              $ZTMP1,
+    $ZTMP2,    $ZTMP3,          $ZTMP4,          $ZTMP5,       $ZTMP6,              $ZTMP7,
+    $ZTMP8,    $ZTMP9,          $ZTMP10,         $ZTMP11,      $ZTMP12,             $ZTMP13,
+    $ZTMP14,   $ZTMP15,         $ZTMP16,         $ZTMP17,      $ZTMP18,             $ZTMP19,
+    $ZTMP20,   $ZTMP21,         $ZTMP22,         $ADDBE_4x4,   $ADDBE_1234,         $GL,
+    $GH,       $GM,             "first_time",    $ENC_DEC,     $data_in_out_offset, $AAD_HASHz,
+    $IA0);
+
+  # ;; ==== GHASH 1 x 16 blocks
+  &GHASH_16(
+    "mid", $GH, $GM, $GL, "%rsp", $STACK_LOCAL_OFFSET, (16 * 16),
+    "%rsp", &HashKeyOffsetByIdx(32, "frame"),
+    0, "no_hash_input", $ZTMP0, $ZTMP1, $ZTMP2, $ZTMP3, $ZTMP4, $ZTMP5, $ZTMP6, $ZTMP7, $ZTMP8, $ZTMP9);
+
+  # ;; ==== GHASH 1 x 16 blocks with reduction + cipher and ghash on the reminder
+  $ghashin_offset = ($STACK_LOCAL_OFFSET + (32 * 16));
+  $code .= <<___;
+        sub               \$`(16 * 16)`,$LENGTH
+        add               \$`(16 * 16)`,$DATA_OFFSET
+___
+  &GCM_ENC_DEC_LAST(
+    $AES_KEYS,    $GCM128_CTX, $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN,
+    $DATA_OFFSET, $LENGTH,     $CTR_BLOCKz,     $CTR_CHECK,
+    &HashKeyOffsetByIdx(16, "frame"), $ghashin_offset, $SHUF_MASK, $ZTMP0,
+    $ZTMP1,       $ZTMP2,     $ZTMP3,     $ZTMP4,
+    $ZTMP5,       $ZTMP6,     $ZTMP7,     $ZTMP8,
+    $ZTMP9,       $ZTMP10,    $ZTMP11,    $ZTMP12,
+    $ZTMP13,      $ZTMP14,    $ZTMP15,    $ZTMP16,
+    $ZTMP17,      $ZTMP18,    $ZTMP19,    $ZTMP20,
+    $ZTMP21,      $ZTMP22,    $ADDBE_4x4, $ADDBE_1234,
+    "end_reduce", $GL,        $GH,        $GM,
+    $ENC_DEC,     $AAD_HASHz, $IA0,       $IA5,
+    $MASKREG,     $PBLOCK_LEN);
+
+  $code .= "vpshufb           @{[XWORD($SHUF_MASK)]},$CTR_BLOCKx,$CTR_BLOCKx\n";
+  $code .= <<___;
+        jmp               .L_ghash_done_${rndsuffix}
+
+.L_message_below_32_blocks_${rndsuffix}:
+        # ;; 32 > number of blocks > 16
+
+        sub               \$`(16 * 16)`,$LENGTH
+        add               \$`(16 * 16)`,$DATA_OFFSET
+___
+  $ghashin_offset = ($STACK_LOCAL_OFFSET + (0 * 16));
+
+  # ;; calculate offset to the right hash key
+  $code .= "mov               @{[DWORD($LENGTH)]},@{[DWORD($IA0)]}\n";
+
+  # ;; Get Htable pointer
+  $code .= "lea               `$CTX_OFFSET_HTable`($GCM128_CTX),$IA1\n";
+  &precompute_hkeys_on_stack($IA1, $HKEYS_READY, $ZTMP0, $ZTMP1, $ZTMP2, $ZTMP3, $ZTMP4, $ZTMP5, $ZTMP6, "mid16");
+  $code .= "mov     \$1,$HKEYS_READY\n";
+
+  $code .= <<___;
+and               \$~15,@{[DWORD($IA0)]}
+mov               \$`@{[HashKeyOffsetByIdx(16,"frame")]}`,@{[DWORD($HASHK_PTR)]}
+sub               @{[DWORD($IA0)]},@{[DWORD($HASHK_PTR)]}
+___
+
+  &GCM_ENC_DEC_LAST(
+    $AES_KEYS,   $GCM128_CTX, $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN,  $DATA_OFFSET, $LENGTH,
+    $CTR_BLOCKz, $CTR_CHECK,  $HASHK_PTR,      $ghashin_offset, $SHUF_MASK,   $ZTMP0,
+    $ZTMP1,      $ZTMP2,      $ZTMP3,          $ZTMP4,          $ZTMP5,       $ZTMP6,
+    $ZTMP7,      $ZTMP8,      $ZTMP9,          $ZTMP10,         $ZTMP11,      $ZTMP12,
+    $ZTMP13,     $ZTMP14,     $ZTMP15,         $ZTMP16,         $ZTMP17,      $ZTMP18,
+    $ZTMP19,     $ZTMP20,     $ZTMP21,         $ZTMP22,         $ADDBE_4x4,   $ADDBE_1234,
+    "start",     $GL,         $GH,             $GM,             $ENC_DEC,     $AAD_HASHz,
+    $IA0,        $IA5,        $MASKREG,        $PBLOCK_LEN);
+
+  $code .= "vpshufb           @{[XWORD($SHUF_MASK)]},$CTR_BLOCKx,$CTR_BLOCKx\n";
+  $code .= <<___;
+        jmp           .L_ghash_done_${rndsuffix}
+
+.L_message_below_equal_16_blocks_${rndsuffix}:
+        # ;; Determine how many blocks to process
+        # ;; - process one additional block if there is a partial block
+        mov               @{[DWORD($LENGTH)]},@{[DWORD($IA1)]}
+        add               \$15,@{[DWORD($IA1)]}
+        shr               \$4, @{[DWORD($IA1)]}     # ; $IA1 can be in the range from 0 to 16
+___
+  &GCM_ENC_DEC_SMALL(
+    $AES_KEYS,    $GCM128_CTX, $CIPH_PLAIN_OUT, $PLAIN_CIPH_IN, $PLAIN_CIPH_LEN, $ENC_DEC,
+    $DATA_OFFSET, $LENGTH,     $IA1,            $CTR_BLOCKx,    $AAD_HASHx,      $ZTMP0,
+    $ZTMP1,       $ZTMP2,      $ZTMP3,          $ZTMP4,         $ZTMP5,          $ZTMP6,
+    $ZTMP7,       $ZTMP8,      $ZTMP9,          $ZTMP10,        $ZTMP11,         $ZTMP12,
+    $ZTMP13,      $ZTMP14,     $IA0,            $IA3,           $MASKREG,        $SHUF_MASK,
+    $PBLOCK_LEN);
+
+  # ;; fall through to exit
+
+  $code .= ".L_ghash_done_${rndsuffix}:\n";
+
+  # ;; save the last counter block
+  $code .= <<___;
+        vmovdqu64         $CTR_BLOCKx,`$CTX_OFFSET_CurCount`($GCM128_CTX)
+.L_enc_dec_done_${rndsuffix}:
+        # LE->BE conversion
+        vpshufb           SHUF_MASK(%rip),$AAD_HASHx,$AAD_HASHx
+        vmovdqu64         $AAD_HASHx,`$CTX_OFFSET_AadHash`($GCM128_CTX)
+.L_enc_dec_abort_${rndsuffix}:
+___
+}
+
+# ;;; ===========================================================================
+# ;;; Encrypt/decrypt the initial 16 blocks
+sub INITIAL_BLOCKS_16 {
+  my $IN          = $_[0];     # [in] input buffer
+  my $OUT         = $_[1];     # [in] output buffer
+  my $AES_KEYS    = $_[2];     # [in] pointer to expanded keys
+  my $DATA_OFFSET = $_[3];     # [in] data offset
+  my $GHASH       = $_[4];     # [in] ZMM with AAD (low 128 bits)
+  my $CTR         = $_[5];     # [in] ZMM with CTR BE blocks 4x128 bits
+  my $CTR_CHECK   = $_[6];     # [in/out] GPR with counter overflow check
+  my $ADDBE_4x4   = $_[7];     # [in] ZMM 4x128bits with value 4 (big endian)
+  my $ADDBE_1234  = $_[8];     # [in] ZMM 4x128bits with values 1, 2, 3 & 4 (big endian)
+  my $T0          = $_[9];     # [clobered] temporary ZMM register
+  my $T1          = $_[10];    # [clobered] temporary ZMM register
+  my $T2          = $_[11];    # [clobered] temporary ZMM register
+  my $T3          = $_[12];    # [clobered] temporary ZMM register
+  my $T4          = $_[13];    # [clobered] temporary ZMM register
+  my $T5          = $_[14];    # [clobered] temporary ZMM register
+  my $T6          = $_[15];    # [clobered] temporary ZMM register
+  my $T7          = $_[16];    # [clobered] temporary ZMM register
+  my $T8          = $_[17];    # [clobered] temporary ZMM register
+  my $SHUF_MASK   = $_[18];    # [in] ZMM with BE/LE shuffle mask
+  my $ENC_DEC     = $_[19];    # [in] ENC (encrypt) or DEC (decrypt) selector
+  my $BLK_OFFSET  = $_[20];    # [in] stack frame offset to ciphered blocks
+  my $DATA_DISPL  = $_[21];    # [in] fixed numerical data displacement/offset
+  my $IA0         = $_[22];    # [clobered] temporary GP register
+
+  my $B00_03 = $T5;
+  my $B04_07 = $T6;
+  my $B08_11 = $T7;
+  my $B12_15 = $T8;
+
+  my $rndsuffix = &random_string();
+
+  my $stack_offset = $BLK_OFFSET;
+  $code .= <<___;
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        # ;; prepare counter blocks
+
+        cmpb              \$`(256 - 16)`,@{[BYTE($CTR_CHECK)]}
+        jae               .L_next_16_overflow_${rndsuffix}
+        vpaddd            $ADDBE_1234,$CTR,$B00_03
+        vpaddd            $ADDBE_4x4,$B00_03,$B04_07
+        vpaddd            $ADDBE_4x4,$B04_07,$B08_11
+        vpaddd            $ADDBE_4x4,$B08_11,$B12_15
+        jmp               .L_next_16_ok_${rndsuffix}
+.L_next_16_overflow_${rndsuffix}:
+        vpshufb           $SHUF_MASK,$CTR,$CTR
+        vmovdqa64         ddq_add_4444(%rip),$B12_15
+        vpaddd            ddq_add_1234(%rip),$CTR,$B00_03
+        vpaddd            $B12_15,$B00_03,$B04_07
+        vpaddd            $B12_15,$B04_07,$B08_11
+        vpaddd            $B12_15,$B08_11,$B12_15
+        vpshufb           $SHUF_MASK,$B00_03,$B00_03
+        vpshufb           $SHUF_MASK,$B04_07,$B04_07
+        vpshufb           $SHUF_MASK,$B08_11,$B08_11
+        vpshufb           $SHUF_MASK,$B12_15,$B12_15
+.L_next_16_ok_${rndsuffix}:
+        vshufi64x2        \$0b11111111,$B12_15,$B12_15,$CTR
+        addb               \$16,@{[BYTE($CTR_CHECK)]}
+        # ;; === load 16 blocks of data
+        vmovdqu8          `$DATA_DISPL + (64*0)`($IN,$DATA_OFFSET,1),$T0
+        vmovdqu8          `$DATA_DISPL + (64*1)`($IN,$DATA_OFFSET,1),$T1
+        vmovdqu8          `$DATA_DISPL + (64*2)`($IN,$DATA_OFFSET,1),$T2
+        vmovdqu8          `$DATA_DISPL + (64*3)`($IN,$DATA_OFFSET,1),$T3
+
+        # ;; move to AES encryption rounds
+        vbroadcastf64x2    `(16*0)`($AES_KEYS),$T4
+        vpxorq            $T4,$B00_03,$B00_03
+        vpxorq            $T4,$B04_07,$B04_07
+        vpxorq            $T4,$B08_11,$B08_11
+        vpxorq            $T4,$B12_15,$B12_15
+___
+  foreach (1 .. ($NROUNDS)) {
+    $code .= <<___;
+        vbroadcastf64x2    `(16*$_)`($AES_KEYS),$T4
+        vaesenc            $T4,$B00_03,$B00_03
+        vaesenc            $T4,$B04_07,$B04_07
+        vaesenc            $T4,$B08_11,$B08_11
+        vaesenc            $T4,$B12_15,$B12_15
+___
+  }
+  $code .= <<___;
+        vbroadcastf64x2    `(16*($NROUNDS+1))`($AES_KEYS),$T4
+        vaesenclast         $T4,$B00_03,$B00_03
+        vaesenclast         $T4,$B04_07,$B04_07
+        vaesenclast         $T4,$B08_11,$B08_11
+        vaesenclast         $T4,$B12_15,$B12_15
+
+        # ;;  xor against text
+        vpxorq            $T0,$B00_03,$B00_03
+        vpxorq            $T1,$B04_07,$B04_07
+        vpxorq            $T2,$B08_11,$B08_11
+        vpxorq            $T3,$B12_15,$B12_15
+
+        # ;; store
+        mov               $OUT, $IA0
+        vmovdqu8          $B00_03,`$DATA_DISPL + (64*0)`($IA0,$DATA_OFFSET,1)
+        vmovdqu8          $B04_07,`$DATA_DISPL + (64*1)`($IA0,$DATA_OFFSET,1)
+        vmovdqu8          $B08_11,`$DATA_DISPL + (64*2)`($IA0,$DATA_OFFSET,1)
+        vmovdqu8          $B12_15,`$DATA_DISPL + (64*3)`($IA0,$DATA_OFFSET,1)
+___
+  if ($ENC_DEC eq "DEC") {
+    $code .= <<___;
+        # ;; decryption - cipher text needs to go to GHASH phase
+        vpshufb           $SHUF_MASK,$T0,$B00_03
+        vpshufb           $SHUF_MASK,$T1,$B04_07
+        vpshufb           $SHUF_MASK,$T2,$B08_11
+        vpshufb           $SHUF_MASK,$T3,$B12_15
+___
+  } else {
+    $code .= <<___;
+        # ;; encryption
+        vpshufb           $SHUF_MASK,$B00_03,$B00_03
+        vpshufb           $SHUF_MASK,$B04_07,$B04_07
+        vpshufb           $SHUF_MASK,$B08_11,$B08_11
+        vpshufb           $SHUF_MASK,$B12_15,$B12_15
+___
+  }
+
+  if ($GHASH ne "no_ghash") {
+    $code .= <<___;
+        # ;; === xor cipher block 0 with GHASH for the next GHASH round
+        vpxorq            $GHASH,$B00_03,$B00_03
+___
+  }
+  $code .= <<___;
+        vmovdqa64         $B00_03,`$stack_offset + (0 * 64)`(%rsp)
+        vmovdqa64         $B04_07,`$stack_offset + (1 * 64)`(%rsp)
+        vmovdqa64         $B08_11,`$stack_offset + (2 * 64)`(%rsp)
+        vmovdqa64         $B12_15,`$stack_offset + (3 * 64)`(%rsp)
+___
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;;; Functions definitions
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+$code .= ".text\n";
+{
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;void gcm_init_avx512(u128 Htable[16],
+  # ;                     const uint64_t Xi[2]);
+  # ;
+  # ; Precomputes hashkey table for GHASH optimization.
+  # ; Leaf function (does not allocate stack space, does not use non-volatile registers).
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  $code .= <<___;
+.globl gcm_init_avx512
+.hidden gcm_init_avx512
+.type gcm_init_avx512,\@abi-omnipotent
+.align 32
+gcm_init_avx512:
+.cfi_startproc
+        endbranch
+___
+  if ($CHECK_FUNCTION_ARGUMENTS) {
+    $code .= <<___;
+        # ;; Check Htable != NULL
+        test               $arg1,$arg1
+        jz                .Lexit_init
+
+        # ;; Check Xi != NULL
+        test               $arg2,$arg2
+        jz                .Lexit_init
+___
+  }
+  $code .= <<___;
+        vmovdqu64         ($arg2),%xmm16
+        vpalignr           \$8,%xmm16,%xmm16,%xmm16
+        # ;;;  PRECOMPUTATION of HashKey<<1 mod poly from the HashKey ;;;
+        vmovdqa64         %xmm16,%xmm2
+        vpsllq            \$1,%xmm16,%xmm16
+        vpsrlq            \$63,%xmm2,%xmm2
+        vmovdqa           %xmm2,%xmm1
+        vpslldq           \$8,%xmm2,%xmm2
+        vpsrldq           \$8,%xmm1,%xmm1
+        vporq             %xmm2,%xmm16,%xmm16
+        # ;reduction
+        vpshufd           \$0b00100100,%xmm1,%xmm2
+        vpcmpeqd          TWOONE(%rip),%xmm2,%xmm2
+        vpand             POLY(%rip),%xmm2,%xmm2
+        vpxorq            %xmm2,%xmm16,%xmm16                  # ; xmm16 holds the HashKey<<1 mod poly
+        # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+        vmovdqu64         %xmm16,@{[HashKeyByIdx(1,$arg1)]} # ; store HashKey<<1 mod poly
+___
+  &PRECOMPUTE("$arg1", "%xmm16", "%xmm0", "%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5");
+  if ($CLEAR_SCRATCH_REGISTERS) {
+    &clear_scratch_gps_asm();
+    &clear_scratch_zmms_asm();
+  } else {
+    $code .= "vzeroupper\n";
+  }
+  $code .= <<___;
+.Lexit_init:
+ret
+.cfi_endproc
+.size gcm_init_avx512, .-gcm_init_avx512
+___
+}
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;void gcm_gmult_avx512(uint64_t Xi[2],
+# ;                      const u128 Htable[16])
+# ;
+# ; Leaf function (does not allocate stack space, does not use non-volatile registers).
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+$code .= <<___;
+.globl gcm_gmult_avx512
+.hidden gcm_gmult_avx512
+.type gcm_gmult_avx512,\@abi-omnipotent
+.align 32
+gcm_gmult_avx512:
+.cfi_startproc
+        endbranch
+___
+if ($CHECK_FUNCTION_ARGUMENTS) {
+  $code .= <<___;
+        # ;; Check Xi != NULL
+        test               $arg1,$arg1
+        jz                 .Lexit_gmult
+
+        # ;; Check Htable != NULL
+        test               $arg2,$arg2
+        jz                 .Lexit_gmult
+___
+}
+
+$code .= <<___;
+          vmovdqu64         ($arg1),%xmm1
+
+# ; GHASH_MUL works with reflected inputs, so shuffle current hash
+          vpshufb           SHUF_MASK(%rip),%xmm1,%xmm1
+          vmovdqu64         @{[HashKeyByIdx(1,$arg2)]},%xmm2
+___
+
+&GHASH_MUL("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5");
+
+# ; Store GHASH output in BE
+$code .= <<___;
+          vpshufb           SHUF_MASK(%rip),%xmm1,%xmm1
+          vmovdqu64         %xmm1,($arg1)
+___
+
+if ($CLEAR_SCRATCH_REGISTERS) {
+  &clear_scratch_gps_asm();
+  &clear_scratch_zmms_asm();
+} else {
+  $code .= "vzeroupper\n";
+}
+
+$code .= <<___;
+.Lexit_gmult:
+ret
+.cfi_endproc
+.size gcm_gmult_avx512, .-gcm_gmult_avx512
+___
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;void gcm_ghash_avx512(uint64_t Xi[2],
+# ;                      const u128 Htable[16],
+# ;                      const uint8_t *in,
+# ;                      size_t len)
+# ;
+# ; Updates AAD hash.
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+$code .= <<___;
+.globl gcm_ghash_avx512
+.hidden gcm_ghash_avx512
+.type gcm_ghash_avx512,\@abi-omnipotent
+.align 32
+gcm_ghash_avx512:
+.cfi_startproc
+.Lghash_seh_begin:
+        endbranch
+___
+if ($CHECK_FUNCTION_ARGUMENTS) {
+  $code .= <<___;
+        # ;; Check Xi != NULL
+        test               $arg1,$arg1
+        jz                 .Lexit_ghash
+
+        # ;; Check Htable != NULL
+        test               $arg2,$arg2
+        jz                 .Lexit_ghash
+
+        # ;; Check in != NULL
+        test               $arg3,$arg3
+        jz                 .Lexit_ghash
+___
+}
+
+# ; NOTE: code before PROLOG() must not modify any registers
+&PROLOG(
+  1,    # allocate stack space for hkeys,
+  0,    # do not allocate stack space for AES blocks
+  "ghash");
+
+$code .= <<___;
+          vmovdqu64         ($arg1),%xmm14               # ; load current hash
+          vpshufb           SHUF_MASK(%rip),%xmm14,%xmm14
+___
+
+&CALC_AAD_HASH(
+  "$arg3",  "$arg4", "%xmm14", "$arg2",  "%zmm1",  "%zmm11", "%zmm3",  "%zmm4",  "%zmm5",  "%zmm6",
+  "%zmm7",  "%zmm8", "%zmm9",  "%zmm10", "%zmm12", "%zmm13", "%zmm15", "%zmm16", "%zmm17", "%zmm18",
+  "%zmm19", "%r10",  "%r11",   "%r12",   "%k1");
+
+$code .= <<___;
+        vpshufb           SHUF_MASK(%rip),%xmm14,%xmm14
+        vmovdqu64         %xmm14,($arg1)               # ; save current hash
+___
+
+&EPILOG(
+  1,    # hkeys were allocated
+  $arg4);
+$code .= <<___;
+.Lexit_ghash:
+ret
+.Lghash_seh_end:
+.cfi_endproc
+.size gcm_ghash_avx512, .-gcm_ghash_avx512
+___
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ;void gcm_setiv_avx512 (const AES_KEY *key,
+# ;                       const GCM128_CONTEXT *ctx,
+# ;                       const uint8_t *iv,
+# ;                       size_t ivlen);
+# ;
+# ; Updates current counter Yi in gcm128_context structure.
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+$code .= <<___;
+.globl gcm_setiv_avx512
+.hidden gcm_setiv_avx512
+.type gcm_setiv_avx512,\@abi-omnipotent
+.align 32
+gcm_setiv_avx512:
+.cfi_startproc
+.Lsetiv_seh_begin:
+        endbranch
+___
+
+# ; NOTE: code before PROLOG() must not modify any registers
+&PROLOG(
+  1,    # allocate stack space for hkeys
+  0,    # do not allocate stack space for AES blocks
+  "setiv");
+if ($CHECK_FUNCTION_ARGUMENTS) {
+  $code .= <<___;
+        # ;; Check key != NULL
+        test               $arg1,$arg1
+        jz                 .Lexit_setiv
+
+        # ;; Check ctx != NULL
+        test               $arg2,$arg2
+        jz                 .Lexit_setiv
+
+        # ;; Check iv != 0
+        test               $arg3,$arg3
+        jz                 .Lexit_setiv
+
+        # ;; Check ivlen != 0
+        test               $arg4,$arg4
+        jz                 .Lexit_setiv
+___
+}
+&GCM_INIT_IV(
+  "$arg1",  "$arg2",  "$arg3",  "$arg4",  "%r10",   "%r11",   "%r12",  "%r13",  "%k1",   "%xmm2",
+  "%zmm1",  "%zmm11", "%zmm3",  "%zmm4",  "%zmm5",  "%zmm6",  "%zmm7", "%zmm8", "%zmm9", "%zmm10",
+  "%zmm12", "%zmm13", "%zmm15", "%zmm16", "%zmm17", "%zmm18", "%zmm19");
+$code .= ".Lexit_setiv:\n";
+&EPILOG(
+  1,    # hkeys were allocated
+  $arg4);
+$code .= <<___;
+ret
+.Lsetiv_seh_end:
+.cfi_endproc
+.size gcm_setiv_avx512, .-gcm_setiv_avx512
+___
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ; void aes_gcm_encrypt_avx512(const AES_KEY *key,
+# ;                             const GCM128_CONTEXT *ctx,
+# ;                             unsigned *pblocklen,
+# ;                             const uint8_t *in,
+# ;                             size_t len,
+# ;                             uint8_t *out);
+# ;
+# ; Performs encryption of data |in| of len |len|, and stores the output in |out|.
+# ; Stores encrypted partial block (if any) in |ctx| and its length in |pblocklen|.
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+$code .= <<___;
+.globl aes_gcm_encrypt_avx512
+.hidden aes_gcm_encrypt_avx512
+.type aes_gcm_encrypt_avx512,\@abi-omnipotent
+.align 32
+aes_gcm_encrypt_avx512:
+.cfi_startproc
+.Lencrypt_seh_begin:
+#ifdef BORINGSSL_DISPATCH_TEST
+.extern     BORINGSSL_function_hit
+        movb              \$1,BORINGSSL_function_hit+6(%rip)
+#endif
+        endbranch
+___
+
+# ; NOTE: code before PROLOG() must not modify any registers
+&PROLOG(
+  1,    # allocate stack space for hkeys
+  1,    # allocate stack space for AES blocks
+  "encrypt");
+if ($CHECK_FUNCTION_ARGUMENTS) {
+  $code .= <<___;
+        # ;; Check key != NULL
+        test               $arg1,$arg1
+        jz                 .Lexit_gcm_encrypt
+
+        # ;; Check ctx != NULL
+        test               $arg2,$arg2
+        jz                 .Lexit_gcm_encrypt
+
+        # ;; Check pblocklen != NULL
+        test               $arg3,$arg3
+        jz                 .Lexit_gcm_encrypt
+
+        # ;; Check in != NULL
+        test               $arg4,$arg4
+        jz                 .Lexit_gcm_encrypt
+
+        # ;; Check if len != 0
+        cmp                \$0,$arg5
+        jz                 .Lexit_gcm_encrypt
+
+        # ;; Check out != NULL
+        test               $arg6,$arg6
+        jz                 .Lexit_gcm_encrypt
+___
+}
+$code .= <<___;
+        # ; load number of rounds from AES_KEY structure (offset in bytes is
+        # ; size of the |rd_key| buffer)
+        mov             `4*15*4`($arg1),%eax
+        cmp             \$9,%eax
+        je              .Laes_gcm_encrypt_128_avx512
+        cmp             \$11,%eax
+        je              .Laes_gcm_encrypt_192_avx512
+        cmp             \$13,%eax
+        je              .Laes_gcm_encrypt_256_avx512
+        xor             %eax,%eax
+        jmp             .Lexit_gcm_encrypt
+___
+for my $keylen (sort keys %aes_rounds) {
+  $NROUNDS = $aes_rounds{$keylen};
+  $code .= <<___;
+.align 32
+.Laes_gcm_encrypt_${keylen}_avx512:
+___
+  &GCM_ENC_DEC("$arg1", "$arg2", "$arg3", "$arg4", "$arg5", "$arg6", "ENC");
+  $code .= "jmp .Lexit_gcm_encrypt\n";
+}
+$code .= ".Lexit_gcm_encrypt:\n";
+&EPILOG(1, $arg5);
+$code .= <<___;
+ret
+.Lencrypt_seh_end:
+.cfi_endproc
+.size aes_gcm_encrypt_avx512, .-aes_gcm_encrypt_avx512
+___
+
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+# ; void aes_gcm_decrypt_avx512(const AES_KEY *key,
+# ;                             const GCM128_CONTEXT *ctx,
+# ;                             unsigned *pblocklen,
+# ;                             const uint8_t *in,
+# ;                             size_t len,
+# ;                             uint8_t *out);
+# ;
+# ; Performs decryption of data |in| of len |len|, and stores the output in |out|.
+# ; Stores decrypted partial block (if any) in |ctx| and its length in |pblocklen|.
+# ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+$code .= <<___;
+.globl aes_gcm_decrypt_avx512
+.type aes_gcm_decrypt_avx512,\@abi-omnipotent
+.align 32
+aes_gcm_decrypt_avx512:
+.cfi_startproc
+.Ldecrypt_seh_begin:
+        endbranch
+___
+
+# ; NOTE: code before PROLOG() must not modify any registers
+&PROLOG(
+  1,    # allocate stack space for hkeys
+  1,    # allocate stack space for AES blocks
+  "decrypt");
+if ($CHECK_FUNCTION_ARGUMENTS) {
+  $code .= <<___;
+        # ;; Check key != NULL
+        test               $arg1,$arg1
+        jz                 .Lexit_gcm_decrypt
+
+        # ;; Check ctx != NULL
+        test               $arg2,$arg2
+        jz                 .Lexit_gcm_decrypt
+
+        # ;; Check pblocklen != NULL
+        test               $arg3,$arg3
+        jz                 .Lexit_gcm_decrypt
+
+        # ;; Check in != NULL
+        test               $arg4,$arg4
+        jz                 .Lexit_gcm_decrypt
+
+        # ;; Check if len != 0
+        cmp                \$0,$arg5
+        jz                 .Lexit_gcm_decrypt
+
+        # ;; Check out != NULL
+        test               $arg6,$arg6
+        jz                 .Lexit_gcm_decrypt
+___
+}
+$code .= <<___;
+        # ; load number of rounds from AES_KEY structure (offset in bytes is
+        # ; size of the |rd_key| buffer)
+        mov             `4*15*4`($arg1),%eax
+        cmp             \$9,%eax
+        je              .Laes_gcm_decrypt_128_avx512
+        cmp             \$11,%eax
+        je              .Laes_gcm_decrypt_192_avx512
+        cmp             \$13,%eax
+        je              .Laes_gcm_decrypt_256_avx512
+        xor             %eax,%eax
+        jmp             .Lexit_gcm_decrypt
+___
+for my $keylen (sort keys %aes_rounds) {
+  $NROUNDS = $aes_rounds{$keylen};
+  $code .= <<___;
+.align 32
+.Laes_gcm_decrypt_${keylen}_avx512:
+___
+  &GCM_ENC_DEC("$arg1", "$arg2", "$arg3", "$arg4", "$arg5", "$arg6", "DEC");
+  $code .= "jmp .Lexit_gcm_decrypt\n";
+}
+$code .= ".Lexit_gcm_decrypt:\n";
+&EPILOG(1, $arg5);
+$code .= <<___;
+ret
+.Ldecrypt_seh_end:
+.cfi_endproc
+.size aes_gcm_decrypt_avx512, .-aes_gcm_decrypt_avx512
+___
+
+if ($win64) {
+
+  # Add unwind metadata for SEH.
+
+  # See https://docs.microsoft.com/en-us/cpp/build/exception-handling-x64?view=msvc-160
+  my $UWOP_PUSH_NONVOL = 0;
+  my $UWOP_ALLOC_LARGE = 1;
+  my $UWOP_SET_FPREG   = 3;
+  my $UWOP_SAVE_XMM128 = 8;
+  my %UWOP_REG_NUMBER  = (
+    rax => 0,
+    rcx => 1,
+    rdx => 2,
+    rbx => 3,
+    rsp => 4,
+    rbp => 5,
+    rsi => 6,
+    rdi => 7,
+    map(("r$_" => $_), (8 .. 15)));
+
+  $code .= <<___;
+.section    .pdata
+.align  4
+    .rva    .Lghash_seh_begin
+    .rva    .Lghash_seh_end
+    .rva    .Lghash_seh_info
+
+    .rva    .Lsetiv_seh_begin
+    .rva    .Lsetiv_seh_end
+    .rva    .Lsetiv_seh_info
+
+    .rva    .Lencrypt_seh_begin
+    .rva    .Lencrypt_seh_end
+    .rva    .Lencrypt_seh_info
+
+    .rva    .Ldecrypt_seh_begin
+    .rva    .Ldecrypt_seh_end
+    .rva    .Ldecrypt_seh_info
+
+.section    .xdata
+___
+
+  foreach my $func_name ("ghash", "setiv", "encrypt", "decrypt") {
+    $code .= <<___;
+.align  8
+.L${func_name}_seh_info:
+    .byte   1   # version 1, no flags
+    .byte   .L${func_name}_seh_prolog_end-.L${func_name}_seh_begin
+    .byte   31 # num_slots = 1*8 + 2 + 1 + 2*10
+    # FR = rbp; Offset from RSP = $XMM_STORAGE scaled on 16
+    .byte   @{[$UWOP_REG_NUMBER{rbp} | (($XMM_STORAGE / 16 ) << 4)]}
+___
+
+    # Metadata for %xmm15-%xmm6
+    # Occupy 2 slots each
+    for (my $reg_idx = 15; $reg_idx >= 6; $reg_idx--) {
+
+      # Scaled-by-16 stack offset
+      my $xmm_reg_offset = ($reg_idx - 6);
+      $code .= <<___;
+    .byte   .L${func_name}_seh_save_xmm${reg_idx}-.L${func_name}_seh_begin
+    .byte   @{[$UWOP_SAVE_XMM128 | (${reg_idx} << 4)]}
+    .value  $xmm_reg_offset
+___
+    }
+
+    $code .= <<___;
+    # Frame pointer (occupy 1 slot)
+    .byte   .L${func_name}_seh_setfp-.L${func_name}_seh_begin
+    .byte   $UWOP_SET_FPREG
+
+    # Occupy 2 slots, as stack allocation < 512K, but > 128 bytes
+    .byte   .L${func_name}_seh_allocstack_xmm-.L${func_name}_seh_begin
+    .byte   $UWOP_ALLOC_LARGE
+    .value  `($XMM_STORAGE + 8) / 8`
+___
+
+    # Metadata for GPR regs
+    # Occupy 1 slot each
+    foreach my $reg ("rsi", "rdi", "r15", "r14", "r13", "r12", "rbp", "rbx") {
+      $code .= <<___;
+    .byte   .L${func_name}_seh_push_${reg}-.L${func_name}_seh_begin
+    .byte   @{[$UWOP_PUSH_NONVOL | ($UWOP_REG_NUMBER{$reg} << 4)]}
+___
+    }
+  }
+}
+
+$code .= <<___;
+.data
+.align 16
+POLY:   .quad     0x0000000000000001, 0xC200000000000000
+
+.align 64
+POLY2:
+        .quad     0x00000001C2000000, 0xC200000000000000
+        .quad     0x00000001C2000000, 0xC200000000000000
+        .quad     0x00000001C2000000, 0xC200000000000000
+        .quad     0x00000001C2000000, 0xC200000000000000
+
+.align 16
+TWOONE: .quad     0x0000000000000001, 0x0000000100000000
+
+# ;;; Order of these constants should not change.
+# ;;; More specifically, ALL_F should follow SHIFT_MASK, and ZERO should follow ALL_F
+.align 64
+SHUF_MASK:
+        .quad     0x08090A0B0C0D0E0F, 0x0001020304050607
+        .quad     0x08090A0B0C0D0E0F, 0x0001020304050607
+        .quad     0x08090A0B0C0D0E0F, 0x0001020304050607
+        .quad     0x08090A0B0C0D0E0F, 0x0001020304050607
+
+.align 16
+SHIFT_MASK:
+        .quad     0x0706050403020100, 0x0f0e0d0c0b0a0908
+
+ALL_F:
+        .quad     0xffffffffffffffff, 0xffffffffffffffff
+
+ZERO:
+        .quad     0x0000000000000000, 0x0000000000000000
+
+.align 16
+ONE:
+        .quad     0x0000000000000001, 0x0000000000000000
+
+.align 16
+ONEf:
+        .quad     0x0000000000000000, 0x0100000000000000
+
+.align 64
+ddq_add_1234:
+        .quad  0x0000000000000001, 0x0000000000000000
+        .quad  0x0000000000000002, 0x0000000000000000
+        .quad  0x0000000000000003, 0x0000000000000000
+        .quad  0x0000000000000004, 0x0000000000000000
+
+.align 64
+ddq_add_5678:
+        .quad  0x0000000000000005, 0x0000000000000000
+        .quad  0x0000000000000006, 0x0000000000000000
+        .quad  0x0000000000000007, 0x0000000000000000
+        .quad  0x0000000000000008, 0x0000000000000000
+
+.align 64
+ddq_add_4444:
+        .quad  0x0000000000000004, 0x0000000000000000
+        .quad  0x0000000000000004, 0x0000000000000000
+        .quad  0x0000000000000004, 0x0000000000000000
+        .quad  0x0000000000000004, 0x0000000000000000
+
+.align 64
+ddq_add_8888:
+        .quad  0x0000000000000008, 0x0000000000000000
+        .quad  0x0000000000000008, 0x0000000000000000
+        .quad  0x0000000000000008, 0x0000000000000000
+        .quad  0x0000000000000008, 0x0000000000000000
+
+.align 64
+ddq_addbe_1234:
+        .quad  0x0000000000000000, 0x0100000000000000
+        .quad  0x0000000000000000, 0x0200000000000000
+        .quad  0x0000000000000000, 0x0300000000000000
+        .quad  0x0000000000000000, 0x0400000000000000
+
+.align 64
+ddq_addbe_4444:
+        .quad  0x0000000000000000, 0x0400000000000000
+        .quad  0x0000000000000000, 0x0400000000000000
+        .quad  0x0000000000000000, 0x0400000000000000
+        .quad  0x0000000000000000, 0x0400000000000000
+
+.align 64
+byte_len_to_mask_table:
+        .value      0x0000, 0x0001, 0x0003, 0x0007
+        .value      0x000f, 0x001f, 0x003f, 0x007f
+        .value      0x00ff, 0x01ff, 0x03ff, 0x07ff
+        .value      0x0fff, 0x1fff, 0x3fff, 0x7fff
+        .value      0xffff
+
+.align 64
+byte64_len_to_mask_table:
+        .quad      0x0000000000000000, 0x0000000000000001
+        .quad      0x0000000000000003, 0x0000000000000007
+        .quad      0x000000000000000f, 0x000000000000001f
+        .quad      0x000000000000003f, 0x000000000000007f
+        .quad      0x00000000000000ff, 0x00000000000001ff
+        .quad      0x00000000000003ff, 0x00000000000007ff
+        .quad      0x0000000000000fff, 0x0000000000001fff
+        .quad      0x0000000000003fff, 0x0000000000007fff
+        .quad      0x000000000000ffff, 0x000000000001ffff
+        .quad      0x000000000003ffff, 0x000000000007ffff
+        .quad      0x00000000000fffff, 0x00000000001fffff
+        .quad      0x00000000003fffff, 0x00000000007fffff
+        .quad      0x0000000000ffffff, 0x0000000001ffffff
+        .quad      0x0000000003ffffff, 0x0000000007ffffff
+        .quad      0x000000000fffffff, 0x000000001fffffff
+        .quad      0x000000003fffffff, 0x000000007fffffff
+        .quad      0x00000000ffffffff, 0x00000001ffffffff
+        .quad      0x00000003ffffffff, 0x00000007ffffffff
+        .quad      0x0000000fffffffff, 0x0000001fffffffff
+        .quad      0x0000003fffffffff, 0x0000007fffffffff
+        .quad      0x000000ffffffffff, 0x000001ffffffffff
+        .quad      0x000003ffffffffff, 0x000007ffffffffff
+        .quad      0x00000fffffffffff, 0x00001fffffffffff
+        .quad      0x00003fffffffffff, 0x00007fffffffffff
+        .quad      0x0000ffffffffffff, 0x0001ffffffffffff
+        .quad      0x0003ffffffffffff, 0x0007ffffffffffff
+        .quad      0x000fffffffffffff, 0x001fffffffffffff
+        .quad      0x003fffffffffffff, 0x007fffffffffffff
+        .quad      0x00ffffffffffffff, 0x01ffffffffffffff
+        .quad      0x03ffffffffffffff, 0x07ffffffffffffff
+        .quad      0x0fffffffffffffff, 0x1fffffffffffffff
+        .quad      0x3fffffffffffffff, 0x7fffffffffffffff
+        .quad      0xffffffffffffffff
+___
+
+} else {
+# Fallback for old assembler.
+# Should not be reachable as |avx512vaes| flag is set to 1 explicitly.
+$code .= <<___;
+.globl gcm_init_avx512
+.globl gcm_ghash_avx512
+.globl gcm_gmult_avx512
+.globl gcm_setiv_avx512
+.globl aes_gcm_encrypt_avx512
+.globl aes_gcm_decrypt_avx512
+
+.hidden gcm_init_avx512
+.hidden gcm_ghash_avx512
+.hidden gcm_gmult_avx512
+.hidden gcm_setiv_avx512
+.hidden aes_gcm_encrypt_avx512
+.hidden aes_gcm_decrypt_avx512
+
+.type gcm_init_avx512,\@abi-omnipotent
+gcm_ghash_avx512:
+gcm_gmult_avx512:
+gcm_setiv_avx512:
+aes_gcm_encrypt_avx512:
+aes_gcm_decrypt_avx512:
+    .byte   0x0f,0x0b    # ud2
+    ret
+.size   gcm_init_avx512, .-gcm_init_avx512
+___
+}
+
+$code =~ s/\`([^\`]*)\`/eval $1/gem;
+print $code;
+close STDOUT or die "error closing STDOUT: $!";
\ No newline at end of file
diff -Naur boringssl/src/crypto/fipsmodule/modes/gcm.c boringssl-patched/src/crypto/fipsmodule/modes/gcm.c
--- boringssl/src/crypto/fipsmodule/modes/gcm.c	2022-07-20 05:35:27.000000000 +0800
+++ boringssl-patched/src/crypto/fipsmodule/modes/gcm.c	2023-08-07 15:03:54.000000000 +0800
@@ -144,6 +144,13 @@
   out_key->lo = H[1];
 
 #if defined(GHASH_ASM_X86_64)
+  if (crypto_gcm_avx512_enabled()) {
+    gcm_init_avx512(out_table, H);
+    *out_mult = gcm_gmult_avx512;
+    *out_hash = gcm_ghash_avx512;
+    *out_is_avx = 3;
+    return;
+  }
   if (crypto_gcm_clmul_enabled()) {
     if (CRYPTO_is_AVX_capable() && CRYPTO_is_MOVBE_capable()) {
       gcm_init_avx(out_table, H);
@@ -218,6 +225,7 @@
                     gcm_key->Htable, &is_avx, ghash_key);
 
   gcm_key->use_aesni_gcm_crypt = (is_avx && block_is_hwaes) ? 1 : 0;
+  gcm_key->use_aes_gcm_crypt_avx512 = ((is_avx > 2) && block_is_hwaes) ? 1 : 0;
 }
 
 void CRYPTO_gcm128_setiv(GCM128_CONTEXT *ctx, const AES_KEY *key,
@@ -236,6 +244,13 @@
   ctx->ares = 0;
   ctx->mres = 0;
 
+#if defined(AESNI_GCM)
+  if (ctx->gcm_key.use_aes_gcm_crypt_avx512) {
+    gcm_setiv_avx512(key, ctx, iv, len);
+    return;
+  }
+#endif
+
   uint32_t ctr;
   if (len == 12) {
     OPENSSL_memcpy(ctx->Yi.c, iv, 12);
@@ -529,6 +544,13 @@
     ctx->ares = 0;
   }
 
+#if defined(AESNI_GCM)
+  if (ctx->gcm_key.use_aes_gcm_crypt_avx512 && len > 0) {
+    aes_gcm_encrypt_avx512(key, ctx, &ctx->mres, in, len, out);
+    return 1;
+  }
+#endif
+
   unsigned n = ctx->mres;
   if (n) {
     while (n && len) {
@@ -615,6 +637,13 @@
     ctx->ares = 0;
   }
 
+#if defined(AESNI_GCM)
+  if (ctx->gcm_key.use_aes_gcm_crypt_avx512 && len > 0) {
+    aes_gcm_decrypt_avx512(key, ctx, &ctx->mres, in, len, out);
+    return 1;
+  }
+#endif
+
   unsigned n = ctx->mres;
   if (n) {
     while (n && len) {
@@ -719,5 +748,16 @@
 #else
   return 0;
 #endif
+}
+
+int crypto_gcm_avx512_enabled(void) {
+#if defined(GHASH_ASM_X86_64)
+  const uint32_t *ia32cap = OPENSSL_ia32cap_get();
+  return (ia32cap[2] & 0xC0030000) &&         // check AVX512{F+DQ+BW+VL}
+         (ia32cap[3] & (1u << (41 - 32))) &&  // check AVX512_VAES bit
+         (ia32cap[3] & (1u << (42 - 32)));    // check AVX512_VPCLMULQDQ bit
+#else
+  return 0;
+#endif
 }
 #endif
diff -Naur boringssl/src/crypto/fipsmodule/modes/gcm_test.cc boringssl-patched/src/crypto/fipsmodule/modes/gcm_test.cc
--- boringssl/src/crypto/fipsmodule/modes/gcm_test.cc	2022-07-20 05:35:27.000000000 +0800
+++ boringssl-patched/src/crypto/fipsmodule/modes/gcm_test.cc	2023-08-07 15:03:54.000000000 +0800
@@ -187,6 +187,41 @@
         }
       }
     }
+
+    if (crypto_gcm_avx512_enabled()) {
+      CHECK_ABI_SEH(gcm_init_avx512, Htable, kH);
+      CHECK_ABI_SEH(gcm_gmult_avx512, X, Htable);
+      for (size_t blocks : kBlockCounts) {
+        CHECK_ABI_SEH(gcm_ghash_avx512, X, Htable, buf, 16 * blocks);
+      }
+
+      if (hwaes_capable()) {
+        AES_KEY aes_key;
+        static const uint8_t kKey[16] = {0};
+
+        // aes_gcm_*_avx512 makes assumptions about |GCM128_CONTEXT|'s layout.
+        GCM128_CONTEXT gcm;
+        memset(&gcm, 0, sizeof(gcm));
+        memcpy(&gcm.gcm_key.H, kH, sizeof(kH));
+        memcpy(&gcm.gcm_key.Htable, Htable, sizeof(Htable));
+        memcpy(&gcm.Xi, X, sizeof(X));
+        uint8_t iv[16] = {0};
+
+        aes_hw_set_encrypt_key(kKey, 128, &aes_key);
+
+        CHECK_ABI_SEH(gcm_setiv_avx512, &aes_key, &gcm, iv, sizeof(iv));
+
+        for (size_t blocks : kBlockCounts) {
+          CHECK_ABI_SEH(aes_gcm_encrypt_avx512, &aes_key, &gcm, &gcm.mres, buf,
+                        blocks * 16, buf);
+        }
+        aes_hw_set_decrypt_key(kKey, 128, &aes_key);
+        for (size_t blocks : kBlockCounts) {
+          CHECK_ABI_SEH(aes_gcm_decrypt_avx512, &aes_key, &gcm, &gcm.mres, buf,
+                        blocks * 16, buf);
+        }
+      }
+    }
 #endif  // GHASH_ASM_X86_64
   }
 #endif  // GHASH_ASM_X86 || GHASH_ASM_X86_64
diff -Naur boringssl/src/crypto/fipsmodule/modes/internal.h boringssl-patched/src/crypto/fipsmodule/modes/internal.h
--- boringssl/src/crypto/fipsmodule/modes/internal.h	2022-07-20 05:35:27.000000000 +0800
+++ boringssl-patched/src/crypto/fipsmodule/modes/internal.h	2023-08-07 15:03:54.000000000 +0800
@@ -139,6 +139,10 @@
   // use_aesni_gcm_crypt is true if this context should use the assembly
   // functions |aesni_gcm_encrypt| and |aesni_gcm_decrypt| to process data.
   unsigned use_aesni_gcm_crypt:1;
+  // use_aes_gcm_crypt_avx512 is true if this context should use the assembly
+  // functions |aes_gcm_encrypt_avx512| and |aes_gcm_decrypt_avx512| to
+  // process data.
+  unsigned use_aes_gcm_crypt_avx512:1;
 } GCM128_KEY;
 
 // GCM128_CONTEXT contains state for a single GCM operation. The structure
@@ -165,6 +169,9 @@
 // crypto_gcm_clmul_enabled returns one if the CLMUL implementation of GCM is
 // used.
 int crypto_gcm_clmul_enabled(void);
+// crypto_gcm_avx512_enabled returns one if the AVX512 VAES + VPCLMULQDQ
+// implementation of GCM is used.
+int crypto_gcm_avx512_enabled(void);
 #endif
 
 // CRYPTO_ghash_init writes a precomputed table of powers of |gcm_key| to
@@ -267,11 +274,26 @@
 void gcm_ghash_avx(uint64_t Xi[2], const u128 Htable[16], const uint8_t *in,
                    size_t len);
 
+void gcm_init_avx512(u128 Htable[16], const uint64_t Xi[2]);
+void gcm_gmult_avx512(uint64_t Xi[2], const u128 Htable[16]);
+void gcm_ghash_avx512(uint64_t Xi[2], const u128 Htable[16], const uint8_t *in,
+                      size_t len);
+
 #define AESNI_GCM
 size_t aesni_gcm_encrypt(const uint8_t *in, uint8_t *out, size_t len,
                          const AES_KEY *key, uint8_t ivec[16], uint64_t *Xi);
 size_t aesni_gcm_decrypt(const uint8_t *in, uint8_t *out, size_t len,
                          const AES_KEY *key, uint8_t ivec[16], uint64_t *Xi);
+
+void gcm_setiv_avx512(const AES_KEY *key, const GCM128_CONTEXT *ctx,
+                      const uint8_t *iv, size_t ivlen);
+void aes_gcm_encrypt_avx512(const AES_KEY *key, const GCM128_CONTEXT *ctx,
+                            unsigned *pblocklen, const uint8_t *in, size_t len,
+                            uint8_t *out);
+void aes_gcm_decrypt_avx512(const AES_KEY *key, const GCM128_CONTEXT *ctx,
+                            unsigned *pblocklen, const uint8_t *in, size_t len,
+                            uint8_t *out);
+                            
 #endif  // OPENSSL_X86_64
 
 #if defined(OPENSSL_X86)
diff -Naur boringssl/src/crypto/impl_dispatch_test.cc boringssl-patched/src/crypto/impl_dispatch_test.cc
--- boringssl/src/crypto/impl_dispatch_test.cc	2022-07-20 05:35:27.000000000 +0800
+++ boringssl-patched/src/crypto/impl_dispatch_test.cc	2023-08-07 15:03:54.000000000 +0800
@@ -33,6 +33,9 @@
  public:
   void SetUp() override {
 #if defined(OPENSSL_X86) || defined(OPENSSL_X86_64)
+    vaes_vpclmulqdq_ =
+        (OPENSSL_ia32cap_P[2] & 0xC0030000) &&         // AVX512{F+DQ+BW+VL}
+        (((OPENSSL_ia32cap_P[3] >> 9) & 0x3) == 0x3);  // VAES + VPCLMULQDQ
     aesni_ = CRYPTO_is_AESNI_capable();
     avx_movbe_ = CRYPTO_is_AVX_capable() && CRYPTO_is_MOVBE_capable();
     ssse3_ = CRYPTO_is_SSSE3_capable();
@@ -71,6 +74,7 @@
   }
 
 #if defined(OPENSSL_X86) || defined(OPENSSL_X86_64)
+  bool vaes_vpclmulqdq_ = false;
   bool aesni_ = false;
   bool avx_movbe_ = false;
   bool ssse3_ = false;
@@ -87,15 +91,17 @@
 constexpr size_t kFlag_aes_hw_set_encrypt_key = 3;
 constexpr size_t kFlag_vpaes_encrypt = 4;
 constexpr size_t kFlag_vpaes_set_encrypt_key = 5;
+constexpr size_t kFlag_aes_gcm_encrypt_avx512 = 6;
 
 TEST_F(ImplDispatchTest, AEAD_AES_GCM) {
   AssertFunctionsHit(
       {
-          {kFlag_aes_hw_ctr32_encrypt_blocks, aesni_},
+          {kFlag_aes_hw_ctr32_encrypt_blocks, aesni_ && !vaes_vpclmulqdq_},
           {kFlag_aes_hw_encrypt, aesni_},
           {kFlag_aes_hw_set_encrypt_key, aesni_},
-          {kFlag_aesni_gcm_encrypt, is_x86_64_ && aesni_ && avx_movbe_},
+          {kFlag_aesni_gcm_encrypt, is_x86_64_ && aesni_ && avx_movbe_ && !vaes_vpclmulqdq_},
           {kFlag_vpaes_encrypt, ssse3_ && !aesni_},
+          {kFlag_aes_gcm_encrypt_avx512, is_x86_64_ && aesni_ && vaes_vpclmulqdq_},
           {kFlag_vpaes_set_encrypt_key, ssse3_ && !aesni_},
       },
       [] {
diff -Naur boringssl/src/crypto/internal.h boringssl-patched/src/crypto/internal.h
--- boringssl/src/crypto/internal.h	2022-07-20 05:35:27.000000000 +0800
+++ boringssl-patched/src/crypto/internal.h	2023-08-07 15:03:54.000000000 +0800
@@ -1272,6 +1272,7 @@
 //   3: aes_hw_set_encrypt_key
 //   4: vpaes_encrypt
 //   5: vpaes_set_encrypt_key
+//   6: aes_gcm_encrypt_avx512
 extern uint8_t BORINGSSL_function_hit[7];
 #endif  // BORINGSSL_DISPATCH_TEST
 
diff -Naur boringssl/src/util/generate_build_files.py boringssl-patched/src/util/generate_build_files.py
--- boringssl/src/util/generate_build_files.py	2022-07-20 05:35:27.000000000 +0800
+++ boringssl-patched/src/util/generate_build_files.py	2023-08-07 15:03:54.000000000 +0800
@@ -768,7 +768,7 @@
   """Returns the architectures that a given asm file should be compiled for
   based on substrings in the filename."""
 
-  if 'x86_64' in filename or 'avx2' in filename:
+  if 'x86_64' in filename or 'avx2' in filename or 'avx512' in filename:
     return ['x86_64']
   elif ('x86' in filename and 'x86_64' not in filename) or '586' in filename:
     return ['x86']
